<!DOCTYPE html>
<html lang="en-US">
  
<head>


  <!-- favicon settings -->
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/site-images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/site-images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/site-images/favicon-16x16.png">
  <link rel="manifest" href="/assets/images/site-images/site.webmanifest">
  <link rel="mask-icon" href="/assets/images/site-images/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/assets/images/site-images/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-config" content="/assets/images/site-images/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">



  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });

  
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  

  <!-- Prism is now loaded by being imported in assets/css/style.scss -->
  <!-- <link href="/assets/css/prism.css" rel="stylesheet" /> -->
  <link rel="stylesheet" href="/assets/css/style.css">


  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Ray Tracing in One Weekend:</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Ray Tracing in One Weekend:" />
<meta name="author" content="Evan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Now that we’re familiar with ray tracing through my introduction, we can delve into the titular first section of Peter Shirley’s book." />
<meta property="og:description" content="Now that we’re familiar with ray tracing through my introduction, we can delve into the titular first section of Peter Shirley’s book." />
<link rel="canonical" href="http://localhost:4000/2020/06/19/ray-tracing-in-one-weekend-part-two.html" />
<meta property="og:url" content="http://localhost:4000/2020/06/19/ray-tracing-in-one-weekend-part-two.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-19T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Ray Tracing in One Weekend:" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Evan"},"dateModified":"2020-06-19T00:00:00-04:00","datePublished":"2020-06-19T00:00:00-04:00","description":"Now that we’re familiar with ray tracing through my introduction, we can delve into the titular first section of Peter Shirley’s book.","headline":"Ray Tracing in One Weekend:","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/06/19/ray-tracing-in-one-weekend-part-two.html"},"url":"http://localhost:4000/2020/06/19/ray-tracing-in-one-weekend-part-two.html"}</script>
<!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">




  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167888524-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-167888524-1');
  </script>
  
  <meta charset="UTF-8">

 
  
    <style>

    img{
      
      image-rendering: auto;
      image-rendering: crisp-edges;
      image-rendering: pixelated;
    }
    </style>
  


</head>

  <body>
    <!-- <a id="skip-to-content" href="#content">Skip to the content.</a> -->

    <header class="page-header" role="banner">
      <img class="header-image" src="/assets/images/site-images/chicken.jpg">
      <h1 class="project-name"></h1>
      <h2 class="project-tagline"></h2>
      <!-- <link href="../assets/fontawesome/all.css" rel="stylesheet"> load all styles -->

<script src="/js/site-scripts/toggle-search.js" type="text/javascript"></script>

<nav>
  
  <a href="/"

    

    
      class="nav-bar-link"
    >
    
    <i class="fas fa-home"></i>
    <br>
    Home
  </a>
  
  <a href="/about.html"

    

    
      class="nav-bar-link"
    >
    
    <i class="fas fa-user"></i>
    <br>
    About
  </a>
  
  <a href="/archive.html"

    

    
      class="nav-bar-link"
    >
    
    <i class="fas fa-archive"></i>
    <br>
    Archive
  </a>
  
  <a href="/tags.html"

    

    
      class="nav-bar-link"
    >
    
    <i class="fas fa-tags"></i>
    <br>
    Tags
  </a>
  
  <a href=""

     
      id="search-button" onclick="toggleSearch(); return false;" 
    

    
      class="nav-bar-link"
    >
    
    <i class="fas fa-search"></i>
    <br>
    Search
  </a>
  
</nav>
      <!-- Html Elements for Search -->
<div id="search-container" style="visibility: hidden;">
  <input type="text" id="search-input" placeholder="Search..." />
  <ul id="results-container"></ul>
</div>

<!-- Script pointing to search-script.js -->
<script src="/js/site-scripts/search-script.js" type="text/javascript"></script>

<!-- Configuration -->
<script>
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/search.json',
    searchResultTemplate: '<li><a href="http://localhost:4000{postUrl}">{postTitle} {postSubtitle}</a></li>'
  })
  </script>


      
      
    </header>

    <hr id="header-main-divider">

    <main id="content" class="main-content" role="main">
      <div class="post-header inactive">
<h1 id="post-title">Ray Tracing in One Weekend:</h1>
<h3 id="post-subtitle">Part Two - The First Weekend</h3>
<div class="post-date">
    <i class="fas fa-calendar"></i> <time>19 Jun 2020</time>
</div>
</div>

<img class="post-image" src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/final-render-1.png" alt="Path traced sphere scene render." title="">



<a id="continue-reading-point"></a>

<div class="excerpt">
<p>Now that we’re familiar with ray tracing through <a href="http://localhost:4000/2020/05/20/ray-tracing-in-one-weekend-part-one.html#post-title">my introduction</a>, we can delve into the titular first section of Peter Shirley’s book.</p>

</div>
<!--end-excerpt-->





<p><span class="note">
    I started this path tracer months ago, and only started this blog in late May. The version of Shirley’s book that I used is from some time in 2018 (Version 1.54), and I have found that there is a recently updated version (3.1.2) on <a href="https://raytracing.github.io/">his website</a> from June 6th, 2020! I also supplemented Shirley’s book with <a href="https://viclw17.github.io/writing.html">Victor Li’s posts on the subject</a>. As such, there may be differences in implementation compared to the most recent version of Ray Tracing in One Weekend. <br /><br />I am trying to keep things easy-to-follow, mostly sticking with my original code and only changing or adding what I deem to be necessary for readability, clarity, or image rendering purposes. If you are reading this to build your own ray tracer, I highly recommend Shirley’s book as a primary source.
</span></p>




<hr>

<h2>Contents</h2>

<ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#image-output">Image Output</a></li>
<li class="toc-entry toc-h2"><a href="#timing-execution">Timing Execution</a></li>
<li class="toc-entry toc-h2"><a href="#vec3-class">Vec3 Class</a>
<ul>
<li class="toc-entry toc-h3"><a href="#vector-refresher">Vector Refresher</a></li>
<li class="toc-entry toc-h3"><a href="#vec3-definitions">Vec3 Definitions</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#rays">Rays</a>
<ul>
<li class="toc-entry toc-h3"><a href="#sending-rays-from-the-camera">Sending Rays from the Camera</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#introducing-spheres">Introducing Spheres</a>
<ul>
<li class="toc-entry toc-h3"><a href="#describing-a-sphere">Describing a Sphere</a></li>
<li class="toc-entry toc-h3"><a href="#placing-a-sphere">Placing a Sphere</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#surface-normals">Surface Normals</a>
<ul>
<li class="toc-entry toc-h3"><a href="#simplifying-ray-sphere-intersection">Simplifying Ray-Sphere Intersection</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#multiple-spheres-with-the-hittable-class">Multiple Spheres with the Hittable Class</a></li>
<li class="toc-entry toc-h2"><a href="#front-faces-versus-back-faces">Front Faces Versus Back Faces</a></li>
<li class="toc-entry toc-h2"><a href="#anti-aliasing">Anti-Aliasing</a>
<ul>
<li class="toc-entry toc-h3"><a href="#adding-anti-aliasing-to-the-camera">Adding Anti-Aliasing to the Camera</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#diffuse-materials">Diffuse Materials</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-math-of-diffuse-materials">The Math of Diffuse Materials</a></li>
<li class="toc-entry toc-h3"><a href="#gamma-correction">Gamma Correction</a></li>
<li class="toc-entry toc-h3"><a href="#shadow-acne">Shadow Acne</a></li>
<li class="toc-entry toc-h3"><a href="#true-lambertian-reflection">True Lambertian Reflection</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#common-constants-and-utilities">Common Constants and Utilities</a></li>
<li class="toc-entry toc-h2"><a href="#metal">Metal</a>
<ul>
<li class="toc-entry toc-h3"><a href="#abstract-class-for-materials">Abstract Class for Materials</a></li>
<li class="toc-entry toc-h3"><a href="#describing-ray-object-intersections">Describing Ray-Object Intersections</a></li>
<li class="toc-entry toc-h3"><a href="#light-scatter">Light Scatter</a></li>
<li class="toc-entry toc-h3"><a href="#metal-reflection">Metal Reflection</a></li>
<li class="toc-entry toc-h3"><a href="#adding-metal-spheres-to-the-scene">Adding Metal Spheres to the Scene</a></li>
<li class="toc-entry toc-h3"><a href="#fuzzy-metal">Fuzzy Metal</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#dielectrics">Dielectrics</a>
<ul>
<li class="toc-entry toc-h3"><a href="#refraction">Refraction</a></li>
<li class="toc-entry toc-h3"><a href="#snells-law">Snell’s Law</a></li>
<li class="toc-entry toc-h3"><a href="#total-internal-reflection">Total Internal Reflection</a></li>
<li class="toc-entry toc-h3"><a href="#calculating-the-refraction-vector">Calculating the Refraction Vector</a></li>
<li class="toc-entry toc-h3"><a href="#coding-the-refraction-vector">Coding the Refraction Vector</a></li>
<li class="toc-entry toc-h3"><a href="#dielectric-reflections">Dielectric Reflections</a></li>
<li class="toc-entry toc-h3"><a href="#hollow-dielectric-spheres">Hollow Dielectric Spheres</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#camera-modeling">Camera Modeling</a></li>
<li class="toc-entry toc-h2"><a href="#depth-of-field">Depth of Field</a></li>
<li class="toc-entry toc-h2"><a href="#final-scene">Final Scene</a></li>
</ul>

<hr>

<div id="markdown-content">
    <h2 id="image-output">
<a class="anchor" href="#image-output" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="image-output"></a>Image Output</h2>
<p>Of course, the first step with producing a pretty path traced image is to produce an image. The method suggested by Peter is a simple plaintext <code class="language-plaintext highlighter-rouge">.ppm</code> file. The following is an example snippet and image from <a href="https://en.wikipedia.org/wiki/Netpbm#PPM_example">Wikipedia</a>:</p>

<div class="row">
<pre><code class="language-shell">P3
3 2
255
# The part above is the header
# "P3" means this is an RGB color image in ASCII
# "3 2" is the width and height of the image in pixels
# "255" is the maximum value for each color
# The part below is image data: RGB triplets
255   0   0  # red
  0 255   0  # green
  0   0 255  # blue
255 255   0  # yellow
255 255 255  # white
  0   0   0  # black</code></pre>
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/ppm-example-output.png">

</div>

<p>The code for creating a <code class="language-plaintext highlighter-rouge">.ppm</code> file is as follows:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value
	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			float r = float(i) / float(nx);
			float g = float(j) / float(ny);
			float b = 0.2;
			int ir = int(255.99 * r);
			int ig = int(255.99 * g);
			int ib = int(255.99 * b);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>Note:</p>

<ul>
  <li>Pixels are written from left to right.</li>
  <li>Rows of pixels are written top to bottom.</li>
  <li>In this simple example, from left to right, red goes from 0 to 255. Green goes from 0 to 255, bottom to top. As such, the top right corner should be yellow.</li>
</ul>

<p>Now to compile and redirect the output of our program to a file:</p>
<pre><code class="language-shell">g++ main.cpp
./a.out &gt; hello.ppm</code></pre>

<p>You may have to use a <a href="http://www.cs.rhodes.edu/welshc/COMP141_F16/ppmReader.html">web tool</a> or download a file viewer (I use <a href="https://www.irfanview.com/">IrfanView</a>) to view the <code class="language-plaintext highlighter-rouge">.ppm</code> file as an image. Here’s my resulting image and raw contents of the file:</p>

<p><span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hello-world-ppm.png" alt='The "Hello World" of our path tracer'>
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/hello-world-ppm-raw.png" alt='The "Hello World" of our path tracer'>
</span></p>

<hr>

<h2 id="timing-execution">
<a class="anchor" href="#timing-execution" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="timing-execution"></a>Timing Execution</h2>
<p>Eventually, our program is going to chug when it comes to producing an image. It’s nice to have a total running time output. This is optional, and you can <a href="#vec3-class">skip</a> it if you please.</p>

<p>If you want, you could just run our program in the terminal prepended with <code class="language-plaintext highlighter-rouge">time</code>. Here’s an example of the utility:</p>

<pre><code class="shell">eldun@Evan:/mnt/c/Users/Ev/source/Projects/PathTracer/PathTracer$ time sleep 1

real    0m1.019s
user    0m0.016s
sys     0m0.000s</code></pre>

<p>Otherwise, you can <code class="language-plaintext highlighter-rouge">#include &lt;chrono&gt;</code> (for timing) and <code class="language-plaintext highlighter-rouge">#include &lt;iomanip&gt;</code> (for formatting) in main (or anywhere) to time more specific parts of the program:</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;
<span class="highlight-green">#include &lt;chrono&gt;
#include &lt;iomanip&gt;
</span>

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

   	<span class="highlight-green">	auto start = std::chrono::high_resolution_clock::now();	   </span>

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			float r = float(i) / float(nx);
			float g = float(j) / float(ny);
			float b = 0.2;
			int ir = int(255.99 * r);
			int ig = int(255.99 * g);
			int ib = int(255.99 * b);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	<span class="highlight-green">	auto stop = std::chrono::high_resolution_clock::now(); 


	auto hours = std::chrono::duration_cast&lt;std::chrono::hours&gt;(stop - start);
	auto minutes = std::chrono::duration_cast&lt;std::chrono::minutes&gt;(stop - start) - hours;
	auto seconds = std::chrono::duration_cast&lt;std::chrono::seconds&gt;(stop - start) - hours - minutes;
    	std::cerr &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; "\nDone in:" &lt;&lt; std::endl &lt;&lt; 
	"\t" &lt;&lt; hours.count() &lt;&lt; " hours" &lt;&lt; std::endl &lt;&lt;
	"\t" &lt;&lt; minutes.count() &lt;&lt; " minutes" &lt;&lt; std::endl &lt;&lt;
	"\t" &lt;&lt; seconds.count() &lt;&lt; " seconds." &lt;&lt; std::endl; </span>

}</code></pre>

<hr>

<h2 id="vec3-class">
<a class="anchor" href="#vec3-class" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="vec3-class"></a>Vec3 Class</h2>
<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/vectors/vector.png" alt="Vector">
Vectors! I feel like I haven’t used these since high school math but they are <strong>lovely</strong>. If you need or want a refresher on vectors, make sure to read <a href="#vector-refresher">this section</a>. According to Peter Shirley, almost all graphics programs have some class(es) for storing geometric vectors and colors. In many cases, the vectors are four-dimensional to represent homogenous coordinates for geometry, or to represent the alpha transparency channel for color values. We’ll be using three-dimensional coordinates, as that’s all we need to represent direction, color, location, offset, etc.</p>

<h3 id="vector-refresher">
<a class="anchor" href="#vector-refresher" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="vector-refresher"></a>Vector Refresher</h3>
<p>Need a vector refresher? If so, check out <a href="https://www.mathsisfun.com/algebra/vectors.html">this rundown</a> at mathisfun.com. It’s the best I’ve found.
All the operations within the code above are covered the mathisfun post. Take particular note of <a href="https://www.mathsisfun.com/algebra/vector-unit.html"><code class="language-plaintext highlighter-rouge">make_unit_vector()</code></a>, <a href="https://www.mathsisfun.com/algebra/vectors-dot-product.html"><code class="language-plaintext highlighter-rouge">dot()</code></a>, and <a href="https://www.mathsisfun.com/algebra/vectors-cross-product.html"><code class="language-plaintext highlighter-rouge">cross()</code></a>.</p>

<p>Here are the constructors and declarations of the functions we’ll be using within <code class="language-plaintext highlighter-rouge">vec3.h</code>.</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">#ifndef VEC3H
#define VEC3H

#include &lt;math.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;iostream&gt;

// 3 dimensional vectors will be used for colors, locations, directions, offsets, etc.
class vec3 {
public:
	vec3() {}
	vec3(double e0, double e1, double e2) { e[0] = e0; e[1] = e1; e[2] = e2; }

	inline double x() const { return e[0]; }
	inline double y() const { return e[1]; }
	inline double z() const { return e[2]; }
	inline double r() const { return e[0]; }
	inline double g() const { return e[1]; }
	inline double b() const { return e[2]; }

	// return reference to current vec3 object
	inline const vec3&amp; operator+() const { return *this; }

	// return opposite of vector when using '-'
	inline vec3 operator-() const { return vec3(-e[0], -e[1], -e[2]); }

	// return value or reference to value of vec3 at index i ( I believe)
	inline double operator[](int i) const { return e[i]; }
	inline double&amp; operator[](int i) { return e[i]; };

	inline vec3&amp; operator+=(const vec3&amp; v2);
	inline vec3&amp; operator-=(const vec3&amp; v2);
	inline vec3&amp; operator*=(const vec3&amp; v2);
	inline vec3&amp; operator/=(const vec3&amp; v2);
	inline vec3&amp; operator*=(const double t);
	inline vec3&amp; operator/=(const double t);

	inline double length() const {
		return sqrt(e[0]*e[0] + e[1]*e[1] + e[2]*e[2]);
	}
	inline double squared_length() const {
		return e[0]*e[0] + e[1]*e[1] + e[2]*e[2];
	}
	inline void make_unit_vector();

	double e[3];
};

...</code></pre>

<h3 id="vec3-definitions">
<a class="anchor" href="#vec3-definitions" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="vec3-definitions"></a>Vec3 Definitions</h3>
<p>The next step in our vector class is to define our functions. Be very careful here! This is where I had a few minor typo issues that mangled the final image later in the project. It’s not hard to see why; these are the lowest-level operations of vectors, which will simulate our light rays and their properties.</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">...

// input output overloading
inline std::istream&amp; operator&gt;&gt;(std::istream&amp; is, vec3&amp; t) {
	is &gt;&gt; t.e[0] &gt;&gt; t.e[1] &gt;&gt; t.e[2];
	return is;
}

inline std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const vec3&amp; t) {
	os &lt;&lt; t.e[0] &lt;&lt; " " &lt;&lt; t.e[1] &lt;&lt; " " &lt;&lt; t.e[2];
	return os;
}


inline void vec3::make_unit_vector() {
	double k = 1.0 / sqrt(e[0]*e[0] + e[1]*e[1] + e[2]*e[2]);
	e[0] *= k;
	e[1] *= k;
	e[2] *= k;
}

inline vec3 operator+(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] + v2.e[0], v1.e[1] + v2.e[1], v1.e[2] + v2.e[2]);
}

inline vec3 operator-(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] - v2.e[0], v1.e[1] - v2.e[1], v1.e[2] - v2.e[2]);
}

inline vec3 operator*(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] * v2.e[0], v1.e[1] * v2.e[1], v1.e[2] * v2.e[2]);
}

inline vec3 operator/(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] / v2.e[0], v1.e[1] / v2.e[1], v1.e[2] / v2.e[2]);
}

inline vec3 operator*(double t, const vec3&amp; v) {
	return vec3(t * v.e[0], t * v.e[1], t * v.e[2]);
}

inline vec3 operator/(const vec3 v, double t) {
	return vec3(v.e[0] / t, v.e[1] / t, v.e[2] / t);
}

inline vec3 operator*(const vec3&amp; v, double t) {
	return vec3(t * v.e[0], t * v.e[1], t * v.e[2]);
}

// Dot product
inline double dot(const vec3&amp; v1, const vec3&amp; v2) {
	return
		v1.e[0] * v2.e[0]
		+ v1.e[1] * v2.e[1]
		+ v1.e[2] * v2.e[2];
}

// Cross product
inline vec3 cross(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[1] * v2.e[2] - v1.e[2] * v2.e[1],
				v1.e[2] * v2.e[0] - v1.e[0] * v2.e[2],
				v1.e[0] * v2.e[1] - v1.e[1] * v2.e[0]);
}

inline vec3&amp; vec3::operator+=(const vec3&amp; v) {
	e[0] += v.e[0];
	e[1] += v.e[1];
	e[2] += v.e[2];
	return *this;
}
inline vec3&amp; vec3::operator-=(const vec3&amp; v) {
	e[0] -= v.e[0];
	e[1] -= v.e[1];
	e[2] -= v.e[2];
	return *this;
}

inline vec3&amp; vec3::operator*=(const vec3&amp; v) {
	e[0] *= v.e[0];
	e[1] *= v.e[1];
	e[2] *= v.e[2];
	return *this;
}

inline vec3&amp; vec3::operator/=(const vec3&amp; v) {
	e[0] /= v.e[0];
	e[1] /= v.e[1];
	e[2] /= v.e[2];
	return *this;
}

inline vec3&amp; vec3::operator*=(const double t) {
	e[0] *= t;
	e[1] *= t;
	e[2] *= t;
	return *this;
}

inline vec3&amp; vec3::operator/=(const double t) {
	double k = 1.0 / t;

	e[0] *= k;
	e[1] *= k;
	e[2] *= k;
	return *this;
}

inline vec3 unit_vector(vec3 v) {
	return v / v.length();
}

#endif // !VEC3H</code></pre>

<p>Make sure to include our new vec3.h in main.cpp.</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;

<span class="highlight-green">
#include "vec3.h"
</span>

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value
	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			<span class="highlight-green">
			vec3 col(float(i) / float(nx), float(j) / float(ny), 0.2);
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			</span>
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}
</code></pre>

<hr>

<h2 id="rays">
<a class="anchor" href="#rays" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="rays"></a>Rays</h2>
<p>Ray tracers need rays! These are what will be colliding with objects in the scene. Rays have an origin, a direction, and can be described by the following formula:</p>

<p><strong><em>P</em></strong>(<em>t</em>) = <strong><em>A</em></strong> + <em>t</em><strong><em>B</em></strong></p>

<ul>
  <li>
<strong><em>p</em></strong> is a point on the ray.</li>
  <li>
<strong><em>A</em></strong> is the ray origin.</li>
  <li>
<strong><em>B</em></strong> is the direction of the ray.</li>
  <li>The ray parameter <em>t</em> is a real number (positive or negative) that moves <strong><em>p</em></strong>(t) along the ray.</li>
</ul>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.lerp.png" alt="Our Ray (Illustration from Peter Shirley's book)"></p>

<p>Here’s the header file for our ray class:</p>

<p><code class="language-plaintext highlighter-rouge">ray.h:</code></p>
<pre><code class="language-cpp">#ifndef RAYH
#define RAYH
#include "Vec3.h"

class ray
{
public:
	ray() {}
	ray(const vec3&amp; a, const vec3&amp; b) { A = a; B = b; }
	vec3 origin() const		{ return A; }
	vec3 direction() const	{ return B; }
	vec3 point_at_parameter(double t) const { return A + t * B; }

	vec3 A;
	vec3 B;
};

#endif // !RAYH</code></pre>

<h3 id="sending-rays-from-the-camera">
<a class="anchor" href="#sending-rays-from-the-camera" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="sending-rays-from-the-camera"></a>Sending Rays from the Camera</h3>
<p>Put simply, our ray tracer will send rays through pixels and compute the color seen for each ray. The steps for doing so are as follows:</p>

<ol>
  <li>Calculate the ray from camera to pixel.</li>
  <li>Determine which objects the ray intersects.</li>
  <li>Compute a color for the intersection.</li>
</ol>

<p>We will need a “viewport” of sorts to pass rays through from our “camera.” Since we’re using standard square pixel spacing, the viewport will have the same aspect ratio as our rendered image. Shirley sets the height of the viewport to two units in his book, and we’ll do the same.</p>

<p>Using Peter Shirley’s example, we’re going to set the camera at (0,0,0), and look towards the negative z-axis. The viewport will be traversed with rays from left-to-right, bottom-to-top. Variables u and v will be the offset vectors used to move the camera ray along the viewport:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.cam-geom.png" alt="Camera Geometry (Illustration from Peter Shirley's book)"></p>

<p>Here’s our code for the camera, as well as rendering a blue-to-white gradient:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "ray.h"

/*
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
*/
vec3 color(const ray&amp; r) {
	vec3 unit_direction = unit_vector(r.direction());
	double t = 0.5 * (unit_direction.y() + 1.0);
	return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);

}

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// The values below are derived from making the "camera"/ray origin coordinates (0, 0, 0) relative to the canvas.
	vec3 lower_left_corner(-2.0, -1.0, -1.0);
	vec3 horizontal(4.0, 0.0, 0.0);
	vec3 vertical(0.0, 2.0, 0.0);
	vec3 origin(0.0, 0.0, 0.0);
	for (int j = ny - 1; j &gt;= 0; j--) {
		std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			double u = double(i) / double(nx);
			double v = double(j) / double(ny);

			// Approximate pixel centers on the canvas for each ray r
			ray r(origin, lower_left_corner + u * horizontal + v * vertical);

			vec3 col = color(r);
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>The result:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/gradient.png" alt="Linear Gradient"></p>

<p>We can move the camera code into <code class="language-plaintext highlighter-rouge">camera.h</code>.</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code>:</p>
<pre><code class="language-cpp">
#ifndef CAMERAH
#define CAMERAH

#include "ray.h"

class camera {
public:

	// The values below are derived from making the "camera" / ray origin coordinates(0, 0, 0) relative to the canvas.
	camera() {
		lower_left_corner = vec3(-2.0, -1.0, -1.0);
		horizontal = vec3(4.0, 0.0, 0.0);
		vertical = vec3(0.0, 2.0, 0.0);
		origin = vec3(0.0, 0.0, 0.0);
	}
	ray get_ray(double u, double v) { return ray(origin, lower_left_corner + u * horizontal + v * vertical - origin); }

	vec3 origin;
	vec3 lower_left_corner;
	vec3 horizontal;
	vec3 vertical;
};

#endif // !CAMERAH
</code></pre>

<hr>

<h2 id="introducing-spheres">
<a class="anchor" href="#introducing-spheres" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="introducing-spheres"></a>Introducing Spheres</h2>
<h3 id="describing-a-sphere">
<a class="anchor" href="#describing-a-sphere" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="describing-a-sphere"></a>Describing a Sphere</h3>
<p>We have a beautiful sky-like gradient. Let’s add a sphere! Spheres are popular in ray tracers because they’re mathematically simple.</p>

<p>A sphere centered at the origin of radius $R$ is $x^2 + y^2 + z^2 = R^2$.</p>

<ul>
  <li>This means that if a point $(x,y,z)$ is on a sphere, $x^2 + y^2 + z^2 = R^2$.</li>
  <li>If the point is inside the sphere, $x^2 + y^2 + z^2 &lt; R^2$.</li>
  <li>If the point is outside the sphere, $x^2 + y^2 + z^2 &gt; R^2$</li>
</ul>

<p>If the sphere center isn’t at the origin, the formula is:</p>

\[(x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2 = r^2\]

<p>It’s best if formulas are kept under the hood in the vec3 class.</p>

<p>The vector from center $\mathbf{C} = (C_x,C_y,C_z)$ to point $\mathbf{P} = (x,y,z)$ is $(\mathbf{P} - \mathbf{C})$, and therefore</p>

\[(\mathbf{P} - \mathbf{C}) \cdot (\mathbf{P} - \mathbf{C}) = (x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2\]

<p>Therefore, the equation of a sphere in vector form is:</p>

\[(\mathbf{P} - \mathbf{C}) \cdot (\mathbf{P} - \mathbf{C}) = r^2\]

<p>Any point $\mathbf{P}$ that satisfies this equation is on the sphere.
We’re going to find out if a given ray <em>ever</em> hits the sphere. If it does, there is a value <em>t</em> for which P(t) satisfies this equation:</p>

\[(\mathbf{P}(t) - \mathbf{C}) \cdot (\mathbf{P}(t) - \mathbf{C}) = r^2\]

<p>The same formula, expanded:</p>

\[(\mathbf{A} + t \mathbf{b} - \mathbf{C}) \cdot (\mathbf{A} + t \mathbf{b} - \mathbf{C}) = r^2\]

<p>and again:</p>

\[t^2 \mathbf{b} \cdot \mathbf{b} + 2t \mathbf{b} \cdot (\mathbf{A}-\mathbf{C}) + (\mathbf{A}-\mathbf{C}) \cdot (\mathbf{A}-\mathbf{C}) - r^2 = 0\]

<p>The unknown variable is <em>t</em>, and this is a quadratic equation. Solving for <em>t</em> will lead to a square root operation (aka the discriminant) that is either positive (two real solutions), negative (no real solutions), or zero (one real solution):</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.ray-sphere.png" alt="Ray-Sphere Intersections(Illustration from Peter Shirley's book)"></p>

<h3 id="placing-a-sphere">
<a class="anchor" href="#placing-a-sphere" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="placing-a-sphere"></a>Placing a Sphere</h3>

<p>Prepend <code class="language-plaintext highlighter-rouge">main.cpp</code>’s main function with the following to mathematically hard-code a sphere to be hit by rays:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">...

bool hit_sphere(const vec3&amp; center, double radius, const ray&amp; r) {
	vec3 oc = r.origin() - center;
	double a = dot(r.direction(), r.direction());
	double b = 2.0 * dot(oc, r.direction());
	double c = dot(oc, oc) - radius * radius;
	double discriminant = b*b - 4*a*c;
	return (discriminant &gt; 0);
}

/*
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
*/
vec3 color(const ray&amp; r) {
	if (hit_sphere(vec3(0, 0, -1), 0.5, r))
		return vec3(1, 0, 0);
	vec3 unit_direction = unit_vector(r.direction());
	double t = 0.5 * (unit_direction.y() + 1.0);
	return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
}

...</code></pre>

<p>The result:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/red-sphere.png" alt="Ray traced sphere"></p>

<!-- Be aware that if the sphere center is change to z= +1, we'll still see the same image. We should not be seeing objects behind us. This will be fixed in the next section. -->

<hr>

<h2 id="surface-normals">
<a class="anchor" href="#surface-normals" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="surface-normals"></a>Surface Normals</h2>

<p>Our sphere looks like a circle. To make it more obvious that it <em>is</em> a sphere, we’ll add surface normals to the face. Surface normals are simply vectors that are perpendicular to the surface of an object.</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/Normal_vectors_on_a_curved_surface.svg" alt="Surface Normal"></p>

<p>In our case, the outward normal is the hitpoint minus the center:</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.sphere-normal.png" alt="Surface Normal(from Shirley's book)"></p>

<p>Since we don’t have any lights, we can visualize the normals with a color map.</p>

<p>Our <code class="language-plaintext highlighter-rouge">main.cpp</code> file will now look something like this:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "ray.h"

double hit_sphere(const vec3&amp; center, double radius, const ray&amp; r) {
	vec3 oc = r.origin() - center;
	double a = dot(r.direction(), r.direction());
	double b = 2.0 * dot(oc, r.direction());
	double c = dot(oc, oc) - radius * radius;
	double discriminant = (b*b) - (4*a*c);
	if (discriminant &lt; 0) {
		return -1.0;
	}

<span class="highlight-green">
	else {
		return (-b - sqrt(discriminant)) / (2.0 * a);
</span>}
}

/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
* Draw sphere and surface normals
*/
vec3 color(const ray&amp; r) {
	double t = hit_sphere(vec3(0, 0, -1), 0.5, r); // does the ray hit the values of a sphere placed at (0,0,-1) with a radius of .5?
	if (t &gt; 0.0) { // sphere hit
		vec3 N = unit_vector(r.point_at_parameter(t) - vec3(0, 0, -1)); // N (the normal) is calculated
		return 0.5 * (vec3(N.x() + 1, N.y() + 1, N.z() + 1)); // RGB values assigned based on xyz values
	}
	vec3 unit_direction = unit_vector(r.direction());
	t = 0.5 * (unit_direction.y() + 1.0);
	return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);

}

...</code></pre>

<p>Our resulting image:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/surface-normals-render.png" alt="Sphere with Normals"></p>

<h3 id="simplifying-ray-sphere-intersection">
<a class="anchor" href="#simplifying-ray-sphere-intersection" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="simplifying-ray-sphere-intersection"></a>Simplifying Ray-Sphere Intersection</h3>

<p>As it turns out, we can simplify ray-sphere intersection. Here’s our original equation:</p>
<pre><code class="language-cpp">
vec3 oc = r.origin() - center;
auto a = dot(r.direction(), r.direction());
auto b = 2.0 * dot(oc, r.direction());
auto c = dot(oc, oc) - radius*radius;
auto discriminant = b*b - 4*a*c;
</code></pre>

<p>The dot product of a vector with itself is equal to the squared length of that vector.</p>

<p>Additionally, the equation for b has a factor of two in it. Consider the quadratic equation if b = 2h:</p>

\[\frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]

\[= \frac{-2h \pm \sqrt{(2h)^2 - 4ac}}{2a}\]

\[= \frac{-2h \pm 2\sqrt{h^2 - ac}}{2a}\]

\[= \frac{-h \pm \sqrt{h^2 - ac}}{a}\]

<p>As such, we can refactor our code like so:</p>
<pre><code class="language-cpp">vec3 oc = r.origin() - center;
auto a = r.direction().length_squared();
auto half_b = dot(oc, r.direction());
auto c = oc.length_squared() - radius*radius;
auto discriminant = half_b*half_b - a*c;

if (discriminant &lt; 0) {
    return -1.0;
} else {
    return (-half_b - sqrt(discriminant) ) / a;
}</code></pre>

<p>Cool! But it could be cooler. We need more spheres. The cleanest way to accomplish this is to create an abstract class - a class that must be overwritten by derived classes - of hittable objects.</p>

<hr>

<h2 id="multiple-spheres-with-the-hittable-class">
<a class="anchor" href="#multiple-spheres-with-the-hittable-class" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="multiple-spheres"></a>Multiple Spheres with the Hittable Class</h2>

<p>Our hittable abstract class will have a “hit” function that will be passed a ray and a record containing information about the hit, such as the time(which will be added with motion blur later in this series), position, and the surface normal:</p>

<p><code class="language-plaintext highlighter-rouge">hittable.h:</code></p>
<pre><code class="language-cpp">#ifndef HITTABLEH
#define HITTABLEH

#include "ray.h"


struct hit_record {
	double t;
	vec3 p;
	vec3 normal;
};

/* 
* A class for objects rays can hit.
*/
class hittable {
public:
	virtual bool hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const = 0;
};

#endif // !HITTABLEH</code></pre>

<hr>

<h2 id="front-faces-versus-back-faces">
<a class="anchor" href="#front-faces-versus-back-faces" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="front-faces-versus-back-faces"></a>Front Faces Versus Back Faces</h2>
<p>A question we have to ask ourselves about our normals is whether they should always point outward. Right now, the normal will always be in the <em>direction of the center to the intersection point</em> - outward. So if the ray intersects from the outside, the normal is against the ray. If the ray intersects from the inside (like in a glass ball), the normal would be pointing in the same direction of the ray. The alternative option is to have the normal always point against the ray.</p>

<p>If we decide to always have the normal point outward, we need to determine what side the ray is on when we color it. If they face the same direction, the ray is inside the object traveling outward. If they’re opposite, the ray is outside traveling inward. We can determine this by taking the dot product of the ray and the normal - if the dot is positive, the ray is inside traveling outward.</p>

<pre><code class="language-cpp">if (dot(ray_direction, outward_normal) &gt; 0.0) {
    // ray is inside the sphere
    ...
} else {
    // ray is outside the sphere
    ...
}</code></pre>

<p>Suppose we take the other option: always having the normals point against the ray. We would have to store what side of the surface the ray is on:</p>
<pre><code class="language-cpp">bool front_face;
if (dot(ray_direction, outward_normal) &gt; 0.0) {
    // ray is inside the sphere
    normal = -outward_normal;
    front_face = false;
}
else {
    // ray is outside the sphere
    normal = outward_normal;
    front_face = true;
}
</code></pre>

<p>You can choose whichever method you please, but Shirley’s book recommends the “outward” boolean method, as we will have more material types than geometric types for the time being.</p>

<p>Following the suggestion of Shirley, we’ll add a <code class="language-plaintext highlighter-rouge">front_face</code> boolean to the <code class="language-plaintext highlighter-rouge">hittable.h</code> <code class="language-plaintext highlighter-rouge">hit_record</code> struct, as well as a function to solve the calculation:</p>

<p><code class="language-plaintext highlighter-rouge">hittable.h</code>:</p>
<pre><code class="language-cpp">#ifndef HITTABLEH
#define HITTABLEH

#include "ray.h"


struct hit_record {
	double t; // parameter of the ray that locates the intersection point
	vec3 p; // intersection point
	vec3 normal;
	<span class="highlight-green">bool front_face;
	
	inline void set_face_normal(const ray&amp; r, const vec3&amp; outward_normal) {
    		front_face = dot(r.direction(), outward_normal) &lt; 0;
    		normal = front_face ? outward_normal :-outward_normal;
	} </span>
};

class hittable {
public: 
	virtual bool hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const = 0;
};

#endif // !HITTABLEH</code></pre>

<p>And now to update our sphere header with the simplified ray intersection and the outward normal calculations):</p>

<p><code class="language-plaintext highlighter-rouge">sphere.h:</code></p>
<pre><code class="language-cpp">#ifndef SPHEREH
#define SPHEREH

#include "hittable.h"

class sphere : public hittable {
public:
	sphere() {}
	sphere(vec3 cen, float r) : center(cen), radius(r) {};
	virtual bool hit(const ray&amp; r, double tmin, double tmax, hit_record&amp; rec) const;
	vec3 center;
	double radius;
};

bool sphere::hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const {
	vec3 oc = r.origin() - center; // Vector from center to ray origin
	double a = r.direction().length_squared();
	double halfB = dot(oc, r.direction());
	double c = oc.length_squared() - radius*radius;
	double discriminant = (halfB * halfB) - (a * c);
	if (discriminant &gt; 0.0) {
        auto root = sqrt(discriminant);

		auto temp = (-halfB - root)/a;

		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
<span class="highlight-green">vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);</span>
			return true;
		}
		temp = (-halfB + root / a;
		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
<span class="highlight-green">vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);</span>
			return true;
		}
	}
	return false;
}

#endif // !SPHEREH
</code></pre>

<p>As well as a new file for a list of hittable objects:</p>

<p><code class="language-plaintext highlighter-rouge">hittableList.h:</code></p>
<pre><code class="language-cpp">#ifndef HITTABLELISTH
#define HITTABLELISTH

#include "hittable.h"

class hittable_list : public hittable {
public:
	hittable_list() {}
	hittable_list(hittable** l, int n) { list = l; list_size = n; }
	virtual bool hit(const ray&amp; r, double tmin, double tmax, hit_record&amp; rec) const;
	hittable** list;
	int list_size;
};

bool hittable_list::hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const {
	hit_record temp_rec;
	bool hit_anything = false;
	double closest_so_far = t_max;
	for (int i = 0; i &lt; list_size; i++) {
		if (list[i]-&gt;hit(r, t_min, closest_so_far, temp_rec)) {
			hit_anything = true;
			closest_so_far = temp_rec.t;
			rec = temp_rec;
		}
	}
	return hit_anything;
}

#endif // !HITTABLELISTH</code></pre>

<p>And the modified <code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "sphere.h"
#include "hittableList.h"
#include "float.h"


/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
* Draw sphere and surface normals
*/
vec3 color(const ray&amp; r, hittable * world) {
	hit_record rec;
	if (world-&gt;hit(r, 0.0, DBL_MAX, rec)) {
		return 0.5 * vec3(rec.normal.x() + 1, rec.normal.y() + 1, rec.normal.z() + 1); // return a vector with values between 0 and 1 (based on xyz) to be converted to rgb values
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// The values below are derived from making the "camera"/ray origin coordinates (0, 0, 0) relative to the canvas. 
	// See the included file "TracingIllustration.png" for a visual representation.
	vec3 lower_left_corner(-2.0, -1.0, -1.0);
	vec3 horizontal(4.0, 0.0, 0.0);
	vec3 vertical(0.0, 2.0, 0.0);
	vec3 origin(0.0, 0.0, 0.0);

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			double u = double(i) / double(nx);
			double v = double(j) / double(ny);

			// Approximate pixel centers on the canvas for each ray r
			ray r(origin, lower_left_corner + u * horizontal + v * vertical);

			vec3 p = r.point_at_parameter(2.0);
			vec3 col = color(r, world);
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	std::cerr &lt;&lt; "\nDone.\n";
}
</code></pre>

<p>The result:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables.png" alt="Sphere hittables"></p>

<p>Stunning. A little bit jaggedy, though, don’t you think? This effect is known as “aliasing.” If you wanted to, you could increase the resolution of our scene for higher fidelity. Another interesting method for better image quality falls under the umbrella term “anti-aliasing.”</p>

<hr>

<h2 id="anti-aliasing">
<a class="anchor" href="#anti-aliasing" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="anti-aliasing"></a>Anti-Aliasing</h2>
<p>Anti-aliasing encompasses a whole slew of methods to combat “jaggies” - from multi-sampling to super-sampling, approximation(FXAA) to temporal, or - more recently - deep learning anti-aliasing. Each of these methods has pros and cons depending on the type of scene portrayed, performance targets, and even scene movement. Usually, there’s a trade-off between image quality and speed. We’ve entered a fascinating time for graphics where raw pixel count (True 4K this! Real 4K that!) is becoming less important - thanks to some incredible leaps in upscaling and anti-aliasing.</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/anti-aliasing.png" alt="Small sample of anti-aliasing methods"></p>

<p>If you want to learn more, I highly suggest watching <a href="https://www.youtube.com/watch?v=NbrA4Nxd8Vo">this video</a> from one of my favorite YouTube channels(<a href="https://www.youtube.com/user/DigitalFoundry">Digital Foundry</a>), or reading <a href="https://techguided.com/what-is-anti-aliasing/">this blog post</a> from techguided.com.</p>

<p>We’re going to be using multisample anti-aliasing (MSAA) in our ray tracer. As you may have supposed, multisampling, in this case, means taking multiple sub-pixel samples from each pixel and averaging the color across the whole pixel. Here’s an example - the raw triangle on the left, and the triangle with four samples per pixel on the right:</p>

<div class="captioned-image">
<span class="row">
![No MSAA](/assets/images/blog-images/path-tracer/the-first-weekend/no-msaa.png)
![MSAA 4x](/assets/images/blog-images/path-tracer/the-first-weekend/msaa.png)
</span>
[Source](https://developer.apple.com/documentation/metal/gpu_features/understanding_gpu_family_4/about_enhanced_msaa_and_imageblock_sample_coverage_control)
</div>

<p>Instead of taking perfectly spaced samples of pixels like in the example above, we’ll be taking random samples of pixels. For that, we’ll need a way of generating random numbers (you can do it however you please):</p>

<p><code class="language-plaintext highlighter-rouge">random.h:</code></p>
<pre><code class="language-cpp">#ifndef RANDOMH
#define RANDOMH

#include &lt;cstdlib&gt;

inline double random_double() {
    // Returns a random real in [0,1).
    return rand() / (RAND_MAX + 1.0);
}

inline double random_double(double min, double max) {
    // Returns a random real in [min,max).
    return min + (max-min)*random_double();
}

#endif // !RANDOMH</code></pre>

<h3 id="adding-anti-aliasing-to-the-camera">
<a class="anchor" href="#adding-anti-aliasing-to-the-camera" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="adding-anti-aliasing-to-the-camera"></a>Adding Anti-Aliasing to the Camera</h3>

<p>Next, we’ll create a camera class to manage the virtual camera and scene sampling:</p>

<pre><code class="language-cpp">#ifndef CAMERAH
#define CAMERAH

#include "ray.h"

class camera {
public:

	// The values below are derived from making the "camera" / ray origin coordinates(0, 0, 0) relative to the canvas.
	camera() {
		lower_left_corner = vec3(-2.0, -1.0, -1.0);
		horizontal = vec3(4.0, 0.0, 0.0);
		vertical = vec3(0.0, 2.0, 0.0);
		origin = vec3(0.0, 0.0, 0.0);
	}
	ray get_ray(double u, double v) { return ray(origin, lower_left_corner + u * horizontal + v * vertical - origin); }

	vec3 origin;
	vec3 lower_left_corner;
	vec3 horizontal;
	vec3 vertical;
};

#endif // !CAMERAH</code></pre>

<p>And our resulting main method:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "sphere.h"
#include "hittableList.h"
#include "camera.h"
#include "random.h"


/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
*/
vec3 color(const ray&amp; r, hittable * world) {
	hit_record rec;
	if (world-&gt;hit(r, 0.0, DBL, rec)) {
		return 0.5 * vec3(rec.normal.x() + 1, rec.normal.y() + 1, rec.normal.z() + 1); // return a vector with values between 0 and 1 (based on xyz) to be converted to rgb values
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {

	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	int ns = 100; // Number of samples for each pixel for anti-aliasing
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 1)) / double(nx);
				double v = (j + random_double(0.0, 1)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				col += color(r, world);
			}

			col /= double(ns); // Average the color between objects/background
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>Keep in mind - these images are only 200x100.
The difference is clear. And blurry. Haha:</p>

<p><span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables.png" alt="Sphere hittables">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables-msaa.png" alt="Sphere hittables">
</span>
<span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables-zoom.png" alt="Sphere hittables">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables-msaa-zoom.png" alt="Sphere hittables">
</span></p>

<hr>

<h2 id="diffuse-materials">
<a class="anchor" href="#diffuse-materials" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="diffuse-materials"></a>Diffuse Materials</h2>

<p>Our ball is pretty, but lacks texture. Let’s add diffuse materials!</p>

<p>Diffuse materials reflect light from their surface such that an incident ray is scattered ay many angles, rather than just one (which is the case with specular reflection):</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/diffuse.png" alt="Diffuse reflection">
<em>An example of light reflecting off of a diffuse surface (<a href="https://en.wikipedia.org/wiki/Diffuse_reflection">source</a>)</em>
</span></p>

<p>From Wikipedia:</p>
<blockquote>
  <p>The visibility of objects, excluding light-emitting ones, is primarily caused by diffuse reflection of light: it is diffusely-scattered light that forms the image of the object in the observer’s eye.</p>
</blockquote>

<p>Diffuse materials also modulate the color of their surroundings with their intrinsic color. In our ray tracer, we’re going to simulate diffuse materials by randomizing ray reflections upon hitting a diffuse object. For example, if we were to shoot three rays into the space between two diffuse surfaces, we might see a result like this:</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/diffuse.png" alt="Diffuse reflection">
<em>How rays might behave in our ray tracer upon hitting a diffuse surface (<a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">source</a>)</em></span></p>

<p>In addition to being reflected, some rays could also be absorbed. Naturally, the darker the surface of a given object, the more likely absorption will take place… which is why that object looks dark. Take Vantablack, one of the darkest substances known. It’s made up of carbon nanotubes, and is essentially a very fine shag carpet. Light gets lost (or diffused) within this forest of tubes to create a pretty striking diffuse material:</p>

<p><span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/vantablack-zoom.png" alt="Vantablack">
<!-- *[source](https://en.wikipedia.org/wiki/Vantablack)* -->
</span>
<span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/vantablack.png" alt="Vantablack">
<!-- *[source](https://www.techbriefs.com/component/content/article/tb/supplements/pit/features/applications/27558)* -->
</span></p>

<p>When I was originally reading Peter Shirley’s guide, he described an incorrect (but close) approximation of ideal Lambertian reflectance(think unfinished wood or charcoal - no shiny specular highlights). We’ll go through how I originally did it, and then modify the code to make matte surfaces more true-to-life, thanks to an update to his book.</p>

<h3 id="the-math-of-diffuse-materials">
<a class="anchor" href="#the-math-of-diffuse-materials" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="the-math-of-diffuse-materials"></a>The Math of Diffuse Materials</h3>

<p>First of all, we need to form a unit sphere tangent to the hitpoint <strong>p</strong> on the scene object. The center of this sphere will be the coordinates at the end of the surface normal <strong>n</strong>. Be aware that there are two spheres tangential to the collided sphere - one inside the object(<strong>p</strong> - <strong>n</strong>), and one outside(<strong>p</strong> + <strong>n</strong>). we’ll pick the tangent sphere that’s on the same side of the surface as the ray origin. Next, we’ll select a random point <strong>s</strong> in the tangent unit sphere and send a ray from the hit point <strong>p</strong> to the random point <strong>s</strong> - which results in the vector <strong>s</strong> - <strong>p</strong>.</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/ray-tracing-diffuse.png" alt="Diffuse material illustration">
<em>Generation of random diffuse bounce ray (<a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">source</a>)</em>
</span></p>

<p>Now we need a way to pick the aforementioned random point <strong>s</strong>. Following Shirley’s lead, we’ll use a rejection method; picking a random point in the unit cube where x, y, and z all range from -1 to 1. If the point is outside the sphere (x<sup>2</sup> + y<sup>2</sup> + z<sup>2</sup> &gt; 1), we reject it and try again:</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">vec3 random_unit_sphere_coordinate() {
	vec3 p;
	do {
		p = 2.0 * vec3(random_double(0, 1), random_double(0, 1), random_double(0, 1)) - vec3(1, 1, 1);
	} while (p.squared_length() &gt;= 1.0);
	return p;
}</code></pre>

<p>Now we have to update our <code class="language-plaintext highlighter-rouge">color</code> function to use the random coordinates:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">vec3 color(const ray&amp; r, hittable * world) {
	hit_record rec;
	// Light that reflects off a diffuse surface has its direction randomized.
	// Light may also be absorbed.
	if (world-&gt;hit(r, 0.0, DBL_MAX, rec)) {
		<span class="highlight-green">
		vec3 target = rec.p + rec.normal + random_unit_sphere_coordinate(); 
		return 0.5 * color(ray(rec.p, target - rec.p), world); // light is absorbed continually by the sphere or reflected into the world.
		</span>
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}
...</code></pre>

<p>Notice that our new code is recursive and will only stop recursing when the ray fails to hit any object. In some scenes (or some unlucky sequences of random numbers), this could wreak havoc on performance. For that reason, we’ll enforce a bounce limit:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code></p>
<pre><code class="language-cpp"><span class="highlight-green"> vec3 color(const ray&amp; r, hittable * world, int depth) {</span>
	hit_record rec;

<span class="highlight-green">	if (depth &lt;= 0)
        return vec3(0,0,0); // Bounce limit reached - return darkness</span>

	// Light that reflects off a diffuse surface has its direction randomized.
	// Light may also be absorbed.
	if (world-&gt;hit(r, 0.0, DBL_MAX, rec)) {
		<span class="highlight-green">	vec3 target = rec.p + rec.normal + random_unit_sphere_coordinate();

	// light is absorbed continually by the sphere or reflected into the world.
	return 0.5 * color(ray(rec.p, target - rec.p), world, depth-1);</span>

	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {

	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	int ns = 100; // Number of samples for each pixel for anti-aliasing
	<span class="highlight-green">	int maxDepth = 50; // Bounce limit</span>
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 1)) / double(nx);
				double v = (j + random_double(0.0, 1)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				<span class="highlight-green">
				col += color(r, world, maxDepth);
				</span>
			}

			col /= double(ns); // Average the color between objects/background
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>The result:
<span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse.png" alt="Diffuse sphere">
<em>Diffuse sphere</em>
</span></p>

<h3 id="gamma-correction">
<a class="anchor" href="#gamma-correction" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="gamma-correction"></a>Gamma Correction</h3>

<p>Our spheres are reflecting 50% of each bounce, so why is our picture so dark? Most image viewers assume images to be “gamma-corrected.” Ours is not. Here’s an explanation of gamma correction from <a href="https://en.wikipedia.org/wiki/Gamma_correction">Wikipedia</a>:</p>

<blockquote>
  <p>Gamma correction, or often simply gamma, is a nonlinear operation used to encode and decode luminance or tristimulus values in video or still image systems.</p>
</blockquote>

<p>You can read more about gamma correction <a href="https://www.cambridgeincolour.com/tutorials/gamma-correction.htm">here</a> if you feel so compelled.</p>

<p>So basically, we need to transform our values before storing them. For a start, we’ll use gamma 2 - which would mean raising the colors to the power of 1/<em>gamma</em> (or.5) - mathematically identical to the square root:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">...

int main() {

	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	int ns = 10; // Number of samples for each pixel for anti-aliasing
	int maxDepth = 50; // Bounce limit
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 1)) / double(nx);
				double v = (j + random_double(0.0, 1)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				col += color(r, world, maxDepth);
			}

			col /= double(ns); // Average the color between objects/background
<span class="highlight-green">			col = vec3(sqrt(col[0]), sqrt(col[1]), sqrt(col[2]));  // set gamma to 2</span>
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-gamma.png" alt="Diffuse sphere with gamma correction">
</span></p>

<h3 id="shadow-acne">
<a class="anchor" href="#shadow-acne" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="shadow-acne"></a>Shadow Acne</h3>

<p>There’s one small issue left to fix, known as shadow acne. Some of the rays hit the sphere (or any object, really) not at t = 0, but rather at something like t = ±0.0000001 due to floating-point approximation. In that case, we’ll just change our hit detection specs in <code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>

<pre><code class="language-cpp">if (world.hit(r, 0.001, DBL_MAX, rec)) {</code></pre>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-shadow-acne.png" alt="Shadow acne sphere">
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png" alt="Sphere no shadow acne">
  </div>
</div>
  (Mouseover) Fix shadow acne
</div>

<p>You can view the images separately, as well. Here’s <a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-shadow-acne.png">the one with shadow acne</a> and <a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png">the one without</a>.</p>

<h3 id="true-lambertian-reflection">
<a class="anchor" href="#true-lambertian-reflection" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="true-lambertian-reflection"></a>True Lambertian Reflection</h3>
<p>Recall that Lambertian reflectance is the “ideal” matte surface - the apparent brightness of a Lambertian surface to an observer is the same regardless of the observer’s angle of view.</p>

<p>Here’s Shirley’s explanation of the implementation of true Lambertian reflectance:</p>

<blockquote>
  <p>The rejection method presented here produces random points in the unit ball offset along the surface normal. This corresponds to picking directions on the hemisphere with high probability close to the normal, and a lower probability of scattering rays at grazing angles. This distribution scales by the cos3(ϕ) where ϕ is the angle from the normal. This is useful since light arriving at shallow angles spreads over a larger area, and thus has a lower contribution to the final color.
However, we are interested in a Lambertian distribution, which has a distribution of cos(ϕ). True Lambertian has the probability higher for ray scattering close to the normal, but the distribution is more uniform. This is achieved by picking points on the surface of the unit sphere, offset along the surface normal. Picking points on the sphere can be achieved by picking points in the unit ball, and then normalizing those.</p>
</blockquote>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/rand-unit-vector.png" alt="Generation of random unit vector"></p>

<p>And our total replacement for <code class="language-plaintext highlighter-rouge">random_unit_sphere_coordinate()</code>:
(use 3.14 as pi for now, we’ll address it in the next section)</p>

<pre><code class="language-cpp">vec3 random_unit_vector() {
    auto a = random_double(0, 2*pi);
    auto z = random_double(-1, 1);
    auto r = sqrt(1 - z*z);
    return vec3(r*cos(a), r*sin(a), z);
}</code></pre>

<p>The result:</p>
<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png" alt="Lambertian approximation">
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/lambertian.png" alt="True lambertian reflection">
  </div>
</div>
  (Mouseover)True lambertian reflectance
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png">Lambertian approximation</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/lambertian.png">True Lambertian</a></li>
</ul>

<p>It’s a subtle difference, but a difference nonetheless. Notice that the shadows are not as pronounced and that both spheres are lighter.</p>

<p>These changes are both due to the more uniform scattering toward the normal. For diffuse objects, they appear lighter because more light bounces toward the camera. For shadows, less light bounces straight up.</p>

<hr>

<h2 id="common-constants-and-utilities">
<a class="anchor" href="#common-constants-and-utilities" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="common-constants-and-utilities"></a>Common Constants and Utilities</h2>

<p>You may have noticed in <code class="language-plaintext highlighter-rouge">random_unit_vector()</code> that <em>pi</em> is not defined. That’s because in Shirley’s newer edition, he creates a general main header file with some constants and utilities:</p>

<pre><code class="language-cpp">#ifndef RTWEEKEND_H
#define RTWEEKEND_H

#include &lt;cmath&gt;
#include &lt;cstdlib&gt;
#include &lt;limits&gt;
#include &lt;memory&gt;


// Usings
using std::shared_ptr;
using std::make_shared;
using std::sqrt;

// Constants
const double infinity = std::numeric_limits&lt;double&gt;::infinity();
const double pi = 3.1415926535897932385;

// Utility Functions
inline double degrees_to_radians(double degrees) {
    return degrees * pi / 180;
}

// Common Headers
#include "ray.h"
#include "vec3.h"

#endif
</code></pre>

<p>And uses it in <code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;cfloat&gt;

<span class="highlight-green">
#include "rtweekend.h"
</span>

#include "sphere.h"
#include "hittableList.h"
#include "camera.h"
#include "random.h"

vec3 random_unit_vector() {
	auto a = random_double(0, 2*pi);
    auto z = random_double(-1, 1);
    auto r = sqrt(1 - z*z);
    return vec3(r*cos(a), r*sin(a), z);
}

/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
* Draw sphere and surface normals
*/
vec3 color(const ray&amp; r, hittable * world, int depth) {
	hit_record rec;

	if (depth &lt;= 0)
        return vec3(0,0,0); // Bounce limit reached - return darkness

<span class="highlight-green">	if (world-&gt;hit(r, 0.001, infinity, rec)) { </span>
		vec3 target = rec.p + rec.normal + random_unit_vector(); 
		return 0.5 * color(ray(rec.p, target - rec.p), world, depth-1); // light is absorbed continually by the sphere or reflected into the world.
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {

	int nx = 1600; // Number of horizontal pixels
	int ny = 800; // Number of vertical pixels
	int ns = 10; // Number of samples for each pixel for anti-aliasing
	int maxDepth = 50; // Bounce limit

	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	        std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;

		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 0.999)) / double(nx);
				double v = (j + random_double(0.0, 0.999)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				col += color(r, world, maxDepth);
			}

			col /= double(ns); // Average the color between objects/background
			col = vec3(sqrt(col[0]), sqrt(col[1]), sqrt(col[2]));  // set gamma to 2
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
}</code></pre>

<p>While we’re here making changes, let’s clean things up a bit by:</p>
<ul>
  <li>Moving <code class="language-plaintext highlighter-rouge">random_unit_vector()</code> to <code class="language-plaintext highlighter-rouge">vec3.h</code>
</li>
  <li>Moving our <code class="language-plaintext highlighter-rouge">random number generator</code> to <code class="language-plaintext highlighter-rouge">rtweekend.h</code>
</li>
</ul>

<hr>

<h2 id="metal">
<a class="anchor" href="#metal" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="metal"></a>Metal</h2>

<h3 id="abstract-class-for-materials">
<a class="anchor" href="#abstract-class-for-materials" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="abstract-class-for-materials"></a>Abstract Class for Materials</h3>
<p>We’re going to use an abstract material class that encapsulates behavior which will do two things:</p>
<ul>
  <li>Produce a scattered ray (or absorb the incident ray)</li>
  <li>Determine attenuation (reduction of magnitude) of a scattered ray</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">#ifndef MATERIAL_H
#define MATERIAL_H

class material {
    public:
        virtual bool scatter(
            const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered
        ) const = 0;
};

#endif</code></pre>

<h3 id="describing-ray-object-intersections">
<a class="anchor" href="#describing-ray-object-intersections" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="describing-ray-object-intersections"></a>Describing Ray-Object Intersections</h3>
<p>The <code class="language-plaintext highlighter-rouge">hit_record</code> struct in <code class="language-plaintext highlighter-rouge">hittable.h</code> is where we’ll be storing whatever information we want about hits. We’ll be adding material to the struct.</p>

<p><code class="language-plaintext highlighter-rouge">hittable.h</code>:</p>
<pre><code class="language-cpp">#ifndef HITTABLEH
#define HITTABLEH

#include "ray.h"

<span class="highlight-green">class material; // forward declaration</span>

struct hit_record {
	double t; // parameter of the ray that locates the intersection point
	vec3 p; // intersection point
	vec3 normal;
	bool front_face;
<span class="highlight-green">	material* material_ptr;</span>

	inline void set_face_normal(const ray&amp; r, const vec3&amp; outward_normal) {
        front_face = dot(r.direction(), outward_normal) &lt; 0;
        normal = front_face ? outward_normal : -outward_normal;
    }
};

/* 
* A class for objects rays can hit.
*/
class hittable {
public: 
	virtual bool hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const = 0;
};

#endif // !HITTABLEH</code></pre>

<p>When a ray hits a surface, the material pointer within the hit struct will point to the material the object was instantiated with. As such, we’ll have to reference the material within our sphere class to be included with the <code class="language-plaintext highlighter-rouge">hit_record</code>.</p>

<p><code class="language-plaintext highlighter-rouge">sphere.h:</code></p>
<pre><code class="language-cpp">#ifndef SPHEREH
#define SPHEREH

#include "hittable.h"

class sphere : public hittable {
public:
	sphere() {}
<span class="highlight-green">	sphere(vec3 cen, float r, material* material) : center(cen), radius(r), material_ptr(material) {};</span>
	virtual bool hit(const ray&amp; r, double tmin, double tmax, hit_record&amp; rec) const;
	vec3 center;
	double radius;
<span class="highlight-green">	material* material_ptr;</span>
};

bool sphere::hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const {
	vec3 oc = r.origin() - center; // Vector from center to ray origin
	double a = r.direction().length_squared();
	double halfB = dot(oc, r.direction());
	double c = oc.length_squared() - radius*radius;
	double discriminant = (halfB * halfB) - (a * c);
	if (discriminant &gt; 0.0) {
        auto root = sqrt(discriminant);

		auto temp = (-halfB - root) / a;

		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
			vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);
<span class="highlight-green">			rec.material_ptr = material_ptr;</span>
			return true;
		}
		temp = (-halfB + root) / a;
		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
			vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);
<span class="highlight-green">			rec.material_ptr = material_ptr;</span>
			return true;
		}
	}
	return false;
}

#endif // !SPHEREH</code></pre>

<h3 id="light-scatter">
<a class="anchor" href="#light-scatter" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="light-scatter"></a>Light Scatter</h3>
<p>The Lambertian material we modeled previously would either scatter and attenuate by its reflectance <em>R</em>, or scatter with no attenuation but absorb 1 - <em>R</em> of rays, or somewhere in between. This is represented in code as follows:</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code></p>
<pre><code class="language-cpp">...
class lambertian : public material {
    public:
        lambertian(const vec3&amp; a) : albedo(a){};
        virtual bool scatter(const ray&amp; ray_in,
                            const hit_record&amp; rec,
                            vec3&amp; attenuation,
                            ray&amp; scattered) const {
            vec3 scatter_direction = rec.p + rec.normal + random_unit_vector();
            scattered = ray(rec.p, target - rec.p);
            attenuation = albedo;
            return true;
        }
    vec3 albedo; // reflectivity

};
...</code></pre>

<h3 id="metal-reflection">
<a class="anchor" href="#metal-reflection" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="metal-reflection"></a>Metal Reflection</h3>
<p>Metal is definitely NOT Lambertian - here’s a simple sketch depecting a general mirrored reflection:</p>

<p><span class="captioned-image"> <img src="/assets/images/blog-images/path-tracer/the-first-weekend/metal-reflect.png" alt="Mirrored Reflection"><em>Metal Reflection (<a href="http://viclw17.github.io/2018/07/30/raytracing-reflecting-materials">source</a>)</em></span></p>

<div class="math-block">
$$
\vec r = \vec v - (-2 * \vert \vec a\vert  * \vec n)
$$

where 

$$
\vert \vec a\vert  = \vert \vec v\vert  * cos(\theta)
$$

since 

$$
dot(\vec v, \vec n) = \vert \vec v\vert \vert \vec n\vert cos(\pi - \theta) = -\vert \vec v\vert cos(\theta)
$$

that means

$$
\vert \vec a\vert  = -dot(\vec v, \vec n)
$$

and

$$
\vec r = \vec v - (2 * dot(\vec v, \vec n) * \vec n)
$$
</div>

<p>In other words, the reflected ray is v + 2a. N is a unit vector, but that might not be the case for v. Also, because v points inward, we’re going have to flip it by negating it. This yields the following formula:</p>

<pre><code class="language-cpp">vec3 reflect(const vec3&amp; v, const vec3&amp; n){
    return v - 2*dot(v,n)*n;
}</code></pre>

<p>We can go ahead and incorporate this formula into our metal material:</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code></p>
<pre><code class="language-cpp">vec3 reflect(const vec3&amp; v, const vec3&amp; n){
    return v - 2*dot(v,n)*n; // v enters the hittable, which is why subtraction is required.
}

class material {
    public:
    virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const = 0;
};

// Matte surface
// Light that reflects off a diffuse surface has its direction randomized.
// Light may also be absorbed. See Diffuse.png for illustration and detailed description
class lambertian : public material {
    public:
        lambertian(const vec3&amp; a) : albedo(a){};
        virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const {
            vec3 target = rec.p + rec.normal + random_unit_sphere_coordinate();
            scattered = ray(rec.p, target - rec.p);
            attenuation = albedo;
            return true;
        }
    vec3 albedo; // reflectivity

};

class metal : public material {
    public:
        metal(const vec3&amp; a) : albedo(a) {}
        virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const {
        vec3 reflected = reflect(unit_vector(ray_in.direction()), rec.normal);
        scattered = ray(rec.p, reflected);
        attenuation = albedo;
        return dot(scattered.direction(), rec.normal) &gt; 0.0;
    }

    vec3 albedo;
};</code></pre>

<p>And of course, we’re going to need to update our <code class="language-plaintext highlighter-rouge">color</code>() function to use our new material:
<code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">vec3 color(const ray&amp; r, hittable *world, int depth) {
    hit_record rec;

    if (depth &lt;= 0) {
        return vec3(0,0,0);
    }  
    if (world-&gt;hit(r, 0.001, DBL_MAX, rec)) {
        ray scattered;
        vec3 attenuation;
		// Scatter formulas vary based on material
        if (rec.material_ptr-&gt;scatter(r, rec, attenuation, scattered)) {
            return attenuation*color(scattered, world, depth-1);
        }
        else {
            return vec3(0,0,0);
        }
    }
    else {
        vec3 unit_direction = unit_vector(r.direction());
        double t = 0.5*(unit_direction.y() + 1.0);
        return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
    }
}</code></pre>

<h3 id="adding-metal-spheres-to-the-scene">
<a class="anchor" href="#adding-metal-spheres-to-the-scene" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="adding-metal-spheres-to-the-scene"></a>Adding Metal Spheres to the Scene</h3>

<p>Now that we’ve got some shiny new spheres, let’s add ‘em to the scene, render ‘em, and check ‘em out:
<code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">int main {
	...

	hittable *list[4];
		hittable *list[4];
    	list[0] = new sphere(vec3(0,0,-1), 0.5, new lambertian(vec3(0.8, 0.3, 0.3)));
		list[1] = new sphere(vec3(0,-100.5,-1), 100, new lambertian(vec3(0.8, 0.8, 0.0)));
		list[2] = new sphere(vec3(1,0,-1), 0.5, new metal(vec3(0.8, 0.6, 0.2)));
		list[3] = new sphere(vec3(-1,0,-1), 0.5, new metal(vec3(0.8, 0.8, 0.8)));
		hittable *world = new hittable_list(list,4);

		camera cam(lookFrom, lookAt, vec3(0,1,0), 20,double(nx)/double(ny), aperture, distToFocus);	

		auto start = std::chrono::high_resolution_clock::now();


		for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas

		...</code></pre>

<p>You’ll get something like this:</p>

<p><span class="captioned-image"><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal.png" alt="Metal and Lambertian spheres"><em>Metal and Lambertian spheres</em></span></p>

<p>Feel free to mess around with the color, positioning, and material, as well:</p>

<p><span class="captioned-image"><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal-edit.png" alt="Metal spheres"><em>Your new album cover</em></span></p>

<h3 id="fuzzy-metal">
<a class="anchor" href="#fuzzy-metal" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="fuzzy-metal"></a>Fuzzy Metal</h3>

<p>In addition to perfectly polished metal spheres, we can simulate rough metal as well, with some “fuzziness.” To do so, we just need to append a random vector to the reflected rays:</p>

<p><span class="captioned-image"><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/reflect-fuzzy.png" alt="Fuzzy metal reflections"><em>Generating fuzzy reflections</em> (<a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html"><em>source</em></a>)</span></p>

<p>The larger the sphere, the fuzzier the reflections will be. If the sphere is too large, we may scatter below the surface of an object. If that happens, we can just have the surface absorb those rays.</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">class metal : public material {
    public:
        metal(const vec3&amp; a, double f) : albedo(a) {
			if (f&lt;1) fuzz = f; else fuzz = 1;
        }
        virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const {
        vec3 reflected = reflect(unit_vector(ray_in.direction()), rec.normal);
        scattered = ray(rec.p, reflected);
        attenuation = albedo;
        return dot(scattered.direction(), rec.normal) &gt; 0.0;
    }

    vec3 albedo;
	double fuzz;
};</code></pre>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/all-metal-no-fuzz.png" alt="Metal - no fuzz">
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal-fuzz.png" alt="Fuzzy Metal">
  </div>
</div>
(Mouseover) Fuzziness from left to right: .5, 0, and 1
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/all-metal-no-fuzz.png">No fuzz</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal-fuzz.png">Fuzz</a></li>
</ul>

<hr>

<h2 id="dielectrics">
<a class="anchor" href="#dielectrics" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="dielectrics"></a>Dielectrics</h2>
<p>Dielectrics are materials like glass. When a light ray hits one, the ray splits into a reflected ray and a refracted ray. In this path tracer, we’ll be randomly choosing which ray to simulate, only generating one ray per interaction.</p>

<h3 id="refraction">
<a class="anchor" href="#refraction" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="refraction"></a>Refraction</h3>
<p>Refraction is the deflection of a ray from a straight path due to passing obliquely from one medium to another.</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/refraction.png" alt="Refraction">
Notice both the reflected beam (top right) and the refracted beam (bottom right) (<a href="https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/F%C3%A9nyt%C3%B6r%C3%A9s.jpg/1200px-F%C3%A9nyt%C3%B6r%C3%A9s.jpg">source</a>)
</span></p>

<p>The refractive index describes the angle that light propagates through different mediums and is defined as:</p>

\[n={\frac {c}{v}}\]

<p>where <em>c</em> is the speed of light in a vacuum and v is the speed of light in the medium.</p>

<p>For reference, here are some refractive indices:</p>

<table>
  <thead>
    <tr>
      <th>Material</th>
      <th>Refractive Index</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Vacuum</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Air</td>
      <td>1.000293</td>
    </tr>
    <tr>
      <td>Carbon Dioxide</td>
      <td>1.001</td>
    </tr>
    <tr>
      <td>Ice</td>
      <td>1.31</td>
    </tr>
    <tr>
      <td>Water</td>
      <td>1.333</td>
    </tr>
    <tr>
      <td>Kerosene</td>
      <td>1.39</td>
    </tr>
    <tr>
      <td>Vegetable Oil</td>
      <td>1.47</td>
    </tr>
    <tr>
      <td>Window Glass</td>
      <td>1.52</td>
    </tr>
    <tr>
      <td>Amber</td>
      <td>1.55</td>
    </tr>
    <tr>
      <td>Diamond</td>
      <td>2.417</td>
    </tr>
    <tr>
      <td>Germanium</td>
      <td>4.05</td>
    </tr>
  </tbody>
</table>

<h3 id="snells-law">
<a class="anchor" href="#snells-law" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="snells-law"></a>Snell’s Law</h3>
<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/snells-law.svg" alt="Snell's Law"></p>

<p>Snell’s law states that the ratio of the sines of the angles of incidence and refraction is equivalent to the ratio of phase velocities in the two media, or equivalent to the reciprocal of the ratio of the indices of refraction:</p>

\[{\frac {\sin \theta _{2}}{\sin \theta _{1}}}={\frac {v_{2}}{v_{1}}}={\frac {n_{1}}{n_{2}}}\]

<p>with each θ as the angle measured from the normal of the boundary, v as the velocity of light in the respective medium, n as the refractive index (which is unitless) of the respective medium.</p>

<p>If we render a dielectric object with <code class="language-plaintext highlighter-rouge">ref_index</code> in a vacuum, (since the refractive index is 1 in a vacuum) we get this:</p>

\[\frac {n_{1}}{n_{2}} = \frac {1}{ref\_index}\]

<p>and when the ray shoots back out into the vacuum:</p>

\[\frac {n_{1}}{n_{2}} = {ref\_index}\]

<h3 id="total-internal-reflection">
<a class="anchor" href="#total-internal-reflection" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="total-internal-reflection"></a>Total Internal Reflection</h3>
<p>Total internal reflection is an optical phenomenon that occurs when light travels from a medium with a higher refractive index to a lower one, and the angle of incidence is greater than a certain angle (known as the “critical angle”).</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/total-internal-reflection.svg" alt="Total internal reflection" style="background: wheat;"></p>
<h3 id="calculating-the-refraction-vector">
<a class="anchor" href="#calculating-the-refraction-vector" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="calculating-the-refraction-vector"></a>Calculating the Refraction Vector</h3>
<p>Now that we’re familiar with the components of refraction, we can calculate the refraction vector geometrically.</p>

<!-- 
![Refraction geometry](/assets/images/blog-images/path-tracer/the-first-weekend/refraction-geometry.png)

We already know [Snell's law](#snells-law):

$$
{\frac {\sin \theta _{2}}{\sin \theta _{1}}}={\frac {v_{2}}{v_{1}}}={\frac {n_{1}}{n_{2}}}
$$

To determine the direction of the refracted ray, we'll have to solve for $\sin\theta_2$.

So we'll re-arrange Snell's law:

$$
\sin\theta_2 = \frac{n_1}{n_2} \cdot \sin\theta
$$

On the side of the surface with the refracted ray, there's a refracted ray $\vec R_2$ and a normal $n_2$, with an angle $theta_2$ between them. What we can do is split $R_2$ into components that are parallel, and components that are perpendicular to $n_2$.

$$
\mathbf{R_2} = \mathbf{R_2}_{\parallel} + \mathbf{R_2}_{\bot}
$$

Solving for $\mathbf{R_2}_{\parallel}$:

$$

$$ -->

<p>We can model the relationships of the vectors with a unit circle to make things easier.</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/refraction-vector-circle.png" alt="Refraction unit circle">
(<em><a href="http://viclw17.github.io/2018/08/05/raytracing-dielectric-materials">source</a></em>)
</span></p>

\[\vec r = \vec A + \vec B\]

<p>We’ll project $\vec r$ onto $\vec M$ to get $\vec A$:</p>

\[\vec A = sin\theta_{2} \cdot \vec M\]

<p>Similarly, we’ll project $\vec r$ onto $- \vec N$ to get $\vec B$:</p>

\[\vec B = cos\theta_{2} \cdot -\vec N\]

<p>We’ll define $\vec M$ as perpendicular($\bot$) to $\vec N$:</p>

\[\vec M = Normalize(\vec C + \vec v) = \frac {(\vec C + \vec v)}{sin\theta_{1}}\]

<p>And we’ll project $\vec r$ onto $- \vec N$ to get $\vec C$:</p>

\[\vec C = cos\theta_{1} \cdot \vec N\]

<p>So, with terms expanded, we go from:</p>

\[\vec r = \vec A + \vec B\]

<p>to:</p>

\[\vec r = sin\theta_{2} \cdot \vec M - cos\theta_{2} \cdot \vec N\]

<p>If we expand and re-arrange the equation, we’ll end up with this:</p>

\[\vec r = \frac {n_{1}}{n_{2}} \cdot (\vec v + cos\theta_{1} \cdot \vec N) - cos\theta_{2} \cdot \vec N\]

<p>After normalizing incidence ray direction $\vec v$ , we can calculate $cos\theta_{1}$ by $dot(\vec v, \vec n) = \vert \vec v\vert \vert \vec n\vert cos\theta_{1} = cos\theta_{1}$.</p>

<p>Since Snell’s law can also be interpreted as:</p>

\[sin\theta_{2} = \frac {n_{1}}{n_{2}} \cdot sin\theta_{1},\]

\[cos^2\theta_{2} = 1 - sin^2\theta_{2} = 1 - \frac {(n_1)^2}{(n_2)^2} \cdot sin^2\theta_{1} = 1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - cos^2\theta_{1}) = 1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - dot(\vec v, \vec n))\]

<p>and the equation can be written as:</p>

\[\vec r = \frac {n_{1}}{n_{2}} \cdot (\vec v - dot(\vec v, \vec n) \cdot \vec N) - \sqrt{1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - dot(\vec v, \vec n))} \cdot \vec N\]

<p>Which means that:</p>

\[cos^2\theta_{2} = 1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - dot(\vec v, \vec n))\]

<p>is the discriminant.</p>

<table>
  <thead>
    <tr>
      <th>Discriminant</th>
      <th>Ray Behavior</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$&lt;0$</td>
      <td>Total internal reflection</td>
    </tr>
    <tr>
      <td>$=0$</td>
      <td>Boundary of total reflection - no resultant ray</td>
    </tr>
    <tr>
      <td>$&gt;0$</td>
      <td>Refracted ray $\vec r$</td>
    </tr>
  </tbody>
</table>

<h3 id="coding-the-refraction-vector">
<a class="anchor" href="#coding-the-refraction-vector" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="coding-the-refraction-vector"></a>Coding the Refraction Vector</h3>

<ul>
  <li>$\frac {n_{1}}{n_{2}}$ is <code class="language-plaintext highlighter-rouge">n1_over_n2</code>
</li>
  <li>v is the incidence ray</li>
  <li>n is the surface normal</li>
  <li>refracted is the refracted ray’s direction</li>
</ul>

<pre><code class="language-cpp">bool refract(const vec3&amp; v, const vec3&amp; n, float n1_over_n2, vec3&amp; refracted) {
    vec3 uv = unit_vector(v);
    float dt = dot(uv, n);
    float discriminat = 1.0 - ni_over_nt * ni_over_nt * (1-dt*dt);
    if(discriminat &gt; 0){
        refracted = ni_over_nt * (uv-n*dt) - n*sqrt(discriminat);
        return true;
    }
    else
        return false; // no refracted ray
}</code></pre>

<h3 id="dielectric-reflections">
<a class="anchor" href="#dielectric-reflections" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="dielectric-reflections"></a>Dielectric Reflections</h3>
<p>When light strikes a dielectric object, both reflection and refraction may occur. Looking at a puddle at a sharp enough angle makes it look almost like a mirror!</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/fresnel.png" alt="Fresnel puddle">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/partial-transmittance.gif" alt="Patrial transmittance"></p>

<p>For low-precision applications that don’t involve polarized light, <a href="https://en.wikipedia.org/wiki/Schlick%27s_approximation">Schlick’s approximation</a> will serve our purposes just fine, rather than computing the effective reflection coefficient for every angle.</p>

<p>Schlick’s model states that the specular reflection coefficient can be approximated by:</p>

<p>\(R(\theta_{1})=R_{0}+(1-R_{0})(1-\cos \theta_{1} )^{5}\)
\(R_{0}=\left({\frac {n_{1}-n_{2}}{n_{1}+n_{2}}}\right)^{2}\)</p>

<ul>
  <li>$\theta_1$ is the incident angle.</li>
  <li>$n_1$ and $n_2$ are the refractive indices of the two media.</li>
  <li>$R_0$ is the reflection coefficient for light incoming parallel to the normal.</li>
</ul>

<p>The refractive index of air(1.000293) is often approximated as 1.</p>

\[\frac {n_{1}}{n_{2}} = \frac {1}{n_{dielectric}} \Rightarrow {n_{dielectric}} = \frac {n_{2}}{n_{1}}\]

\[cos\theta_{1} = dot(\vec v, \vec n)\]

<p>We can add Schlick’s approximation to <code class="language-plaintext highlighter-rouge">material.h</code></p>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">float schlick(float cosine, float ref_idx) {
    float r0 = (1 - ref_index) / (1 + ref_index); // ref_index = n2/n1
    r0 = r0 * r0;
    return r0 + (1 - r0) * pow((1 - cosine), 5);
}</code></pre>

<p>If the incident ray produces a refraction ray (which we can check by seeing if <code class="language-plaintext highlighter-rouge">refract()</code> returns true), we are going to calculate the reflective coefficient <code class="language-plaintext highlighter-rouge">reflect_probability</code>. Otherwise, the ray exhibits total internal reflection and the reflective coefficient should be 1.</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/refraction-reflection.gif" alt="Refraction and reflection gif"></p>

<p>We do this by getting whichever value is smaller between:</p>
<ul>
  <li>the dot product of flipped unit incident ray and the normal of the hit point OR</li>
  <li>1.0 (Total internal reflection)</li>
</ul>

<p>You may be wondering how we are to represent the refraction and reflection of light if we can only pick one scattered ray. The answer is multi-sampling - averaging the color between samples that may have been reflected or refracted.</p>

<p>To get an accurate result, we’ll use our random number generator. We’ll generate a number between 0.0 and 1.0. If the resulting number is smaller than the reflective coefficient, the ray will be reflected. Otherwise, it will be refracted.</p>

<p>We can now bundle everything up into our dielectric material’s <code class="language-plaintext highlighter-rouge">scatter()</code> method:</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">...

class dielectric : public material {
    public:
        dielectric(vec3 a, double ri) : albedo(a), ref_idx(ri) {}

        virtual bool scatter(
            const ray&amp; r_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered
        ) const {

            attenuation = albedo; // color

            double n1_over_n2 = (rec.front_face) ? (1.0 / ref_idx) : (ref_idx);

            vec3 unit_direction = unit_vector(r_in.direction());
            
            double cosine = dot(-unit_direction, rec.normal);
            double reflect_random = random_double(0,1);
            double reflect_probability;

            vec3 refracted;
            vec3 reflected;

			// refracted ray exists
            if (refract(unit_direction, rec.normal, n1_over_n2, refracted)) {
                reflect_probability = schlick(cosine, ref_idx);

                if (reflect_random &lt; reflect_probability) {
                    vec3 reflected = reflect(unit_direction, rec.normal);
                    scattered = ray(rec.p, reflected);
                    return true;
                }
                scattered = ray(rec.p, refracted);
                return true;
            }

            else {
                reflected = reflect(unit_direction, rec.normal);
                scattered = ray(rec.p, reflected);
                return true;
            }
        }
    public:
        double ref_idx;
        vec3 albedo;
};</code></pre>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/no-fresnel.png" alt="Dielectric without Fresnel">
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/fresnel.png" alt="Dielectric with Fresnel">
  </div>
</div>
  (Mouseover) Implementation of Fresnel reflections
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/no-fresnel.png">No fresnel</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/fresnel.png">Fresnel</a></li>
</ul>

<h3 id="hollow-dielectric-spheres">
<a class="anchor" href="#hollow-dielectric-spheres" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="hollow-dielectric-spheres"></a>Hollow Dielectric Spheres</h3>

<p>Bonus fun fact! We can create a hollow glass sphere by creating a smaller sphere with a <em>negative</em> radius <em>inside</em> our existing sphere! The geometry is unaffected, but the normal points inward.</p>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-solid.png" alt="Solid Dielectric">
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-hollow.png" alt="Hollow Dielectric">
  </div>
</div>
  (Mouseover) Hollow dielectric sphere
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-solid.png">Solid</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-hollow.png">Hollow</a></li>
</ul>

<p>And of course, you can change the color of your pretty new dielectric sphere if you please.
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/purple-dielectric.png" alt="Purple dielectric"></p>

<hr>

<h2 id="camera-modeling">
<a class="anchor" href="#camera-modeling" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="camera-modeling"></a>Camera Modeling</h2>
<p>Up until now, we’ve been using a very simple camera (though I have changed framing a little bit for illustrating certain renders). Our simple camera was described way back in chapter 4 of Shirley’s book, and it had fixed world position at the origin (0, 0, 0), a fixed image plane (or near-clipping plane) size, and the plane was position at (0, 0, -1), effectively setting our sights along the negative z-axis.</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code>:</p>
<pre><code class="language-cpp">#ifndef CAMERAH
#define CAMERAH

#include "ray.h"

class camera {
public:

	// The values below are derived from making the "camera" / ray origin coordinates(0, 0, 0) relative to the canvas.
	camera() {
		lower_left_corner = vec3(-2.0, -1.0, -1.0);
		horizontal = vec3(4.0, 0.0, 0.0);
		vertical = vec3(0.0, 2.0, 0.0);
		origin = vec3(0.0, 0.0, 0.0);
	}
	ray get_ray(double u, double v) { return ray(origin, lower_left_corner + u * horizontal + v * vertical - origin); }

	vec3 origin;
	vec3 lower_left_corner;
	vec3 horizontal;
	vec3 vertical;
};

#endif // !CAMERAH</code></pre>

<p>We’re going to expand the capability of the camera and make it more flexible by defining a few different variables:</p>

<ul>
  <li>Camera position <code class="language-plaintext highlighter-rouge">look_from</code>
</li>
  <li>Camera objective <code class="language-plaintext highlighter-rouge">look_at</code>
</li>
  <li>Vector describing which way is “up” <code class="language-plaintext highlighter-rouge">vup</code>
</li>
  <li>Vertical field-of-view <code class="language-plaintext highlighter-rouge">vfov</code>
</li>
  <li>Image plane aspect ratio <code class="language-plaintext highlighter-rouge">aspect_ratio</code>
</li>
  <li>Camera lens size <code class="language-plaintext highlighter-rouge">aperture</code>
</li>
  <li>Image plane to camera distance <code class="language-plaintext highlighter-rouge">focus_distance</code>
</li>
</ul>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/camera-model.png" alt="Camera visualization"></p>

<p>If the angle of the camera lens is <code class="language-plaintext highlighter-rouge">theta</code>, we can see that <code class="language-plaintext highlighter-rouge">half_height</code> = <code class="language-plaintext highlighter-rouge">tan(theta/2)</code>.</p>

\[tan(\frac {\theta}{2}) = \frac{opposite}{adjacent} = \frac {half\_height}{1}\]

<p><code class="language-plaintext highlighter-rouge">theta</code> is the vertical field of view <code class="language-plaintext highlighter-rouge">vfov</code>. For the convenience of calculation, we can convert theta to radians and define it as <code class="language-plaintext highlighter-rouge">vfov * pi / 180</code>.</p>

<p>Keeping other camera settings the same, we can rewrite our camera:
<code class="language-plaintext highlighter-rouge">camera.h</code></p>
<pre><code class="language-cpp">...
lower_left_corner(-half_width, -half_height,-1.0);
horizontal(2*half_width, 0.0, 0.0); // horizontal range
vertical(0.0, 2*half_height, 0.0);  // vertical range
origin = (0,0,0);
...</code></pre>

<p>In world space(three dimensions), the vectors</p>

\[e1 = (1,0,0),\]

\[e2 = (0,1,0),\]

\[e3 = (0,0,1)\]

<p>form the <a href="https://en.wikipedia.org/wiki/Standard_basis">standard basis</a>.</p>

<p>The standard basis (for a Euclidean space) is an <a href="https://en.wikipedia.org/wiki/Orthonormal_basis">orthonormal basis</a>, where the relevant inner product is the dot product of vectors. Put simply, this means the vectors are orthogonal: at right angles to each other; and normal: all of the same length 1.</p>

<p>All vectors (x,y,z) in world space can be expressed as a sum of the scaled basis vectors.</p>

\[{\displaystyle (x,y,z)=xe_{1}+ye_{2}+ze_{3}}\]

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/standard-basis.svg" alt="Standard basis">
Every vector $a$ in three dimensions is a linear combination of the standard basis vectors $i$, $j$, and $k$.
</span></p>

<p>Therefore, the previous camera code should be revised to</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code></p>
<pre><code class="language-cpp">lower_left_corner= origin - half_width * e1 - half_height * e2 - e3
horizontal = 2 * half_width * e1
vertical = 2 *half_height * e2</code></pre>

<p>However, if we want to move our “camera” to position <code class="language-plaintext highlighter-rouge">look_from</code> pointing to <code class="language-plaintext highlighter-rouge">look_at</code>, we have to build a new orthonormal basis for camera space with vectors u, v, and w:</p>

<pre><code class="language-cpp">w = unit_vector(lookfrom - lookat) // similar to the Z axis
u = unit_vector(cross(vup, w)) // similar to the X axis
v = cross(w, u) // similar to the Y axis</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/orthonormal-basis.png" alt="New orthonormal basis">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/orthonormal-basis.png" alt="Shirley orthonormal basis"></p>

<p>The <code class="language-plaintext highlighter-rouge">vup</code> vector describes which direction is up for the camera. You can also think of this as tilt in any (x,y,z).</p>

<div class="row-fill">
	<div class="captioned-image">
	<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/vup-010.png">
	vup (0,1,0)
	</div>
	<div class="captioned-image">
	<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/vup-0neg10.png">
	vup (0,-1,0)
	</div>
	<div class="captioned-image">
	<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/vup-110.png">
	vup (1,1,0)
	</div>
</div>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/orthonormal-vup.png" alt="Shirley vector-up"></p>

<pre><code class="language-cpp">class camera {
public:
    camera(vec3 look_from, vec3 look_at, vec3 vup, float vfov, float aspect_ratio) {
        vec3 u, v, w;

        float theta = vfov*pi/180;
        float half_height = tan(theta/2);
        float half_width = aspect_ratio * half_height;
        origin = lookfrom;

        w = unit_vector(look_from - look_at);
        u = unit_vector(cross(vup, w));
        v = cross(w, u);

        lower_left_corner = origin - half_width*u - half_height*v -w;
        horizontal = 2*half_width*u;
        vertical = 2*half_height*v;
    }
    ray get_ray(float s, float t) {return ray(origin, lower_left_corner + s*horizontal + t*vertical - origin);}

    vec3 lower_left_corner;
    vec3 horizontal;
    vec3 vertical;
    vec3 origin;
};</code></pre>

<hr>

<h2 id="depth-of-field">
<a class="anchor" href="#depth-of-field" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="depth-of-field"></a>Depth of Field</h2>

<p>Depth of field! If you have eyes that kind of work, you’re familiar with it. You can read more about DOF at <a href="https://en.wikipedia.org/wiki/Depth_of_field">Wikipedia</a>. Depth of field (DOF) is the distance between the nearest and farthest objects that are acceptably sharp in an image. The subjects outside the DOF are subject to blur.</p>

<p>From Wikipedia:</p>
<blockquote>
  <p>DOF can be calculated based on focal length, distance to subject, the acceptable “circle of confusion size”, and aperture.</p>
</blockquote>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/depth-of-field.png" alt="Depth of field (PBR!)">
<em>PBR!</em> (<a href="https://www.joepylephotography.com/depth-of-field-joe-pyle-photography/"><em>source</em></a>)
</span></p>

<blockquote>
  <p>In optics, an aperture is a hole or an opening through which light travels. More specifically, the aperture and focal length of an optical system determine the cone angle of a bundle of rays that come to a focus in the image plane.</p>
</blockquote>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/aperture-diagram.gif" alt="Aperture diagram"></p>

<p>From the beginning, all scene rays have originated from <code class="language-plaintext highlighter-rouge">look_from</code>. To simulate a variable aperature, we’ll generate rays randomly originating from inside a disk centered at <code class="language-plaintext highlighter-rouge">look_from</code>. The larger we make this disc, the stronger the blur will be. With a disk radius of zero, the rays all originate from <code class="language-plaintext highlighter-rouge">look_from</code>, eliminating blur. One such method for generating a point inside a unit disk is as follows:</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">vec3 random_unit_disk_coordinate() {
    while (true) {
        auto p = vec3(random_double(-1,1), random_double(-1,1), 0);
        if (p.length_squared() &gt;= 1) continue;
        return p;
    }
}</code></pre>

<p>Up until now, the focus distance was -1 on the z (or w) axes. We’ll now make it <code class="language-plaintext highlighter-rouge">focus_distance</code> and define our image plane accordingly:</p>

<pre><code class="language-cpp">lower_left_corner = origin - half_width*focus_distance*u - half_height*focus_distance*v - focus_distance*w;
horizontal = 2 * half_width*focus_distance*u;
vertical = 2 * half_height*focus_distance*v;</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/camera-model-summary-aperture.png" alt="Aperture diagram"></p>

<p>With that, we have a complete camera class:</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code></p>
<pre><code class="language-cpp">class camera {
    public:
        camera(vec3 look_from, vec3 look_at, vec3 vUp, double vFov, double aspect_ratio, double aperture, double focus_distance) {
            
            lens_radius = aperture / 2;
            
            double theta = vFov*pi/180;
            double half_height = tan(theta/2);
            double half_width = aspect_ratio * half_height;
            origin = look_from;
            
            w = unit_vector(look_from - look_at);
            u = unit_vector(cross(vUp, w));
            v = cross(w, u);

            lowerLeftCorner = origin
                              - half_width * focus_distance * u
                              - half_height * focus_distance * v
                              - focus_distance * w;
            horizontal = 2*half_width*focus_distance*u;
            vertical = 2*half_height*focus_distance*v;
        }

        ray get_ray(double s, double t) {
            vec3 rd = lens_radius*random_unit_disk_coordinate();
            vec3 offset = u * rd.x() + v * rd.y();

            return ray(origin + offset,
                       lowerLeftCorner + s*horizontal + t*vertical - origin - offset);
        }

        vec3 origin;
        vec3 lowerLeftCorner;
        vec3 horizontal;
        vec3 vertical;
        vec3 u, v, w;
        double lens_radius;
};</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dof-0.png" alt="Aperture 0">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dof-point-2.png" alt="Aperture 0.2">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dof-point-5.png" alt="Aperture 0.5"></p>

<hr>

<h2 id="final-scene">
<a class="anchor" href="#final-scene" aria-hidden="true"><span class="octicon octicon-link"></span></a><a id="final-scene"></a>Final Scene</h2>

<p>Lastly, we’ll create a random scene of spheres. Feel free to customize. And be aware - this may take some time to render!</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code></p>
<pre><code class="language-cpp">...
hittable *random_scene() {
    int n = 500;
    hittable **list = new hittable*[n+1];
    list[0] =  new sphere(vec3(0,-1000,0), 1000, new lambertian(vec3(0.5, 0.5, 0.5))); // "Ground"
    int i = 1;
    for (int a = -11; a &lt; 11; a++) {
        for (int b = -11; b &lt; 11; b++) {
            double randomMaterial = random_double(0,1);
            vec3 center(a+0.9*random_double(0,1),0.2,b+0.9*random_double(0,1));
            if ((center-vec3(4,0.2,0)).length() &gt; 0.9) {
                if (randomMaterial &lt; 0.68) {  // diffuse
                    list[i++] = new sphere(center, 0.2,
                        new lambertian(vec3(random_double(0,1)*random_double(0,1),
                                            random_double(0,1)*random_double(0,1),
                                            random_double(0,1)*random_double(0,1))
                        )
                    );
                }
                else if (randomMaterial &lt; 0.87) { // metal
                    list[i++] = new sphere(center, 0.2,
                            new metal(vec3(0.5*(1 + random_double(0,1)),
                                           0.5*(1 + random_double(0,1)),
                                           0.5*(1 + random_double(0,1))),
                                      0.5*random_double(0,1)));
                }
                else {  // glass
                    list[i++] = new sphere(center, 0.2, new dielectric(vec3(random_double(0,1),random_double(0,1),random_double(0,1)), 1.5));
                }
            }
        }
    }

    list[i++] = new sphere(vec3(0, 1, 0), 1.0, new dielectric(vec3(1.0,1.0,1.0), 1.5));
    list[i++] = new sphere(vec3(-4, 1, 0), 1.0, new lambertian(vec3(0.4, 0.2, 0.1)));
    list[i++] = new sphere(vec3(4, 1, 0), 1.0, new metal(vec3(1.0, 1.0, 1.0), 0.0));

    return new hittable_list(list,i);
}

int main() {

...

double aperture = 0.2; // bigger = blurrier

    hittable *world = random_scene();

	camera cam(lookFrom, lookAt, vec3(0,1,0), 20,double(nx)/double(ny), aperture, distToFocus);
...
}
...</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/final-render-1.png" alt="Final render"></p>

  </div>

<hr>
<h1><i class="fas fa-hand-peace"></i></h1>
<div class="post-tags">
    
  
  <a class="tag-link"
    href=/tags/graphics
    rel="category tag">
    <code>#graphics</code></a>

  
  <a class="tag-link"
    href=/tags/ray-tracing-in-one-weekend
    rel="category tag">
    <code>#ray-tracing-in-one-weekend</code></a>

  
  <a class="tag-link"
    href=/tags/c++
    rel="category tag">
    <code>#c++</code></a>

</div>

      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>

    <!-- link to Prism Syntax Highlighter -->
    <script src="/js/site-scripts/prism.js"></script>

    <!-- Autoloader configuration (https://prismjs.com/plugins/autoloader/) -->
    <script> 
      Prism.plugins.autoloader.languages_path = '/assets/prism-components/'; 
    </script> 

<!-- Diff-highlighter is very picky about whitespace -->
    <!-- <script>Prism.plugins.NormalizeWhitespace.setDefaults({
      'remove-trailing': true,
      'remove-indent': true,
      'left-trim': false,
      'right-trim': true,
      // 'break-lines': 80,
      'indent': 0,
      'remove-initial-line-feed': true,
      'tabs-to-spaces': 4,
      // 'spaces-to-tabs': 4
    });</script> -->
    

      <!-- Back to top -->
    <script src="/js/site-scripts/vanilla-back-to-top.min.js"></script>
    <script>addBackToTop()</script>

  </body>
</html>
