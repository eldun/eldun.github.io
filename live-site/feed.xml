<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="eldun.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="eldun.github.io/" rel="alternate" type="text/html" /><updated>2024-02-06T22:01:40-05:00</updated><id>eldun.github.io/feed.xml</id><entry><title type="html">Mixing it up:</title><link href="eldun.github.io/2023/03/01/model-samples-randomizer.html" rel="alternate" type="text/html" title="Mixing it up:" /><published>2023-03-01T00:00:00-05:00</published><updated>2023-03-01T00:00:00-05:00</updated><id>eldun.github.io/2023/03/01/model-samples-randomizer</id><content type="html" xml:base="eldun.github.io/2023/03/01/model-samples-randomizer.html"><![CDATA[<p><a id="continue-reading-point"></a></p>

<hr />

<h3 id="what-is-the-modelsamples">What is the Model:Samples?</h3>
<p>The <a href="https://www.elektron.se/us/modelsamples-explorer">Model:Samples</a> (hereinafter referred to as the Samples) is a capable mid-range six-track sample mangler with a powerful built-in sequencer. You can check out a review of it below to get accquainted with its “workflow”:</p>

<iframe width="560" height="315" style="display: block; margin: auto;" src="https://www.youtube.com/embed/y3NBzKJ9R5A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<p>If you didn’t watch the video, I’ll cover the basics for you:</p>

<ul>
  <li>6 audio tracks (all of which may be used as MIDI tracks)</li>
  <li>6 × velocity-sensitive pads</li>
  <li>96 projects</li>
  <li>96 patterns per project</li>
  <li>Elektron sequencer up to 64 steps with unique length and scale settings per track</li>
  <li>real-time or grid recording of notes and parameters</li>
  <li>64 MB sample memory</li>
  <li>1 GB storage</li>
</ul>

<p>1 GB of storage might not sound like a lot, but samples are often very short - the storage goes a <em>long</em> way.</p>

<hr />

<h3 id="the-goal">The Goal</h3>
<p>Many samplers/sample players have the ability to shuffle the active samples; a feature intended to spark creativity or simply create a new, weird “kit.” The Samples, being Elektron’s more budget-friendly device (as opposed to the <a href="https://www.elektron.se/us/digitakt-explorer">Digitakt</a>), lacks this feature, among others. I’ve found myself making beat after beat with the default kit, just because the process of flippantly selecting samples by scrolling through all my shallow folders really sucks up some time! I’ve grown tired of the factory kits, and have realized that there’s a wonky workaround.</p>

<hr />

<h3 id="the-solution">The Solution</h3>
<p>Thankfully, the devs included a feature to automatically load all samples in a folder:</p>

<p><img src="/assets/images/blog-images/model-samples/load-folder.png" alt="Details on loading an entire set of samples" /></p>

<p>All we have to do is write a script to copy 6 random files from our desired sample library to their own folder. It’s hacky, but it’ll still be loads faster for making completely stupid beats than browsing. Of course, I still want to reserve some space for my curated sounds on the Samples (like for when I need to find and use the <a href="https://www.youtube.com/watch?v=wRl88VATFrs">Goldeneye sound</a>).</p>

<p>The first party software used for communicating with and transferring samples to &amp; from the Samples is <a href="https://www.elektron.se/us/download-support-transfer">Elektron Transfer</a> - available for Mac and Windows. For Linux, the third party <a href="https://github.com/dagargo/elektroid">Elektroid</a> works just as well.</p>

<hr />

<h3 id="the-gameplan">The Gameplan</h3>
<p>We’ll be using Python.</p>

<p>First, we should think about scope and how the user will interact with the program.</p>

<hr />

<h4 id="scope">Scope</h4>
<p>This is not a life-changing program and does not need to be terribly robust.</p>

<hr />

<h4 id="usage">Usage</h4>
<p>At the moment, following cp’s example seems to make a lot of sense (Specifically, the first and second lines of the synopsis):</p>

<pre><code class="language-terminal">
NAME
       cp - copy files and directories

SYNOPSIS
       cp [OPTION]... [-T] SOURCE DEST
       cp [OPTION]... SOURCE... DIRECTORY
       cp [OPTION]... -t DIRECTORY SOURCE...

DESCRIPTION
       Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.
</code></pre>

<hr />

<h3 id="the-script">The Script</h3>

<hr />

<h4 id="parsing-arguments">Parsing Arguments</h4>
<p>To get started, we’ll need to parse command line arguments. Most of what you would ever need to know can about command line arguments can be found <a href="https://realpython.com/python-command-line-arguments">here</a>. Late in the article, you’ll find the <a href="https://realpython.com/python-command-line-arguments/#the-python-standard-library">recommendation to use the existing Python standard library</a>, <a href="https://docs.python.org/3/library/argparse.html"><code class="language-plaintext highlighter-rouge">argparse</code></a>.</p>

<p>One of the most useful parts of the documentation on <code class="language-plaintext highlighter-rouge">argparse</code> is an <a href="https://docs.python.org/3/library/argparse.html#the-add-argument-method">overview of the 'add_argument' method</a>.</p>

<p>Using argparse, the start of our program will look like this:</p>
<pre><code class="language-python">
import argparse


def init_argparse() -&gt; argparse.ArgumentParser:
    # print('Initializing parser')
    parser = argparse.ArgumentParser(description='Create seqeuntially named directories - each containing six random files collated from the provided source directories')

    parser.add_argument('sources', nargs='+')
    parser.add_argument('dest', nargs=1)
    parser.add_argument('--kits', '-k',
                        nargs='?',
                        type=int,
                        default=DEFAULT_KIT_SIZE,
                        help='The number of output folders(\'kits\') to create. Default: ' + str(DEFAULT_KIT_SIZE))
    # print('Parser initialized')
    return parser

if __name__ == "__main__":
    parser = init_argparse()
    args = parser.parse_args() 
    print(args)
</code></pre>

<p>Let’s do a quick test:</p>
<pre><code class="language-python">
$ python3 msrandomizer.py goodmorning goodafternoon goodnight
</code></pre>

<p>The result:</p>
<pre><code class="language-python">
Namespace(sources=['goodmorning', 'goodafternoon'], dest=['goodnight'], kits=25)
</code></pre>

<p>Perfect! We can access these values using dot notation (e.g. <code class="language-plaintext highlighter-rouge">args.sources</code>)</p>

<hr />

<h4 id="filename-cleanup">Filename Cleanup</h4>
<p>Some samples in my collection start with dots(designating them as hidden) and others (less seriously) with non-numeric characters(impacting legibility). To fix this, import <code class="language-plaintext highlighter-rouge">re</code> for <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expression</a> matching, and add the following method:</p>
<pre><code class="language-python"> 
    def remove_leading_non_alphanumeric(input_string):
        return re.sub(r'^[^a-zA-Z0-9]*', '', input_string)
</code></pre>

<hr />

<h4 id="generating-the-kits">Generating the Kits</h4>
<p>An excellent resource on working with files in python can be found <a href="https://realpython.com/working-with-files-in-python/#pythons-with-open-as-pattern">here</a>.</p>

<p>The procedure be as follows:</p>
<ol>
  <li>Select 6 unique random audio files from the supplied source(s)</li>
  <li>Create a directory (named sequentially in hex) and copy the files to it</li>
  <li>Repeat until the desired number of kits is reached OR the size of the files copied reaches ~1 gig (the size of the Samples’ drive)</li>
</ol>

<p>To complete these steps, we’ll be importing a few modules:</p>

<ul>
  <li><a href="https://docs.python.org/3/library/shutil.html"><code class="language-plaintext highlighter-rouge">shutil</code></a> - High-level file operations (<strong>sh</strong>ell <strong>util</strong>ities)</li>
  <li><a href="https://docs.python.org/3/library/random.html"><code class="language-plaintext highlighter-rouge">random</code></a></li>
  <li><a href="https://docs.python.org/3/library/os.html"><code class="language-plaintext highlighter-rouge">os</code></a></li>
</ul>

<p>Let’s create a function <code class="language-plaintext highlighter-rouge">generate_kits</code>:</p>

<pre><code class="language-python">def generate_kits(args) -&gt; None:
    random_source = lambda: random.choice(args.sources)
    dest = args.dest[0]
    kit_count = args.kits

    current_kit_label = 0
    cumulative_size = 0


    for kit in range(kit_count):

        if cumulative_size &gt; MAX_TRANSFER_LIMIT_IN_BYTES:
            exit_string = str.format('Max transfer size({} GiB) reached', cumulative_size/1024.0/1024.0/1024.0)
            sys.exit(exit_string)
        
        attempts = 0
        samples_added = 0
        kit_folder = f'{current_kit_label:x}'
        output_folder = os.path.join(dest, kit_folder)

        if os.path.isdir(output_folder):
            shutil.rmtree(output_folder)
    

        os.mkdir(output_folder)

        while samples_added &lt; KIT_SIZE:

            source = random_source() # Randomly choose a new source dir each iteration

            # Build list of files
            audio_files = []
            for path, subdirs, files in os.walk(source):
                for file in files:


                    # Only grab audio files
                    if file.lower().endswith(('.wav', '.mp3', '.aiff')):
                        audio_files.append(os.path.join(path, file))

            random_file_path = random.choice(audio_files)
            random_file = os.path.basename(random_file_path)

            root, ext =  os.path.splitext(random_file)



            # Un-hide files that start with '.', as well as make files more legible
            renamed_output_file = remove_leading_non_alphanumeric_characters(root)

            # Add 1-6 suffix to accommodate how the Model:Samples loads kits
            renamed_output_file = f"{renamed_output_file}-{samples_added+1}{ext}"
            
            renamed_output_file_path = os.path.join(output_folder, renamed_output_file)
            shutil.copy(random_file_path, os.path.join(output_folder, renamed_output_file))

            cumulative_size += os.path.getsize(random_file_path)
            cumulative_size_string = '{:.2f} {}'.format(cumulative_size/1024.0/1024.0, 'MiB copied')

            # Columnated console output
            renamed_output_file = f'\'{renamed_output_file}\'' # You can't put backslashes inside of f-string braces, so I inserted them here
            print(f'{random_file_path:&lt;100} copied to \'{kit_folder}\' {"":&lt;10}as {renamed_output_file:&lt;30}  {cumulative_size_string:&gt;30}'  )

            samples_added += 1


        print('\n')
        current_kit_label = current_kit_label + 1
</code></pre>

<p><span class="warning">
Using the <code class="language-plaintext highlighter-rouge">file</code> command, I found the Model:Samples wouldn’t load any ‘AppleDouble’ files - the kind found in <a href="https://superuser.com/a/104501">useless <code class="language-plaintext highlighter-rouge">\__MACOSX</code> folders</a>. I deleted all <code class="language-plaintext highlighter-rouge">__MACOSX</code> folders from my sample library, rather than checking each file.
</span></p>

<p>And that’s it! The complete &amp; final code can be found <a href="https://github.com/eldun/msrandomizer/blob/main/msrandomizer.py">here</a>.</p>

<hr />

<h3 id="results">Results</h3>
<p>Check back in a couple days and I’ll have made some preposturous beats with my ultra-fresh random kits.</p>

<hr />

<h3 id="bonus-ai-test">Bonus AI Test</h3>
<p>I’m a litte late to the party - I haven’t tried using <a href="https://openai.com/blog/chatgpt">ChatGPT3</a> yet! Can it generate a comparable script in minutes?</p>

<p><span class="note">
I generated the following script before I realized the issue with hidden files. I’m sure ChatGPT would be up to the task, but I don’t feel like going through any more iterations to get it just right.
</span>
…</p>

<p>I knew chat GPT was impressive, but using it to generate solutions for my personal projects feels magical. After a few iterations and clarifications, this was my request:</p>

<pre><code class="language-terminal" style="white-space:pre-line">Generate a python script that accepts the following arguments in the following order: 'source', 'destination', and 'num_folders'. The user should be able to specify more than one source directory. The arguments should be parsed using argparse.

The script should create num_folders folders(named sequentially in hex) at the destination path with 6 unique audio files in each folder. Each of the 6 unique audio files should be randomly selected and copied from a random selection of the source directories and its sub-directories. If fewer than 6 unique audio files are found, notify the user, remove any folders and files that have been created, and exit the program.

Keep track of the cumulative size 'cumulative_size' of the files that have been copied. If cumulative_size is about to exceed 1 GiB, notify the user, delete the most recently created hex folder and files, leave the others, and exit the program.  When copying a file, print(in vertically-aligned columns) the last 40 characters of the relative path of the file being copied, the folder it's being copied to, and cumulative_size (labeled 'MiB copied so far').</code></pre>

<p>The result might look a bit familiar:</p>

<pre><code class="language-python">import os
import shutil
import random
import argparse

MAX_SIZE = 1024 ** 3  # 1 GiB in bytes


def main():
    parser = argparse.ArgumentParser(description='Copy random audio files to folders.')    
    parser.add_argument('sources', nargs='+', help='the source folder(s) to select audio files from')
    parser.add_argument('destination', help='the destination folder to create sub-folders in')
    parser.add_argument('num_folders', type=int, help='the number of folders to create')
    args = parser.parse_args()

    # create destination folder if it doesn't exist
    os.makedirs(args.destination, exist_ok=True)

    cumulative_size = 0
    for i in range(args.num_folders):
        folder_name = hex(i)[2:].zfill(2)  # convert to hex and zero-pad to 2 digits
        folder_path = os.path.join(args.destination, folder_name)
        os.mkdir(folder_path)

        files = []
        for source in args.sources:
            for root, _, filenames in os.walk(source):
                for filename in filenames:
                    if filename.endswith('.mp3') or filename.endswith('.wav'):
                        files.append(os.path.join(root, filename))

        if len(files) &lt; 6:
            print(f"Not enough audio files found for folder {folder_name}.")
            shutil.rmtree(folder_path)
            break

        folder_size = 0
        selected_files = random.sample(files, 6)
        for file_path in selected_files:
            file_size = os.path.getsize(file_path)
            if cumulative_size + folder_size + file_size &gt; MAX_SIZE:
                print(f"Folder {folder_name} exceeds 1 GiB. Deleting folder.")
                shutil.rmtree(folder_path)
                return

            relative_path = os.path.relpath(file_path, args.sources[0])[-40:]
            dest_path = os.path.join(folder_path, os.path.basename(file_path))
            shutil.copy2(file_path, dest_path)
            folder_size += file_size
            cumulative_size += file_size

            print(f"{relative_path:40} {folder_name} {cumulative_size / (1024 ** 2):10.2f} MiB copied so far.")

if __name__ == '__main__':
    main()
</code></pre>

<p>The only thing I changed was the order of the arguments.</p>

<p>Thanks for reading - see you next time!</p>]]></content><author><name>Evan</name></author><category term="python" /><category term="scripting" /><category term="music" /><category term="ai" /><summary type="html"><![CDATA[One of my favorite afforadable samplers lacks the ability to create random kits - are there any workarounds to do so?]]></summary></entry><entry><title type="html">Morso:</title><link href="eldun.github.io/2022/11/21/morso-pt-one.html" rel="alternate" type="text/html" title="Morso:" /><published>2022-11-21T00:00:00-05:00</published><updated>2022-11-21T00:00:00-05:00</updated><id>eldun.github.io/2022/11/21/morso-pt-one</id><content type="html" xml:base="eldun.github.io/2022/11/21/morso-pt-one.html"><![CDATA[<h2 id="why-morse-code">Why Morse Code?</h2>
<p>Occasionally, I find myself at a bar, yelling in someone’s ear just to be heard. No good - it’s one of my least-favorite things to do or to have done to me.</p>

<p>One of my favorite things in this world is the process of getting better at useless-adjacent things. Rather than spending hours learning <a href="https://en.wikipedia.org/wiki/American_Sign_Language">American Sign Language</a> (which <strong>is</strong> on my to-do list), I could just learn 26 characters in <a href="https://en.wikipedia.org/wiki/Morse_code">Morse code</a> and tap out a message across the table or upon my friend’s shoulder!</p>

<p>I already learned the <a href="https://en.wikipedia.org/wiki/Dvorak_keyboard_layout">Dvorak keyboard layout</a> (not recommended) - which is unsurprisingly pretty similar to the structure of Morse code (look at the home row)! Obviously, the most used letters are the most accessible.</p>

<p><img src="/assets/images/blog-images/morso/dvorak-layout.png" alt="Dvorak layout" /></p>

<div style="background-color: white">
<img src="/assets/images/blog-images/morso/morse-code-tree.png" alt="Morse code structure" />
<img src="/assets/images/blog-images/morso/morse-code-chart.png" alt="Morse code chart" />
</div>

<p>Additionally, the scope of this project seems perfect for getting back into Android development(sorry <a href="https://github.com/eldun/SeeNatural">SeeNatural</a>) and learning Kotlin.</p>

<hr />

<h2 id="the-general-idea">The General Idea</h2>
<p>I believe the best way to learn is by doing, which is why I want to create a custom Morse keyboard. Android has a useful “language/input” button for switching the keyboard quickly:
<img src="/assets/images/blog-images/morso/keyboard-button.png" alt="Android keyboard button" /></p>

<p>I’m not trying to reinvent the keyboard - just creating a handy, accessible practice tool. For that reason, I’ll be structuring my application in much the same way that GBoard does - a keyboard with some settings and utilities accessible from the top row.</p>

<p>Some practice ideas I’ve had are as follows:</p>

<ul>
  <li>Reading/Typing Morse</li>
  <li>Reading Morse through vibration/sound/flashing</li>
  <li>Time Trials</li>
</ul>

<p>Morso will be written in <a href="https://developer.android.com/kotlin/first">Kotlin</a>, Android’s “official” language.</p>

<hr />
<h2 id="getting-started">Getting Started</h2>
<p>The most obvious first step to me is to create an input <a href="https://developer.android.com/reference/android/app/Service">service</a> that can be used system-wide. The <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method">Android Developer article on input methods</a> and <a href="https://stackoverflow.com/a/44939816">this StackOverflow answer</a> will be exceedingly helpful.</p>

<hr />
<h2 id="declaring-ime-components-in-the-manifest">Declaring IME Components in the Manifest</h2>
<p>Official documentation for this step can be found <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#DefiningIME">here</a>, but we’re basically going to throw a snippet into our app’s <a href="https://developer.android.com/guide/topics/manifest/manifest-intro"><code class="language-plaintext highlighter-rouge">AndroidManifest.xml</code></a>.</p>

<blockquote>
  <p>The following snippet declares an IME service. It requests the permission BIND_INPUT_METHOD to allow the service to connect the IME to the system, sets up an intent filter that matches the action android.view.InputMethod, and defines metadata for the IME:</p>
</blockquote>

<pre><code class="language-xml">
&lt;!-- Declares the input method service --&gt;
&lt;service android:name="MorsoIME"
    android:label="@string/morso_label"
    android:permission="android.permission.BIND_INPUT_METHOD"&gt;
    &lt;intent-filter&gt;
        &lt;action android:name="android.view.InputMethod" /&gt;
    &lt;/intent-filter&gt;
    &lt;meta-data android:name="android.view.im"
               android:resource="@xml/method" /&gt;
&lt;/service&gt;
</code></pre>

<p>I was notified of an error about how <code class="language-plaintext highlighter-rouge">android:exported</code> must be set to <code class="language-plaintext highlighter-rouge">true</code> or <code class="language-plaintext highlighter-rouge">false</code> - I set it to true. You can read about the exported attribute <a href="https://developer.android.com/guide/topics/manifest/service-element#exported">here</a>.</p>

<p>I was also warned that the <code class="language-plaintext highlighter-rouge">android.preference</code> library is deprecated when I created the <code class="language-plaintext highlighter-rouge">xml/method</code> file. I added this line to my <code class="language-plaintext highlighter-rouge">build.gradle</code> (Module: app):</p>

<pre><code class="language-kotlin">
dependencies {
    ...
    implementation "androidx.preference:preference:1.1.0"
    ...
}
</code></pre>

<p><a href="https://stackoverflow.com/a/56833739">Source</a></p>

<p>Another issue I was having was that I was unable to select Morso as an input method. The reason why was that I had not yet added any <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#IMESubTypes">subtypes</a> to <code class="language-plaintext highlighter-rouge">res/xml/method.xml</code>. For now, you can paste this minimal example into <code class="language-plaintext highlighter-rouge">res/xml/method.xml</code>:</p>

<pre><code class="language-xml">
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;input-method
    xmlns:android="http://schemas.android.com/apk/res/android"&gt;

    &lt;subtype
        android:imeSubtypeMode="keyboard"/&gt;

&lt;/input-method&gt;
</code></pre>

<p>We will cover subtypes in further detail later on in this series.</p>

<p>My complete <code class="language-plaintext highlighter-rouge">AndroidManifest.xml</code> looks like this (I have no activities, as you can see.):</p>

<pre><code class="language-xml">
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;manifest xmlns:android="http://schemas.android.com/apk/res/android"
    package="net.eldun.morso"&gt;

    &lt;application
        android:allowBackup="true"
        android:icon="@mipmap/ic_launcher"
        android:label="@string/app_name"
        android:roundIcon="@mipmap/ic_launcher_round"
        android:supportsRtl="true"
        android:theme="@style/Theme.Morso" /&gt;

    &lt;!-- Declares the input method service --&gt;
    &lt;service android:name="MorsoIME"
        android:label="@string/morso_label"
        android:permission="android.permission.BIND_INPUT_METHOD"
        android:exported="true"&gt;
        &lt;intent-filter&gt;
            &lt;action android:name="android.view.InputMethod" /&gt;
        &lt;/intent-filter&gt;
        &lt;meta-data android:name="android.view.im"
            android:resource="@xml/method" /&gt;
    &lt;/service&gt;

&lt;/manifest&gt;
</code></pre>

<hr />
<h2 id="declaring-the-settings-activity-for-the-ime">Declaring the Settings Activity for the IME</h2>

<blockquote>
  <p>This next snippet declares the settings activity for the IME. It has an intent filter for ACTION_MAIN that indicates this activity is the main entry point for the IME application:</p>
</blockquote>

<!-- Optional: an activity for controlling the IME settings -->
<activity android:name="MorsoIMESettings" android:label="@string/morso_settings">
    <intent-filter>
        <action android:name="android.intent.action.MAIN" />
    </intent-filter>
</activity>

<blockquote>
  <p>You can also provide access to the IME’s settings directly from its UI.</p>
</blockquote>

<p>We will create the settings screen later on in this series.</p>

<hr />

<h2 id="creating-our-morsoime-class">Creating our MorsoIME Class</h2>
<p>You may have noticed that we have a warning that a service by the name of <code class="language-plaintext highlighter-rouge">MorsoIME</code> could not be found! Go ahead and create a new Kotlin class - <code class="language-plaintext highlighter-rouge">MorsoIME</code> - that extends <code class="language-plaintext highlighter-rouge">InputMethodService</code>:</p>

<blockquote>
  <p>The central part of an IME is a service component, a class that extends <a href="https://developer.android.com/reference/android/inputmethodservice/InputMethodService">InputMethodService</a>. In addition to implementing the normal <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#InputMethodLifecycle">service lifecycle</a>, this class has callbacks for providing your IME’s UI, handling user input, and delivering text to the field that currently has focus. By default, the <code class="language-plaintext highlighter-rouge">InputMethodService</code> class provides most of the implementation for managing the state and visibility of the IME and communicating with the current input field.</p>
</blockquote>

<p>This is what our placeholder <code class="language-plaintext highlighter-rouge">MorsoIME</code> looks like (we’ll create our UI in the next section):</p>

<pre><code class="language-kotlin">
class MorsoIME : InputMethodService() {

    override fun onCreateInputView(): View {
        return layoutInflater.inflate(R.layout.input, null).apply {
            if (this is MorsoView) {
                setOnKeyboardActionListener(this)
//                keyboard = latinKeyboard
            }
        }
    }

    private fun setOnKeyboardActionListener(action : MorsoIME) {

    }

}
</code></pre>
<p>If you’re confused by that <code class="language-plaintext highlighter-rouge">apply</code> block, read about Kotlin’s <a href="https://kotlinlang.org/docs/scope-functions.html">scope functions</a> and <a href="https://kotlinlang.org/docs/lambdas.html">higher-order functions</a>.</p>

<hr />

<h2 id="creating-our-input-view">Creating our Input View</h2>

<p>We have two options for <a href="https://developer.android.com/develop/ui">designing our UI</a> - the traditional “Views” method, and the newer “Jetpack Compose” method. <del>We’ll go through both for completeness, starting…</del> (On second thought, we’ll try Jetpack Compose when we implement our settings activity. I like to keep moving forward in these blog posts).</p>

<p>Almost everything you could want to know about views can be found</p>

<ul>
  <li><a href="https://developer.android.com/guide/topics/ui/how-android-draws">here</a> (how android draws views),</li>
  <li><a href="https://developer.android.com/develop/ui/views/layout/declaring-layout">here</a> (layouts),</li>
  <li><a href="https://developer.android.com/develop/ui/views/layout/custom-views/custom-components">here</a> (creating custom components),</li>
  <li>and <a href="https://developer.android.com/codelabs/advanced-android-kotlin-training-custom-views#0">here</a> (a codelab on creating custom views).</li>
</ul>

<hr />

<h3 id="creating-a-placeholder-layout">Creating a Placeholder Layout</h3>
<p>Create <code class="language-plaintext highlighter-rouge">res/morso.xml</code>:</p>
<pre><code class="language-xml">
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"&gt;

    &lt;ImageView
        android:id="@+id/morsoView"
        android:layout_width="0dp"
        android:layout_height="wrap_content"
        app:srcCompat="@drawable/ic_launcher_background"
        app:layout_constraintBottom_toBottomOf="parent"
        app:layout_constraintEnd_toEndOf="parent"
        app:layout_constraintStart_toStartOf="parent" /&gt;

&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;
</code></pre>
<p><img src="/assets/images/blog-images/morso/android-placeholder.png" alt="Placeholder View" /></p>

<p>Note that setting the <code class="language-plaintext highlighter-rouge">ImageView</code> dimensions to <code class="language-plaintext highlighter-rouge">0dp</code> is equivalent to <code class="language-plaintext highlighter-rouge">match_constraint</code>.</p>

<p>Once we create our custom <code class="language-plaintext highlighter-rouge">MorsoView</code> class, we’ll replace the <code class="language-plaintext highlighter-rouge">&lt;ImageView&gt;</code> tag with <code class="language-plaintext highlighter-rouge">&lt;net.eldun.morso.MorsoView&gt;</code>.</p>

<hr />

<h3 id="creating-our-custom-morsoview">Creating our Custom MorsoView</h3>

<p>Create a new Kotlin class called MorsoView.</p>

<p>Modify the class definition to extend View.</p>

<p>Click on View and then click the red bulb. Choose Add Android View constructors using ‘@JvmOverloads’. Android Studio adds the constructor from the View class. The @JvmOverloads annotation instructs the Kotlin compiler to generate overloads for this function that substitute default parameter values.</p>

<pre><code class="language-kotlin">
package net.eldun.morso

import android.content.Context
import android.util.AttributeSet
import android.view.View

class MorsoView @JvmOverloads constructor(
    context: Context,
    attrs: AttributeSet? = null,
    defStyleAttr: Int = 0
) : View(context, attrs, defStyleAttr) {

}
</code></pre>

<p>Now let’s follow the steps necessary to <a href="https://developer.android.com/codelabs/advanced-android-kotlin-training-custom-views#4">draw a custom view</a>. How about we start with a black rectangle with “Morso” in the center?</p>

<p>We could do both of these tasks in XML by extending a <code class="language-plaintext highlighter-rouge">Button</code> view (rather than a generic <code class="language-plaintext highlighter-rouge">View</code>) and using the <code class="language-plaintext highlighter-rouge">background</code> and <code class="language-plaintext highlighter-rouge">buttonText</code> attributes - or by <a href="https://developer.android.com/develop/ui/views/layout/custom-views/create-view#customattr">Defining Custom Attributes</a> for our totally custom view. I don’t have a great reason for setting properties programmatically, other than the fact that it’s fast.</p>

<hr />
<h3 id="overriding-onsizechanged">Overriding <code class="language-plaintext highlighter-rouge">onSizeChanged()</code></h3>

<p>First, we’ll override the <code class="language-plaintext highlighter-rouge">onSizeChanged()</code> -</p>
<blockquote>
  <p>The onSizeChanged() method is called any time the view’s size changes, including the first time it is drawn when the layout is inflated. Override onSizeChanged() to calculate positions, dimensions, and any other values related to your custom view’s size, instead of recalculating them every time you draw.</p>
</blockquote>

<p>Add member floats <code class="language-plaintext highlighter-rouge">centerX</code> and <code class="language-plaintext highlighter-rouge">centerY</code> to our <code class="language-plaintext highlighter-rouge">MorsoView</code> class, and then calculate them in <code class="language-plaintext highlighter-rouge">onSizeChanged</code></p>

<pre><code class="language-kotlin">
override fun onSizeChanged(width: Int, height: Int, oldWidth: Int, oldHeight: Int) {
   centerX = (width.toFloat / 2.0)
   centerY = (height.toFloat / 2.0)
}
</code></pre>

<p>This won’t <em>exactly</em> center the text (there’s a lot going on with fonts!) - it’s only a placeholder. If you wish you can add the code to <a href="https://stackoverflow.com/a/32081250">actually center it</a>.</p>

<hr />

<h3 id="creating-a-paint-object-for-drawing-text">Creating a <code class="language-plaintext highlighter-rouge">Paint</code> Object for Drawing Text</h3>
<p>We’re going to need a <code class="language-plaintext highlighter-rouge">Paint</code> object for drawing text:</p>

<pre><code class="language-diff-kotlin diff-highlight">
class MorsoView @JvmOverloads constructor(
    context: Context,
    attrs: AttributeSet? = null,
    defStyleAttr: Int = 0
) : View (context, attrs, defStyleAttr) {

+    private val paint = Paint(Paint.ANTI_ALIAS_FLAG).apply {
+        // Paint styles used for rendering are initialized here. This
+        // is a performance optimization, since onDraw() is called
+        // for every screen refresh.
+        style = Paint.Style.FILL
+        textAlign = Paint.Align.CENTER
+        textSize = 55.0f
+    }

    private var centerX = 100F
    private var centerY = 100F
</code></pre>

<hr />
<h3 id="drawing-our-view">Drawing our View</h3>
<p>Next, we’ll draw our view by overriding <code class="language-plaintext highlighter-rouge">onDraw()</code> (we’ll also set the background color here):</p>

<pre><code class="language-kotlin">
    override fun onDraw(canvas: Canvas) {
        super.onDraw(canvas)

        this.setBackgroundColor(Color.BLACK)
        canvas.drawText("Morso", centerX, centerY, paint)
    }
</code></pre>

<p>If you take a look at our generated layout, you’ll notice <code class="language-plaintext highlighter-rouge">MorsoView</code> takes up almost the whole screen. While this might be useful for typing without looking, it’s definitely not a reasonable default.</p>

<p>In order to determine how much space MorsoView is alloted, we’ll have to override <code class="language-plaintext highlighter-rouge">onMeasure()</code>. Two helpful snippets about <code class="language-plaintext highlighter-rouge">onMeasure()</code> can be found <a href="https://developer.android.com/develop/ui/views/layout/custom-views/custom-components#extend-ondraw-and-onmeasure">here</a> and <a href="https://developer.android.com/develop/ui/views/layout/custom-views/custom-drawing#layouteevent">here</a>.</p>

<p>From the second link:</p>
<blockquote>
  <p><code class="language-plaintext highlighter-rouge">onMeasure</code>’s parameters are View.MeasureSpec values that tell you how big your view’s parent wants your view to be, and whether that size is a hard maximum or just a suggestion. As an optimization, these values are stored as packed integers, and you use the static methods of <code class="language-plaintext highlighter-rouge">View.MeasureSpec</code> to unpack the information stored in each integer.</p>
</blockquote>

<p>This <a href="https://stackoverflow.com/a/12267248">StackOverflow answer</a> has a nice description of the different <code class="language-plaintext highlighter-rouge">MeasureSpec</code>s and how they relate to the width and height we set in our <code class="language-plaintext highlighter-rouge">res/morso.xml</code></p>

<p>Add a helper function to get the screen height:</p>
<pre><code class="language-kotlin">
    fun getScreenHeight(): Int {
        return Resources.getSystem().getDisplayMetrics().heightPixels
    }
</code></pre>

<p>Now we can override <code class="language-plaintext highlighter-rouge">MorsoView</code>’s <code class="language-plaintext highlighter-rouge">onMeasure()</code> and set the height to a quarter of the screen height:</p>

<pre><code class="language-kotlin">
override fun onMeasure(widthMeasureSpec: Int, heightMeasureSpec: Int) {
        val desiredWidth = 100;
        val desiredHeight = getScreenHeight() / 4;

        val widthMode = MeasureSpec.getMode(widthMeasureSpec);
        val widthSize = MeasureSpec.getSize(widthMeasureSpec);
        val heightMode = MeasureSpec.getMode(heightMeasureSpec);
        val heightSize = MeasureSpec.getSize(heightMeasureSpec);

        var width : Int;
        var height : Int;

        //Measure Width
        when (widthMode) {
            MeasureSpec.EXACTLY -&gt; width = widthSize;
            MeasureSpec.AT_MOST -&gt; width = Math.min(desiredWidth, widthSize);
            else -&gt; width = desiredWidth;
        }

        // Measure Height
        when (heightMode) {
            MeasureSpec.EXACTLY -&gt; height = heightSize;
            MeasureSpec.AT_MOST -&gt; height = Math.min(desiredHeight, heightSize);
            else -&gt; height = desiredHeight;
        }

        //MUST CALL THIS
        setMeasuredDimension(width, height);
    }

</code></pre>

<p><img src="/assets/images/blog-images/morso/morso-measured-view.png" alt="Morso measured view" /></p>

<!-- OK, I think that gives us a rough idea of what it's like to work with the traditional view method. -->

<!-- #### With "Jetpack Compose"
-- Right off the bat, the fact that we're using an `InputMethodService` instead of an `Activity` or `Fragment` makes following the [Compose tutorial](https://developer.android.com/jetpack/compose/tutorial) a little more challenging. Thankfully, [somebody else has gone through this same issue](https://stackoverflow.com/a/66958772) --


### Starting our IME Service
When  -->

<hr />

<h3 id="making-our-view-interactive">Making our View Interactive</h3>
<p>The article for this section can be found <a href="https://developer.android.com/develop/ui/views/layout/custom-views/making-interactive">here</a>.</p>

<blockquote>
  <p>Like many other UI frameworks, Android supports an input event model. User actions are turned into events that trigger callbacks, and you can override the callbacks to customize how your application responds to the user. The most common input event in the Android system is touch, which triggers onTouchEvent(android.view.MotionEvent).</p>

  <p>Touch events by themselves are not particularly useful. Modern touch UIs define interactions in terms of gestures such as tapping, pulling, pushing, flinging, and zooming. To convert raw touch events into gestures, Android provides GestureDetector.</p>
</blockquote>

<p>The obvious gestures that we’re going to be looking for are taps(dots) and holds(dashes). Eventually, we might want to listen for swipes to signal the end of a string or to switch to numerical input.</p>

<p>To learn more about gestures, go <a href="https://developer.android.com/develop/ui/views/touch-and-input/gestures">here</a>.</p>

<blockquote>
  <p>If you only want to process a few gestures, you can extend <code class="language-plaintext highlighter-rouge">GestureDetector.SimpleOnGestureListener</code> instead of implementing the <code class="language-plaintext highlighter-rouge">GestureDetector.OnGestureListener</code> interface.</p>
</blockquote>

<p>Create a new <code class="language-plaintext highlighter-rouge">MorsoGestureListener</code> class that extends <code class="language-plaintext highlighter-rouge">SimpleGestureListener</code>:</p>

<pre><code class="language-kotlin">
package net.eldun.morso

import android.util.Log
import android.view.GestureDetector
import android.view.MotionEvent

class MorsoGestureListener : GestureDetector.SimpleOnGestureListener() {

    val TAG = "MorsoGestureListener"

    override fun onDown(e: MotionEvent): Boolean {
        Log.i(TAG, "downMotion detected!")

        return true
    }

    override fun onSingleTapUp(e: MotionEvent): Boolean {
        Log.i(TAG, "tap detected!")
        return true
    }

}
</code></pre>

<blockquote>
  <p>Whether or not you use GestureDetector.SimpleOnGestureListener, you must always implement an onDown() method that returns true. This step is necessary because all gestures begin with an onDown() message. If you return false from onDown(), as GestureDetector.SimpleOnGestureListener does, the system assumes that you want to ignore the rest of the gesture, and the other methods of GestureDetector.OnGestureListener never get called. The only time you should return false from onDown() is if you truly want to ignore an entire gesture. Once you’ve implemented GestureDetector.OnGestureListener and created an instance of GestureDetector, you can use your GestureDetector to interpret the touch events you receive in onTouchEvent().</p>
</blockquote>

<p>In our MorsoView, add the following (along with any necessary imports):</p>

<pre><code class="language-kotlin">
class MorsoView @JvmOverloads constructor(
    context: Context,
    attrs: AttributeSet? = null,
    defStyleAttr: Int = 0
) : View (context, attrs, defStyleAttr) {

    val TAG = "MorsoView"

+    private val gestureListener =  MorsoGestureListener()
+    private val gestureDetector = GestureDetector(context, gestureListener)


    // ...
    // code
    // ...


+    override fun onTouchEvent(event: MotionEvent): Boolean {
+        return gestureDetector.onTouchEvent(event)
+    }

}
</code></pre>

<blockquote>
  <p>When you pass onTouchEvent() a touch event that it doesn’t recognize as part of a gesture, it returns false. You can then run your own custom gesture-detection code.</p>
</blockquote>

<p>This is where we can implement gestures like triple taps in the future.</p>

<p>…</p>

<p>Thinking about it some more, we’re probably going to end up creating all of our own gestures from within MorsoView’s on <code class="language-plaintext highlighter-rouge">onTouchEvent</code>. In doing so, we’ll be able to specify custom timing delays for dots and dashes and the like. However, it’s good for now.</p>

<p>Let’s put things in place to replace the “Morso” text on touch inputs with the appropriate characters.</p>

<hr />

<h3 id="representing-ui-state">Representing UI State</h3>
<p>The main article for this section is <a href="https://developer.android.com/topic/architecture/ui-layer#define-ui-state">here</a>. I suggest reading it.</p>

<p>Details on how IMEs handle config changes can be found <a href="https://developer.android.com/reference/android/inputmethodservice/InputMethodService#onConfigurationChanged(android.content.res.Configuration)">here</a>.</p>

<!-- Long story short, we're going to use [ViewModels](https://developer.android.com/topic/libraries/architecture/viewmodel). If you already know the story with ViewModels, you can skip to [the next section](/drawing-symbols-on-input) -->

<!-- We'll be using a [data class](https://kotlinlang.org/docs/data-classes.html) to represent the UI state. It's not much at the moment:

<pre><code class="language-kotlin">
package net.eldun.morso

data class MorsoViewState(){
    val mainText : String
}
</code></pre> -->

<p>Here’s an explanation for why the UI state member(s) are <a href="https://kotlinlang.org/docs/basic-syntax.html#variables">immutable</a>:</p>

<pre><code class="language-kotlin">
data class NewsUiState(
    val isSignedIn: Boolean = false,
    val isPremium: Boolean = false,
    val newsItems: List&lt;NewsItemUiState&gt; = listOf(),
    val userMessages: List&lt;Message&gt; = listOf()
)

data class NewsItemUiState(
    val title: String,
    val body: String,
    val bookmarked: Boolean = false,
    ...
)
</code></pre>

<blockquote>
  <p>The UI state definition in the example above is immutable. The key benefit of this is that immutable objects provide guarantees regarding the state of the application at an instant in time. This frees up the UI to focus on a single role: to read the state and update its UI elements accordingly. As a result, you should never modify the UI state in the UI directly unless the UI itself is the sole source of its data. Violating this principle results in multiple sources of truth for the same piece of information, leading to data inconsistencies and subtle bugs.</p>

  <p>For example, if the bookmarked flag in a NewsItemUiState object from the UI state in the case study were updated in the Activity class, that flag would be competing with the data layer as the source of the bookmarked status of an article. Immutable data classes are very useful for preventing this kind of antipattern.</p>

  <p>Key Point: Only sources or owners of data should be responsible for updating the data they expose.</p>
</blockquote>

<p>Great. You might be wondering - as I was - “How does anything ever change, then?”</p>

<p>The answer is by using a mediator to process events and produce the UI state.</p>

<blockquote>
  <p>Interactions and their logic may be housed in the UI itself, but this can quickly get unwieldy as the UI starts to become more than its name suggests: it becomes data owner, producer, transformer, and more. Furthermore, this can affect testability because the resulting code is a tightly coupled amalgam with no discernable boundaries. Unless the UI state is very simple, the UI’s sole responsibility should be to consume and display UI state.</p>
</blockquote>

<blockquote>
  <p>The classes that are responsible for the production of UI state and contain the necessary logic for that task are called <a href="https://developer.android.com/topic/architecture/ui-layer#state-holders">state holders</a>.</p>
</blockquote>

<blockquote>
  <p>Key Point: The <a href="https://developer.android.com/topic/libraries/architecture/viewmodel">ViewModel</a> type is the recommended implementation for the management of screen-level UI state with access to the data layer. Furthermore, it survives configuration changes (like rotations) automatically. ViewModel classes define the logic to be applied to events in the app and produce updated state as a result.</p>
</blockquote>

<blockquote>
  <p>There are many ways to model the codependency between the UI and its state producer. However, because the interaction between the UI and its ViewModel class can largely be understood as event input and its ensuing state output, the relationship can be represented as shown in the following diagram illustrating the “Unidirectional Data Flow” pattern:</p>
</blockquote>

<p><img src="/assets/images/blog-images/morso/udf.png" alt="Unidirectional Data Flow" /></p>

<ul>
  <li>The ViewModel holds and exposes the state to be consumed by the UI. The UI state is application data transformed by the ViewModel.</li>
  <li>The UI notifies the ViewModel of user events.</li>
  <li>The ViewModel handles the user actions and updates the state.</li>
  <li>The updated state is fed back to the UI to render.</li>
  <li>The above is repeated for any event that causes a mutation of state.</li>
</ul>

<p><a href="https://developer.android.com/topic/architecture/ui-layer#why-use-udf">Why use UDF?</a></p>

<p>Here’s an rudimentary example of what would happen if a user were to bookmark an article in a simple news app:</p>

<p><img src="/assets/images/blog-images/morso/udf-example.png" alt="Unidirectional Data Flow Example" /></p>

<!-- ##### Creating and Storing our UI State Class with ViewModel
For this section, we'll mostly be looking at the [Android article on `ViewModel`](https://developer.android.com/reference/androidx/lifecycle/ViewModel) and the [ViewModel Codelab](https://developer.android.com/codelabs/basic-android-kotlin-training-viewmodel#0). Neither of these resources mention services or IME's (only activities and fragments), but I don't see any reason not to use ViewModels.

Our first implementation of our `ViewModel` class will merely hold the value of what is displayed in `MorsoIME` - which happens to be `MorsoView`. Further down the line, we may end up with more views - in which case we could add to `MorsoViewModel` *or* create new viewmodels for each view to avoid a monolithic `MorsoViewModel`. "Why is the `ViewModel` assosciated with the service and not the view?" you might ask. The reason is that the viewmodel is tied to the lifecycle of the activity/fragment/service. Also - the view should retain no data about state - merely display it.

![Default MorsoView](/assets/images/blog-images/morso/morso-measured-view.png)

With that being said, let's create `MorsoViewModel`:

<pre><code class="language-kotlin">
package net.eldun.morso

import androidx.lifecycle.ViewModel

class MorsoViewModel : ViewModel() {
}
</code></pre>

Now we have to assosciate our `ViewModel` with our IME - we'll add a member of type `MorsoViewModel` to `MorsoIME` and initialize it using the `by viewModels()` property delegate:

<pre><code class="language-kotlin">
private val viewModel: MorsoViewModel by viewModels()
}
 </code></pre> -->

<!-- ###### What are Property Delegates?

> In Kotlin, each mutable (var) property has default getter and setter functions automatically generated for it. The setter and getter functions are called when you assign a value or read the value of the property.
>
> For a read-only property (val), it differs slightly from a mutable property. Only the getter function is generated by default. This getter function is called when you read the value of a read-only property.
>
> Property delegation in Kotlin helps you to handoff the getter-setter responsibility to a different class.
>
> This class (called delegate class) provides getter and setter functions of the property and handles its changes.
>
> A delegate property is defined using the by clause and a delegate class instance:
>
>
> // Syntax for property delegation
> var <property-name> : <property-type> by <delegate-class>()
> In your app, if you initialize the view model using default GameViewModel constructor, like below:
>
>
> private val viewModel = GameViewModel()
> Then the app will lose the state of the viewModel reference when the device goes through a configuration change. For example, if you rotate the device, then the activity is destroyed and created again, and you'll have a new view model instance with the initial state again.
>
> Instead, use the property delegate approach and delegate the responsibility of the viewModel object to a separate class called viewModels. That means when you access the viewModel object, it is handled internally by the delegate class, viewModels. The delegate class creates the viewModel object for you on the first access, and retains its value through configuration changes and returns the value when requested. -->

<!-- ###### Adding Data to our ViewModel
The article for this section can be found [here](https://developer.android.com/codelabs/basic-android-kotlin-training-viewmodel#4)

Right now, the only property in our `MorsoViewModel` is the the background text:
<pre><code class="language-kotlin">
    private var backgroundText = "Morso"
</code></pre>

However,

> Inside the ViewModel, the data should be editable, so they should be private and var. From outside the ViewModel, data should be readable, but not editable, so the data should be exposed as public and val. To achieve this behavior, Kotlin has a feature called a [backing property](https://kotlinlang.org/docs/properties.html#backing-properties).

</code></pre>
class MorsoViewModel : ViewModel() {

    private var _backgroundText = "Morso"
    val backgroundText: String
        get() = _backgroundText()
}
</code></pre>

Mutable data fields from the viewmodel should **never** be exposed. -->

<hr />

<h3 id="storing-ui-data-for-our-input-service">Storing UI Data for our Input Service</h3>
<p>As it turns out, we don’t actually have to use viewmodels, because <code class="language-plaintext highlighter-rouge">InputServiceMethod</code>s <a href="https://developer.android.com/reference/android/inputmethodservice/InputMethodService#onConfigurationChanged(android.content.res.Configuration)">don’t have to worry about configuration changes</a> - which is the main reason to use viewmodels (other than the seperation of ui from state, of course). As is often the case when traveling a bit off the beaten path, the <a href="https://github.com/android/architecture-components-samples/issues/137#issuecomment-327854042">answers are not always <em>totally</em> satisfactory</a>, though. Based on what I’ve read, it sounds like we can get away with a mere <a href="https://developer.android.com/topic/architecture/ui-layer/stateholders#choose_between_a_viewmodel_and_plain_class_for_a_state_holder">plain class for state holding</a>. Furthermore, we’ll make our UI state a <a href="https://en.wikipedia.org/wiki/Singleton_pattern">singleton</a> by using the <a href="https://stackoverflow.com/questions/51834996/singleton-class-in-kotlin">object keyword</a>.</p>

<p>The following info is from <a href="https://developer.android.com/codelabs/basic-android-kotlin-training-viewmodel#4">this codelab</a>. Even though the codelab is about viewmodels, the same principles still apply to our plain state class.</p>

<p>Create <code class="language-plaintext highlighter-rouge">MorsoUiState</code>:</p>

<p>Right now, the only property in our <code class="language-plaintext highlighter-rouge">MorsoUiState</code> is the the background text:</p>
<pre><code class="language-kotlin">
    private var backgroundText = "Morso"
</code></pre>

<p>However,</p>

<blockquote>
  <p>Inside the ViewModel, the data should be editable, so they should be private and var. From outside the ViewModel, data should be readable, but not editable, so the data should be exposed as public and val. To achieve this behavior, Kotlin has a feature called a <a href="https://kotlinlang.org/docs/properties.html#backing-properties">backing property</a>.</p>
</blockquote>

<pre><code class="language-kotlin">
object MorsoUiState {

    private var _backgroundText = "Morso"
    val backgroundText: String
        get() = _backgroundText()

    fun setBackgroundText(input: String) {
        _backgroundText = input
    }
}
</code></pre>

<p>Mutable data fields from state holders should <strong>never</strong> be exposed.</p>

<hr />

<h3 id="updating-our-view-with-new-ui-data">Updating our View with New UI Data</h3>

<!-- The main article for this section can be found [here](https://developer.android.com/topic/libraries/data-binding/architecture). -->

<p>We can automatically update our UI using <a href="https://developer.android.com/topic/libraries/architecture/livedata">LiveData</a> as the binding source.</p>

<p>First, we should update <code class="language-plaintext highlighter-rouge">MorsoView</code> with a function to update all of its fields(which should all be private) from the ui state and then redraw itself (using <a href="https://developer.android.com/reference/android/view/View#invalidate()"><code class="language-plaintext highlighter-rouge">invalidate</code></a>):</p>

<pre><code class="language-kotlin">
...

    private var backgroundText = "Morso"


    fun updateUi(morsoUiState: MorsoUiState) {
        backgroundText = morsoUiState.backgroundText.value.toString()

        invalidate()

    }
...
</code></pre>

<p>To <a href="https://developer.android.com/topic/libraries/architecture/livedata#work_livedata">work with LiveData</a>, we must follow these steps:</p>

<ol>
  <li><a href="https://developer.android.com/topic/libraries/architecture/livedata#create_livedata_objects">Create an instance of LiveData</a> to hold a certain type of data. This is usually done within your ViewModel class.</li>
</ol>

<p>but we’re not using a viewmodel hehe</p>

<pre><code class="language-kotlin">
object MorsoUiState {

    val backgroundText: MutableLiveData&lt;String&gt; by lazy {
        MutableLiveData&lt;String&gt;("Morso")
    }
}
</code></pre>

<ol>
  <li>Create an Observer object that defines the onChanged() method, which controls what happens when the LiveData object’s held data changes. You usually create an Observer object in a UI controller, such as an activity or fragment.</li>
</ol>

<pre><code class="language-kotlin">
class MorsoIME : InputMethodService() {
    private val TAG = "MorsoIME"


    override fun onCreateInputView(): View {
    val morsoLayout = layoutInflater.inflate(R.layout.input_container, null)
        morsoView = morsoLayout.findViewById&lt;MorsoView&gt;(R.id.morsoView)

        // Create the observer which updates the UI.
        val backgroundTextObserver = Observer&lt;String&gt; {

            // Update the UI
            morsoView.updateUi(morsoUiState)
        }

        return morsoLayout
    }

}
</code></pre>

<ol>
  <li>Attach the Observer object to the LiveData object using the observe() method. The observe() method takes a LifecycleOwner object. This subscribes the Observer object to the LiveData object so that it is notified of changes. You usually attach the Observer object in a UI controller, such as an activity or fragment.</li>
</ol>

<blockquote>
  <p>You can register an observer without an associated LifecycleOwner object using the observeForever(Observer) method. In this case, the observer is considered to be always active and is therefore always notified about modifications. You can remove these observers calling the removeObserver(Observer) method.</p>
</blockquote>

<pre><code class="language-kotlin">
class MorsoIME : InputMethodService() {
    private val TAG = "MorsoIME"

    lateinit var morsoView: MorsoView
    lateinit var morsoGestureListener : MorsoGestureListener
    lateinit var morsoUiState: MorsoUiState


    override fun onCreateInputView(): View {

        val morsoLayout = layoutInflater.inflate(R.layout.input_container, null)
        morsoView = morsoLayout.findViewById&lt;MorsoView&gt;(R.id.morsoView)
        morsoGestureListener = morsoView.gestureListener
        morsoUiState = morsoGestureListener.morsoUiState


        // Create the observer which updates the UI.
        val backgroundTextObserver = Observer&lt;String&gt; {

            // Update the UI
            morsoView.updateUi(morsoUiState)
            morsoView.invalidate()
        }

        // Observe the LiveData
        morsoUiState.backgroundText.observeForever(backgroundTextObserver)

        return morsoLayout
    }

}
</code></pre>

<p>When we have a more complex UI state, it might be worthwhile to make the whole <code class="language-plaintext highlighter-rouge">MorsoUiState</code> observable.</p>

<p>Add to <code class="language-plaintext highlighter-rouge">onSingleTapUp</code> in <code class="language-plaintext highlighter-rouge">MorsoGestureListener</code>:</p>

<pre><code class="language-kotlin">
   override fun onSingleTapUp(e: MotionEvent): Boolean {
+        morsoUiState.backgroundText.value = "tapped"
        return true
    }
</code></pre>

<p>Our “Morso” text will change to “tapped” on a single tap.</p>

<hr />

<h3 id="resetting-our-background-text-after-a-delay">Resetting our Background Text After a Delay</h3>
<p>Add the following code to the <code class="language-plaintext highlighter-rouge">backgroundTextObserver</code>:</p>

<pre><code class="language-diff-kotlin diff-highlight">
// Create the observer which updates the UI.
        val backgroundTextObserver = Observer&lt;String&gt; { newBackgroundText -&gt;
            Log.d(TAG, "onCreateInputView: New Text!")
            // Update the UI
            morsoView.backgroundText = newBackgroundText
            morsoView.invalidate()

+            if (morsoUiState.backgroundText.value != "Morso") {
+                Handler(Looper.getMainLooper()).postDelayed({
+                    morsoUiState.backgroundText.value = "Morso"
+                }, 1000)
            }
        }
</code></pre>

<p>We will be able to configure the delay in settings later in the series.</p>

<hr />

<h2 id="representing-morse-code">Representing Morse Code</h2>

<p>I figure <a href="https://kotlinlang.org/docs/enum-classes.html#working-with-enum-constants">enums</a> are a decent way to represent Morse code - we’re dealing with a few dozen values that will never change.</p>

<p>Another option would be to use an immutable ordered binary tree created at compile-time in a companion object. If you use a tree, be aware that <code class="language-plaintext highlighter-rouge">Enum.compareTo()</code> is <code class="language-plaintext highlighter-rouge">final</code> - the order in which the enums are declared is important for comparing/navigating the tree. <a href="https://stackoverflow.com/questions/519788/why-is-compareto-on-an-enum-final-in-java">Why is compareTo final?</a></p>

<p>Lets represent signals in <code class="language-plaintext highlighter-rouge">MorseSignal</code>:</p>
<pre><code class="language-kotlin">
enum class MorseSignal {
    DOT, DASH, SPACE;
}
</code></pre>

<p>and characters in <code class="language-plaintext highlighter-rouge">Character</code>:</p>
<pre><code class="language-kotlin">
enum class Character(vararg var sequence: MorseSignal) {

    START(),

    E(DOT),
    T(DASH),

    I(DOT, DOT),
    A(DOT, DASH),
    N(DASH, DOT),
    M(DASH, DASH),

    S(DOT, DOT, DOT),
    U(DOT, DOT, DASH),
    R(DOT, DASH, DOT),
    W(DOT, DASH, DASH),
    D(DASH, DOT, DOT),
    K(DASH, DOT, DASH),
    G(DASH, DASH, DOT),
    O(DASH, DASH, DASH),

    H(DOT, DOT, DOT, DOT),
    V(DOT, DOT, DOT, DASH),
    F(DOT, DOT, DASH, DOT),
    L(DOT, DASH, DOT, DOT),
    P(DOT, DASH, DASH, DOT),
    J(DOT, DASH, DASH, DASH),
    B(DASH, DOT, DOT, DOT),
    X(DASH, DOT, DOT, DASH),
    C(DASH, DOT, DASH, DOT),
    Y(DASH, DOT, DASH, DASH),
    Z(DASH, DASH, DOT, DOT),
    Q(DASH, DASH, DOT, DASH),

    FIVE(DOT, DOT, DOT, DOT, DOT) {
        override fun toString() = "5"
    },
    FOUR(DOT, DOT, DOT, DOT, DASH){
        override fun toString() = "4"
    },
    THREE(DOT, DOT, DOT, DASH, DASH){
        override fun toString() = "3"
    },
    TWO(DOT, DOT, DASH, DASH, DASH){
        override fun toString() = "2"
    },
    PLUS_SIGN(DOT, DASH, DOT, DASH, DOT){
        override fun toString() = "+"
    },
    ONE(DOT, DASH, DASH, DASH, DASH){
        override fun toString() = "1"
    },
    SIX(DASH, DOT, DOT, DOT, DOT){
        override fun toString() = "6"
    },
    EQUALS_SIGN(DASH, DOT, DOT, DOT, DASH){
        override fun toString() = "="
    },
    DIVIDE_SIGN(DASH, DOT, DOT, DASH, DOT){
        override fun toString() = "/"
    },
    SEVEN(DASH, DASH, DOT, DOT, DOT){
        override fun toString() = "7"
    },
    EIGHT(DASH, DASH, DASH, DOT, DOT){
        override fun toString() = "8"
    },
    NINE(DASH, DASH, DASH, DASH, DOT){
        override fun toString() = "9"
    },
    ZERO(DASH, DASH, DASH, DASH, DASH){
        override fun toString() = "0"
    },
    NULL(){
        override fun toString() = ""
    };
}


</code></pre>

<p>We’re going to want to be able to get the <code class="language-plaintext highlighter-rouge">Character</code> by its sequence once Morso detects a long enough break in input. To do so, we’ll create a map with the key being <code class="language-plaintext highlighter-rouge">sequence</code> and the value being the <code class="language-plaintext highlighter-rouge">Character</code>.</p>

<p>I was originally trying to use the <code class="language-plaintext highlighter-rouge">vararg sequence</code>(which is an array) as a key, but in order to look up the value, the array passed in <a href="https://stackoverflow.com/a/16839191">had to be the exact same array as the key</a> - not just the contents of the array. I ended up converting the sequence in <code class="language-plaintext highlighter-rouge">Character</code>’s construtor to a <code class="language-plaintext highlighter-rouge">List</code>, and using said list as a key for the dictionary:</p>

<pre><code class="language-kotlin">
    ...

    NINE(DASH, DASH, DASH, DASH, DOT){
        override fun toString() = "9"
    },
    ZERO(DASH, DASH, DASH, DASH, DASH){
        override fun toString() = "0"
    };

    private val sequenceList = this.sequence.asList()


    companion object {
        private val map = Character.values().associateBy(Character::sequenceList)
        fun fromSequenceList(seqList: List&lt;MorseSignal&gt;) = map[seqList]
    }
</code></pre>

<p>We can now pass in a list of signals to <code class="language-plaintext highlighter-rouge">fromSequenceList</code> to get the corresponding <code class="language-plaintext highlighter-rouge">Character</code>:</p>
<pre><code class="language-kotlin">
    class MorseTranslator {

        companion object {

            fun decode(vararg sequence: MorseSignal): Character? {

                return Character.fromSequenceList(sequence.asList())
            }
        }
    }
</code></pre>

<hr />

<h2 id="using-morso-for-input">Using Morso for Input</h2>

<p>Now we can start to create an actual input method! Again, the most helpful article for this section can be found <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method">here</a>.</p>

<p>My general idea for the default behavior of Morso is as follows:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">MorsoInputView</code> will show the current input (dots and dashes) up until there’s a word-length pause - at which point <code class="language-plaintext highlighter-rouge">MorsoInputView</code> will once again display ‘Morso’.</li>
  <li>The current input field will reflect the input, but will only be committed upon a word-length pause.</li>
  <li><code class="language-plaintext highlighter-rouge">MorsoInputView</code> will be updated to have a cancel button for the current sequence and a backspace.</li>
</ul>

<p>MorsoCandidatesView will come later.</p>

<hr />

<h2 id="translating-gestures-to-morse-code">Translating Gestures to Morse Code</h2>
<p>Taps are already handled in our <code class="language-plaintext highlighter-rouge">MorsoGestureListener</code>. However - it’s a bit picky about what counts as a tap and doesn’t register gestures like triple-taps. Additionally, we should allow the user to customize the dot time because the dash and space duration will be defined as multiples of the base dot time.</p>

<p>Let’s add to <code class="language-plaintext highlighter-rouge">MorsoGestureListener</code>:</p>

<pre><code class="language-kotlin">
    fun onHold(e: MotionEvent): Boolean {

        Log.d(TAG, "onHold")
        return true

    }
    fun onShortPause(e: MotionEvent): Boolean {
        Log.d(TAG, "onShortPause")
        morsoUiState.reset()
        return true
    }

    fun onLongPause(e: MotionEvent): Boolean {
        Log.d(TAG, "onLongPause")
        inputConnection.commitText(" ", 1)

        return true
    }
</code></pre>

<p>In <code class="language-plaintext highlighter-rouge">MorsoInputView</code>, add the following members:</p>

<pre><code class="language-diff-kotlin diff-highlight">
    class MorsoInputView @JvmOverloads constructor(
    context: Context,
    attrs: AttributeSet? = null,
    defStyleAttr: Int = 0
) : View (context, attrs, defStyleAttr) {

    private val TAG = "MorsoView"

    val gestureListener =  MorsoGestureListener()
    private val gestureDetector = GestureDetector(context, gestureListener)
+    private var downTime: Long = 0
+    private var upTime: Long = 0
+    private val dotTime: Long = 300
+    private val dashTime = 3*dotTime
+    private val signalSpaceTimeout = dotTime
+    private val letterSpaceTimeout: Long = 3*dotTime
+    private val wordSpaceTimeout: Long = 7*dotTime



    ...
</code></pre>

<p>and then update <code class="language-plaintext highlighter-rouge">onTouchEvent</code>:</p>

<pre><code class="language-kotlin">

        override fun onTouchEvent(event: MotionEvent): Boolean {

        val onHoldRunnable = Runnable { gestureListener.onHold(event) }
        val shortPauseRunnable = Runnable { gestureListener.onShortPause(event) }
        val longPauseRunnable = Runnable { gestureListener.onLongPause(event) }


        if (event.actionMasked == MotionEvent.ACTION_DOWN) {
            downTime = SystemClock.elapsedRealtime()

            // Cancel possible pending runnables
            handler.removeCallbacksAndMessages(null)
            
            // Call onHold in dashTime ms
            handler.postDelayed(onHoldRunnable,dashTime)
        }




        else if (event.actionMasked == MotionEvent.ACTION_UP) {
            // Cancel the pending hold runnable and previous pause runnables
            handler.removeCallbacksAndMessages(null)

            upTime = SystemClock.elapsedRealtime()



            // Listen for all taps with no restrictions (slop, triple-taps, etc. - unlike the default gesture detector)
            val elapsedTime = upTime - downTime
            if (elapsedTime &lt; dotTime){
                gestureListener.onSingleTapUp(event)
            }


            // call timeouts if no input has been received
            handler.postDelayed(shortPauseRunnable, letterSpaceTimeout)
            handler.postDelayed(longPauseRunnable, wordSpaceTimeout)
            
            return true
        }
        
        // It's up to MorsoGestureListener to decide
        return gestureDetector.onTouchEvent(event)
    }
</code></pre>

<hr />

<h2 id="implementing-candidates-view">Implementing Candidates View</h2>
<p>The <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#CandidateView">candidates view</a> is something you’re likely familiar with:</p>

<p><img src="/assets/images/blog-images/morso/candidates-view.png" alt="Candidates View" /></p>

<p>For Morso, I’d like to display the left child character, the current character (which can function as a countdown bar to the current character being committed), and the right child character in the candidates view. The rightmost suggestion can also double as a progress bar for dash inputs. One concern is that this is a <em>practice</em> application, so I’m not sure if I should make the suggestions clickable.</p>

<p><span class="todo">Add progress bars to appropriate candidates</span></p>

<p>First, we have to create a view <code class="language-plaintext highlighter-rouge">MorsoCandidateView</code>:</p>

<pre><code class="language-kotlin">
class MorsoCandidateView @JvmOverloads constructor(
    context: Context,
    attrs: AttributeSet? = null,
    defStyleAttr: Int = 0
) : androidx.appcompat.widget.AppCompatButton (context, attrs, defStyleAttr) {

    private val TAG = "MorsoCandidatesView"

    init {
        setBackgroundColor(Color.DKGRAY)
        setTextColor(Color.WHITE)
        gravity = Gravity.CENTER
    }


}
</code></pre>

<p>Next, we have to create a layout <code class="language-plaintext highlighter-rouge">candidates.xml</code>:</p>

<pre><code class="language-xml">
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    xmlns:tools="http://schemas.android.com/tools"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:background="@color/black"&gt;

    &lt;net.eldun.morso.MorsoCandidateView
        android:id="@+id/morsoCandidateView"
        android:layout_width="0dp"
        android:layout_height="wrap_content"
        android:padding="10dp"
        android:text="left"
        app:layout_constraintBottom_toBottomOf="parent"
        app:layout_constraintEnd_toStartOf="@+id/guideline3"
        app:layout_constraintStart_toStartOf="parent"
        app:layout_constraintTop_toTopOf="parent" /&gt;

    &lt;androidx.constraintlayout.widget.Guideline
        android:id="@+id/guideline3"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:orientation="vertical"
        app:layout_constraintGuide_percent=".3333" /&gt;

    &lt;net.eldun.morso.MorsoCandidateView
        android:id="@+id/morsoCandidateView2"
        android:layout_width="0dp"
        android:layout_height="wrap_content"
        android:padding="10dp"
        android:text="center"
        app:layout_constraintBottom_toBottomOf="parent"
        app:layout_constraintEnd_toStartOf="@+id/guideline4"
        app:layout_constraintStart_toStartOf="@+id/guideline3"
        app:layout_constraintTop_toTopOf="parent" /&gt;

    &lt;androidx.constraintlayout.widget.Guideline
        android:id="@+id/guideline4"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:orientation="vertical"
        app:layout_constraintGuide_percent=".6667" /&gt;

    &lt;net.eldun.morso.MorsoCandidateView
        android:id="@+id/morsoCandidateView3"
        android:layout_width="0dp"
        android:layout_height="wrap_content"
        android:padding="10dp"
        android:text="right"
        app:layout_constraintBottom_toBottomOf="parent"
        app:layout_constraintEnd_toEndOf="parent"
        app:layout_constraintStart_toStartOf="@+id/guideline4"
        app:layout_constraintTop_toTopOf="parent" /&gt;

&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;
</code></pre>

<p><img src="/assets/images/blog-images/morso/candidates-layout.png" alt="Candidates layout" /></p>

<blockquote>
  <p><a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#CandidateView">In the IME lifecycle, the system calls onCreateCandidatesView() when it’s ready to display the candidates view. In your implementation of this method, return a layout that shows word suggestions, or return null if you don’t want to show anything.</a></p>
</blockquote>

<blockquote>
  <p><a href="https://developer.android.com/reference/android/inputmethodservice/InputMethodService#onCreateCandidatesView()">To control when the candidates view is displayed, use setCandidatesViewShown(boolean). To change the candidates view after the first one is created by this function, use setCandidatesView(android.view.View).</a></p>
</blockquote>

<p><span class="todo">I think that since this is a practice tool, it’s alright to show the candidates view at all times. I can always add a setting to hide it later.</span></p>

<p>To display our new candidates layout, all we need to do is override <code class="language-plaintext highlighter-rouge">MorsoIME.onCreateCandidatesView()</code>:</p>

<pre><code class="language-kotlin">
    override fun onCreateCandidatesView(): View {
        Log.d(TAG, "onCreateCandidatesView")
        return layoutInflater.inflate(R.layout.candidates, null)
    }

</code></pre>

<p>and <code class="language-plaintext highlighter-rouge">setCandidatesViewShown(true)</code> from <code class="language-plaintext highlighter-rouge">MorsoIME.onCreateInputView()</code>.</p>

<p><img src="/assets/images/blog-images/morso/morso-candidates-runtime.png" alt="Morso candidates at runtime" /></p>

<hr />

<h3 id="updating-candidates">Updating Candidates</h3>
<blockquote>
  <p><a href="https://developer.android.com/reference/android/inputmethodservice/InputMethodService#onCreateCandidatesView()">To change the candidates view after the first one is created by setCandidatesViewShown(), use setCandidatesView(android.view.View).</a></p>
</blockquote>

<p>The default candidates will be “E”, “”, and “T”, respectively (I overrode Character.START’s <code class="language-plaintext highlighter-rouge">toString()</code> to return “”). We’ll be using our Character enum class to look up candidates, similar to how we looked up values by a character’s sequence earlier on:</p>

<pre><code class="language-diff-kotlin diff-highlight">
    companion object {
        val TAG = "Character"

        private val sequenceMap = values().associateBy(Character::sequenceList)
+        private val stringMap = values().associateBy(Character::toString)
+
        fun fromSequenceList(seqList: Listlt:MorseSignal&gt;) = sequenceMap[seqList]
+        fun fromString(stringifiedCharacter: String) = stringMap[stringifiedCharacter]
+
</code></pre>

<p>Let’s also add functions to retrieve the possible options from the current sequence:</p>

<pre><code class="language-kotlin">
fun getDotChild(character: Character): Character {
            val result = fromSequenceList(character.sequenceList + DOT)
            if (result == null)
                return Character.NULL
            return result
        }

        fun getDotChild(characterString: String?): Character {
            val character = fromString(characterString!!)
            return getDotChild(character!!)
        }


        fun getDashChild(character: Character): Character {
            val result = fromSequenceList(character.sequenceList + DASH)
            if (result == null)
                return Character.NULL
            return result
        }

        fun getDashChild(characterString: String?): Character {
            val character = fromString(characterString!!)
            return getDashChild(character!!)
        }
</code></pre>

<p>We can add our candidates to <code class="language-plaintext highlighter-rouge">MorsoUiState</code> now.</p>

<pre><code class="language-kotlin">
object MorsoUiState {

    val backgroundText: MutableLiveData&lt;String&gt; by lazy {
        MutableLiveData&lt;String&gt;("Morso")
    }

    // Default characters
    val currentCandidateText: MutableLiveData&lt;String&gt; by lazy {
            MutableLiveData&lt;String&gt;(Character.START.toString())
        }
    val dotCandidateText: MutableLiveData&lt;String&gt; by lazy {
        MutableLiveData&lt;String&gt;(Character.E.toString())
    }
    val dashCandidateText: MutableLiveData&lt;String&gt; by lazy {
        MutableLiveData&lt;String&gt;(Character.T.toString())
    }

    fun reset() {
        backgroundText.value = "Morso"
        currentCandidateText.value = Character.START.toString()
        dotCandidateText.value = Character.E.toString()
        dashCandidateText.value = Character.T.toString()
    }
}
</code></pre>

<p>We can update our <code class="language-plaintext highlighter-rouge">MorsoUiStateObserver</code> like so:</p>

<pre><code class="language-kotlin">
class MorsoUiStateObserver(val morso: MorsoIME, val uiState: MorsoUiState) {

    init {

        observeBackgroundText()
        observeCandidates()
    }

    private fun observeBackgroundText() {
        // Create the observer which updates the UI.
        val backgroundTextObserver = Observer&lt;String&gt; {

            morso.updateUi()

            if (uiState.backgroundText.value != "Morso") {
                Handler(Looper.getMainLooper()).postDelayed({
                    uiState.backgroundText.value = "Morso"
                }, 1000)
            }
        }

        // Observe the LiveData
        uiState.backgroundText.observeForever(backgroundTextObserver)
    }

    private fun observeCandidates() {

        // Create the observer which updates the UI.
        val candidatesTextObserver = Observer&lt;String&gt; {
            morso.updateUi()
        }

        // Observe the LiveData
        uiState.currentCandidateText.observeForever(candidatesTextObserver)
        uiState.dotCandidateText.observeForever(candidatesTextObserver)
        uiState.dashCandidateText.observeForever(candidatesTextObserver)
    }


}
</code></pre>

<p>Now we can add logic to <code class="language-plaintext highlighter-rouge">MorsoGestureListener</code>, which will notify the UiStateObserver when candidate values change.</p>

<pre><code class="language-kotlin">
    ...
    override fun onSingleTapUp(e: MotionEvent): Boolean {
        Log.d(TAG, "onSingleTapUp")
        morsoUiState.backgroundText.value.apply { "." }

        updateCandidates(MorseSignal.DOT)

        inputConnection.commitText("!", 1)

        return true
    }



    fun onHold(e: MotionEvent): Boolean {

        updateCandidates(MorseSignal.DASH)
        Log.d(TAG, "onHold")
        return true

    }

    private fun updateCandidates(signal: MorseSignal) {

        if (signal == MorseSignal.DOT){
            morsoUiState.currentCandidateText.value = morsoUiState.dotCandidateText.value

            val newCurrent = Character.fromString(morsoUiState.currentCandidateText.value.toString())

            morsoUiState.dotCandidateText.value = Character.getDotChild(newCurrent!!).toString()
            morsoUiState.dashCandidateText.value = Character.getDashChild(newCurrent!!).toString()
        }

        else if (signal == MorseSignal.DASH){
            morsoUiState.currentCandidateText.value = morsoUiState.dashCandidateText.value

            val newCurrent = Character.fromString(morsoUiState.currentCandidateText.value.toString())

            morsoUiState.dotCandidateText.value = Character.getDotChild(newCurrent!!).toString()
            morsoUiState.dashCandidateText.value = Character.getDashChild(newCurrent!!).toString()
        }
    }
</code></pre>

<p>Finally, we have to update our IME class:</p>

<pre><code class="language-diff-kotlin diff-highlight">
class MorsoIME : InputMethodService() {
    private val TAG = "MorsoIME"
    lateinit var morsoInputView: MorsoInputView

+    lateinit var candidatesLayout: View
+    private var candidatesVisible = false

    lateinit var morsoGestureListener : MorsoGestureListener
    private val morsoUiState = MorsoUiState
    lateinit var morsoUiStateObserver: MorsoUiStateObserver



    /**
     * Create and return the view hierarchy used for the input area (such as
     * a soft keyboard).  This will be called once, when the input area is
     * first displayed.  You can return null to have no input area; the default
     * implementation returns null.
     *
     * &lt;p&gt;To control when the input view is displayed, implement
     * {@link #onEvaluateInputViewShown()}.
     * To change the input view after the first one is created by this
     * function, use {@link #setInputView(View)}.
     */
    override fun onCreateInputView(): View {
//        android.os.Debug.waitForDebugger()

        val morsoLayout = layoutInflater.inflate(R.layout.morso, null)
        morsoInputView = morsoLayout.findViewById&lt;MorsoInputView&gt;(R.id.morsoInputView)
        morsoGestureListener = morsoInputView.gestureListener
+       morsoGestureListener.inputConnection = currentInputConnection
        morsoUiStateObserver = MorsoUiStateObserver(this, morsoUiState)

        setCandidatesViewShown(true)

        return morsoLayout
    }


+    /**
+     * Set Morso's GestureListener to the updated selection
+     */
+    override fun onUpdateSelection(
+        oldSelStart: Int,
+        oldSelEnd: Int,
+        newSelStart: Int,
+        newSelEnd: Int,
+        candidatesStart: Int,
+        candidatesEnd: Int
+    ) {
+        super.onUpdateSelection(
+            oldSelStart,
+            oldSelEnd,
+            newSelStart,
+            newSelEnd,
+            candidatesStart,
+            candidatesEnd
+        )
+
+        morsoGestureListener.inputConnection = currentInputConnection
+    }

+    override fun onCreateCandidatesView(): View {
+
+        candidatesVisible = true
+
+        candidatesLayout = layoutInflater.inflate(R.layout.candidates, null)
+
+        return candidatesLayout
+    }
+
+    override fun onFinishCandidatesView(finishingInput: Boolean) {
+        candidatesVisible = false
+        super.onFinishCandidatesView(finishingInput)
+    }

    /**
     * Called automatically from MorsoUiStateObserver whenever the state changes.
     */
+    fun updateUi() {
+        morsoInputView.updateUi(morsoUiState)
+
+        if (candidatesVisible) {
+            var current = candidatesLayout.findViewById&lt;MorsoCandidateView&gt;(R.id.morsoCurrentCandidate)
+            var dot = candidatesLayout.findViewById&lt;MorsoCandidateView&gt;(R.id.morsoDotCandidate)
+            var dash = candidatesLayout.findViewById&lt;MorsoCandidateView&gt;(R.id.morsoDashCandidate)
+
+            Log.d(TAG, "updateUi pre: ${current.text} ${dot.text} ${dash.text}")
+            current.text = morsoUiState.currentCandidateText.value
+            dot.text = morsoUiState.dotCandidateText.value
+            dash.text = morsoUiState.dashCandidateText.value
+            Log.d(TAG, "updateUi post: ${current.text} ${dot.text} ${dash.text}")
+
+
+            candidatesLayout.invalidate()
+        }
+
+    }

}
</code></pre>

<p>The result:</p>

<p><img src="/assets/images/blog-images/morso/candidates.gif" alt="Morso basic candidates operation GIF" /></p>

<p><span class="todo">I just found out that there’s a widget called <a href="https://developer.android.com/reference/android/widget/TextSwitcher">TextSwitcher</a> which is useful for animating text labels. We can implement them later in the series.</span></p>

<hr />
<h2 id="sending-input">Sending Input</h2>

<p><a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#SendText">Main article</a></p>

<p>Now that we register all the correct gestures and update the UI appropriately (minimally, at this point), we can use Morso to send info to text fields.</p>

<p>First, we need to remove the background text reset logic from <code class="language-plaintext highlighter-rouge">MorsoUiStateObserver</code>:</p>

<pre><code class="language-kotlin">
 private fun observeBackgroundText() {
        // Create the observer which updates the UI.
        val backgroundTextObserver = Observer&lt;String&gt; {

            morso.updateUi()

-            if (uiState.backgroundText.value != "Morso") {
-                Handler(Looper.getMainLooper()).postDelayed({
-                    uiState.backgroundText.value = "Morso"
-                }, 1000)
-            }
        }

        // Observe the LiveData
        uiState.backgroundText.observeForever(backgroundTextObserver)
    }
</code></pre>

<p>Add a member <code class="language-plaintext highlighter-rouge">DEFAULT_BACKGROUND_TEXT</code> to <code class="language-plaintext highlighter-rouge">MorsoUiState</code>:</p>

<pre><code class="language-diff-kotlin diff-highlight">
object MorsoUiState {

+    val DEFAULT_BACKGROUND_TEXT = "Morso"

    val backgroundText: MutableLiveData&lt;String&gt; by lazy {
!        MutableLiveData&lt;String&gt;(DEFAULT_BACKGROUND_TEXT)
    }

    // Default characters
    val currentCandidateText: MutableLiveData&lt;String&gt; by lazy {
            MutableLiveData&lt;String&gt;(Character.START.toString())
        }
    val dotCandidateText: MutableLiveData&lt;String&gt; by lazy {
        MutableLiveData&lt;String&gt;(Character.E.toString())
    }
    val dashCandidateText: MutableLiveData&lt;String&gt; by lazy {
        MutableLiveData&lt;String&gt;(Character.T.toString())
    }

    fun reset() {
!        backgroundText.value = DEFAULT_BACKGROUND_TEXT
        currentCandidateText.value = Character.START.toString()
        dotCandidateText.value = Character.E.toString()
        dashCandidateText.value = Character.T.toString()
    }
}
</code></pre>

<p>Now all we have to do is update our gesture listener actions:</p>

<pre><code class="language-diff-kotlin diff-highlight">
class MorsoGestureListener : GestureDetector.SimpleOnGestureListener() {

    val TAG = "MorsoGestureListener"

    private var morsoUiState = MorsoUiState
    lateinit var inputConnection: InputConnection

    /**
     * Notified when a tap occurs with the down [MotionEvent]
     * that triggered it. This will be triggered immediately for
     * every down event. All other events should be preceded by this.
     *
     * @param e The down motion event.
     */
    override fun onDown(e: MotionEvent): Boolean {
        return true
    }

    /**
     * Notified when a tap occurs with the up [MotionEvent]
     * that triggered it.
     *
     * @param e The up motion event that completed the first tap
     * @return true if the event is consumed, else false
     */
    override fun onSingleTapUp(e: MotionEvent): Boolean {
        Log.d(TAG, "onSingleTapUp")

+        if (updateCandidates(MorseSignal.DOT))
+            showUserInput(".")

        return true
    }


    fun onHold(e: MotionEvent): Boolean {
        Log.d(TAG, "onHold")


+        if (updateCandidates(MorseSignal.DASH))
+            showUserInput("-")

        return true

    }

    fun onShortPause(e: MotionEvent): Boolean {
        Log.d(TAG, "onShortPause")
        inputConnection.commitText(morsoUiState.currentCandidateText.value, 1)

        morsoUiState.reset()
        return true
    }

    fun onLongPause(e: MotionEvent): Boolean {
        Log.d(TAG, "onLongPause")
+        inputConnection.commitText(" ", 1)

        return true
    }

+    private fun showUserInput(input: String) {
+
+        if (morsoUiState.backgroundText.value.equals(morsoUiState.DEFAULT_BACKGROUND_TEXT))
+            morsoUiState.backgroundText.value = input
+        else
+            morsoUiState.backgroundText.value += input
+
+    }

+    /**
+     * Update current candidate, dot candidate, and dash candidate IF the character at @param signal
+     * from the current sequence is not null.
+     *
+     * @param signal the newest signal added to the sequence
+     *
+     * @return true if the candidates were updated, otherwise false
+     */
+    private fun updateCandidates(signal: MorseSignal): Boolean {
+
+        if (signal == MorseSignal.DOT) {
+            val dotChild = Character.getDotChild(morsoUiState.currentCandidateText.value)
+
+            if (dotChild == Character.NULL) {
+                return false
+            } 
+            
+            else {
+                val newCurrent = dotChild.toString()
+
+                morsoUiState.currentCandidateText.value = newCurrent
+
+                morsoUiState.dotCandidateText.value = Character.getDotChild(newCurrent).toString()
+                morsoUiState.dashCandidateText.value = Character.getDashChild(newCurrent).toString()
+
+                return true
+            }
+        }
+
+        else if (signal == MorseSignal.DASH) {
+            val dashChild = Character.getDashChild(morsoUiState.currentCandidateText.value)
+
+            if (dashChild == Character.NULL) {
+                return false
+            } 
+            
+            else {
+                val newCurrent = dashChild.toString()
+
+                morsoUiState.currentCandidateText.value = newCurrent
+
+                morsoUiState.dotCandidateText.value = Character.getDotChild(newCurrent).toString()
+                morsoUiState.dashCandidateText.value = Character.getDashChild(newCurrent).toString()
+
+                return true
+            }
+        }

</code></pre>

<p><img src="/assets/images/blog-images/morso/part-1-complete.gif" alt="Morso: part one" /></p>

<hr />

<h2 id="to-be-continued">To be continued</h2>

<p>In the next part of this series, we can focus on making the user experience more polished by adding more visual &amp; haptic feedback, creating a settings screen, and <a href="https://developer.android.com/develop/ui/views/touch-and-input/creating-input-method#GeneralDesign">addressing more general IME considerations</a>.</p>]]></content><author><name>Evan</name></author><category term="android" /><category term="kotlin" /><category term="java" /><category term="morso" /><summary type="html"><![CDATA[I need to learn Morse code if I want to be able to communicate with my cool neighbors.]]></summary></entry><entry><title type="html">Audio Plugins</title><link href="eldun.github.io/2022/10/20/clap-plugins-part-one.html" rel="alternate" type="text/html" title="Audio Plugins" /><published>2022-10-20T00:00:00-04:00</published><updated>2022-10-20T00:00:00-04:00</updated><id>eldun.github.io/2022/10/20/clap-plugins-part-one</id><content type="html" xml:base="eldun.github.io/2022/10/20/clap-plugins-part-one.html"><![CDATA[<h2 id="an-introduction">An Introduction</h2>
<hr />
<h3 id="what-is-an-audio-plugin">What is an Audio Plugin?</h3>

<p>An audio plugin is a piece of software (most often a virtual instrument or effect) that integrates into a <a href="https://en.wikipedia.org/wiki/Digital_audio_workstation">Digital Audio Workstation(DAW)</a> such as <a href="reaper.fm">Reaper</a> or <a href="ableton.com">Ableton Live</a>. There are quite a few different audio plugin formats - the most popular ones being:</p>
<ul>
  <li><a href="https://www.steinberg.net/technology/">VST3</a> - Steinberg’s closed-source solution turned open-source</li>
  <li><a href="https://developer.apple.com/documentation/audiotoolbox/audio_unit_v3_plug-ins">AUv3</a> - The iOS standard</li>
  <li><a href="https://www.avid.com/avid-plugins-by-category">AAX</a> - Avid/ProTools’ solution</li>
  <li>Standalone - As the name would imply, these types of plugins don’t require any host DAW. They can be launched and immediately tinkered with(think NotePad).</li>
</ul>

<p>Many developers release their audio plugins under multiple formats - often by using licensed tools like <a href="https://juce.com/">JUCE</a>.</p>

<hr />
<h3 id="choosing-a-format">Choosing a Format</h3>
<p><a href="https://lwn.net/Articles/890272/">Each format has their pros and cons</a>. After starting and abandoning <a href="https://github.com/eldun/eldun.github.io/blob/source/_drafts/simple-synth.md">VST3</a> due to Linux pains and <a href="https://github.com/eldun/eldun.github.io/blob/source/_drafts/simple-lv2-synth.md">LV2</a> due to GUI issues in Reaper, I’m ready to try <a href="https://cleveraudio.org/">CLAP</a>. my fingers are crossed.</p>

<hr />
<h4 id="why-choose-clap">Why Choose CLAP?</h4>
<p><a href="https://cleveraudio.org/the-story-and-mission/">So many</a> <a href="https://u-he.com/community/clap/">reasons</a>.</p>

<!--### What is LV2?
From [lv2pkug.in](https://lv2plug.in/): 
> LV2 is an extensible open standard for audio plugins. LV2 has a simple core interface, which is accompanied by extensions that add more advanced functionality.
>
> Many types of plugins can be built with LV2, including audio effects, synthesizers, and control processors for modulation and automation. Extensions support more powerful features, such as:
>
> - Platform-native UIs
> - Network-transparent plugin control
> - Portable and archivable persistent state
> - Non-realtime tasks (like file loading) with sample-accurate export
> - Semantic control with meaningful control designations and value units
> - The LV2 specification and accompanying code is permissively licensed free software, with support for all major platforms.
  

### Why Choose LV2?
I originally was going to create a VST3 synth([I even started a blog post :c](https://github.com/eldun/eldun.github.io/blob/source/_drafts/simple-synth.md)), but found Steinberg's [documentation](https://steinbergmedia.github.io/vst3_dev_portal/pages/) to be lacking and poorly organized - especially for the Linux platform, which is where I'm doing most of my coding as of late. LV2, on the other hand is platform-agnostic, [well-documented](https://lv2plug.in/pages/developing.html), and open-source from the start. 

[Here's a list of reasons to use LV2 straight from the source](https://lv2plug.in/pages/why-lv2.html). 
-->

<hr />
<h3 id="what-does-an-audio-plugin-look-like">What Does an Audio Plugin Look Like?</h3>
<p>There are thousands upon thousands of plugins out there - ranging from minimalist retro synths and complex rhythm sequencers to Karplus-Strong string modelers and destructive bit-crushers. Here are some of my favorites:</p>

<p><span class="row-fill">
<img src="/assets/images/blog-images/audio-plugins/part-one/vital.jpg" alt="Vital" />
<img src="/assets/images/blog-images/audio-plugins/part-one/dexed.png" alt="Dexed" />
</span>
<span class="row-fill">
<a href="vital.audio">Vital</a>
<a href="https://asb2m10.github.io/dexed/">Dexed</a>
</span></p>

<p><span class="row-fill">
<img src="/assets/images/blog-images/audio-plugins/part-one/valhalla-delay.webp" alt="Valhalla Freq Echo" />
<img src="/assets/images/blog-images/audio-plugins/part-one/blue-arp.png" alt="BlueARP Arpeggiator" />
</span> 
<span class="row-fill">
<a href="https://valhalladsp.com/shop/delay/valhalla-freq-echo/">Valhalla Freq Echo</a>
<a href="https://omg-instruments.com/wp/?page_id=63">BlueARP Arpeggiator</a> 
</span></p>

<p>It should be noted that although flashy UIs are fun and often very useful, they’re not necessary. Take a look at Reaper’s built-in synth - ReaSynth:
<img src="/assets/images/blog-images/audio-plugins/part-one/reasynth.png" alt="ReaSynth" /></p>

<p>FX plugins are also often very sparse.</p>

<!--## Setting Up LV2

### Resources
LV2 doesn't have an official guide, but comes with a few well-documented example plugins. There's also a "[book](https://lv2plug.in/book/#_introduction)" that walks through the included examples. 

A short list of development topics can be read about [here](https://lv2plug.in/pages/developing.html).

### Downloading LV2
At the time of this writing, LV2 can be downloaded from [LV2's homepage](https://lv2plug.in/). There's also a [gitlab repository](https://gitlab.com/lv2/lv2). 

### Installing LV2
[Installation instructions](https://gitlab.com/lv2/lv2/-/blob/master/INSTALL.md) are included in the LV2 download. 



#### Installing Dependency Meson
- [Meson](https://mesonbuild.com/index.html) is LV2's chosen build system.
- [Installing Meson](https://mesonbuild.com/Getting-meson.html): `sudo apt install meson`. 
- [Getting Started with Meson](https://mesonbuild.com/Quick-guide.html).

#### Configuration
> The build is configured with the setup command, which creates a new build directory with the given name:
>
> `meson setup build`

(Just run `meson setup build` from the LV2 root directory)

From within the newly created `build` directory, you can:
- [`meson compile`](https://gitlab.com/lv2/lv2/-/blob/master/INSTALL.md#building)
- [`meson test`](https://gitlab.com/lv2/lv2/-/blob/master/INSTALL.md#building)
- [`meson install`](https://gitlab.com/lv2/lv2/-/blob/master/INSTALL.md#installation)

> You may need to acquire root permissions to install to a system-wide prefix. For packaging, the installation may be staged to a directory using the DESTDIR environment variable or the --destdir option:
> 
> <pre><code class="language-console">
> DESTDIR=/tmp/mypackage/ meson install
> 
> meson install --destdir=/tmp/mypackage/
> </code></pre> 

> By default, on UNIX-like systems, everything is installed within the <code>prefix</code>,
and LV2 bundles are installed in the "lv2" subdirectory of the <code>libdir</code>.  On
other systems, bundles are installed by default to the standard location for
plugins on the system. The bundle installation directory can be overridden
with the <code>lv2dir</code> option.
> The specification bundles are run-time dependencies of LV2 applications.
Programs expect their data to be available somewhere in `LV2_PATH`.  See
[http://lv2plug.in/pages/filesystem-hierarchy-standard.html](http://lv2plug.in/pages/filesystem-hierarchy-standard.html ) for details on the
standard installation paths.

> Configuration options(such as `lv2dir`) can be inspected with the `configure` command from within the `build` directory:
> 
> <pre><code class="language-console">
> cd build
> meson configure
> </code></pre> 

> Options can be set by passing C-style "define" options to configure:
>
> `meson configure -Dc_args="-march=native" -Dprefix="/opt/mypackage/"`
>
>
> Note that some options, such as strict and werror are for
developer/maintainer use only.  Please don't file issues about anything that
happens when they are enabled.
-->

<hr />
<h2 id="setting-up-an-environment-for-clap">Setting up an Environment for CLAP</h2>
<p><a href="https://clapdb.tech/category/hostsdaws">CLAP isn’t widely supported yet</a>. However, <a href="bitwig.com">BitWig</a> - a DAW that’s been gaining a lot of steam recently - supports it. My current DAW of choice - <a href="reaper.fm">Reaper</a> <del><a href="https://forum.cockos.com/showthread.php?t=267906">may be supporting it before long</a></del> supports it on the dev branch, as of <a href="https://audiosex.pro/threads/reaper-c-l-a-p-support-now-a-reality.65864/">less than a month ago</a>. I’ll be using Reaper v6.68+dev1004 (October 4).</p>

<p>Just to make sure everything was set up correctly, I installed <a href="https://github.com/asb2m10/dexed">dexed</a> from source and tested it in Reaper.</p>

<p><img src="/assets/images/blog-images/audio-plugins/part-one/dexed.png" alt="The CLAP version of dexed running in Reaper" /></p>

<hr />
<h2 id="learning-to-create-clap-plugins">Learning to Create CLAP Plugins</h2>
<hr />
<h3 id="where-to-start">Where to Start?</h3>
<p>I don’t know much about creating plugins, and unfortunately, at the time of this post, it looks like the <a href="https://cleveraudio.org/developers-getting-started/">“Getting Started” page has a ways to go</a>.</p>

<p><img src="/assets/images/blog-images/audio-plugins/part-one/clap-getting-started.png" alt="CLAP Documentation Under Construction" /></p>

<p>The way forward is through <a href="https://github.com/free-audio/clap#examples">example</a> and <a href="https://www.youtube.com/playlist?list=PLqRWeSPiYQ64DhTiE2dEIF5xRIw0s5XLS">this youtube playlist</a>(Developing with CLAP - Jürgen Moßgraber).</p>

<p>Actually, I watched the videos and they don’t help that much. Let’s move on.</p>

<hr />
<h3 id="building-example-plugins">Building Example Plugins</h3>
<p><a href="https://github.com/free-audio/clap-plugins">This repo</a> is full of juicy info on CLAP plugins.</p>

<p>The repo has a lengthy note about GUIs, builds and symbols:</p>

<blockquote>
  <p>The plugins use Qt for the GUI.</p>

  <p>It is fine to dynamically link to Qt for a host, but it is very dangerous for a plugin.</p>

  <p>Also one very important aspect of the plugin is the distribution. Ideally a clap plugin should be self contained: it should not rely upon symbols from the host, and it should export only one symbol: clap_entry.</p>

  <p>You should be aware that even if you hide all your symbols some may still remain visible at unexpected places. Objective-C seems to register every classes including those coming from plugins in a flat namespace. Which means that if two plugins define two different Objective-C classes but with the same, they will clash which will result in undeflined behavior.</p>

  <p>Qt uses a few Objective-C classes on macOS. So it is crucial to use QT_NAMESPACE.</p>

  <p>We have two different strategies to work with that.</p>

  <p>local: statically link every thing
remote: start the gui in a child process</p>
  <ol>
    <li>has the advantage of being simple to deploy. 2. is more complex due to its inter-process nature. It has a few advantages:</li>
  </ol>

  <p>if the GUI crash, the audio engine does not
the GUI can use any libraries, won’t be subject to symbol or library clash etc…
We abstracted the relation between the plugin and the GUI: AbstractGui and AbstractGuiListener which lets us transparently insert proxies to support the remote model.</p>

  <p>The GUI itself work with proxy objects to the parameters, transport info, … They are then bound into QML objects. See Knob.qml and parameter-proxy.hh.</p>

  <p>We offer two options:</p>

  <p>static build, cmake preset: ninja-vcpkg or vs-vcpkg on Windows.
dynamic builg, cmake preset: ninja-system
Static builds are convenient for deployment as they are self containded. They use the local gui model.</p>

  <p>Dynamic builds will get your started quickly if your system provides Qt6, and you have an host that do not expose the Qt symbols. Static builds will require more time and space.</p>
</blockquote>

<p>(<a href="https://stackoverflow.com/a/311889">Difference between build methods</a>)</p>

<p>Build instructions for different platforms can be found <a href="https://github.com/free-audio/clap-plugins#building-on-various-platforms">here</a>.</p>

<p>The plugins will be installed to <code class="language-plaintext highlighter-rouge">usr/local/lib</code>. From there, you can try them out in your preferred plugin host.</p>

<hr />
<h3 id="a-simpler-example">A Simpler Example</h3>
<p>There’s a lot going on in the last section. A minimal example (linked to from the official CLAP repository) can be found <a href="https://github.com/schwaaa/clap-imgui.git">here</a>. I highly reccommend reading through <a href="https://github.com/schwaaa/clap-imgui#readme">the repo’s README</a>.</p>

<p>Excluding reference and image files, these are the contents of the repo:</p>
<pre><code class="language-treeview">

./
|-- build/
|   |-- lin/
|   |-- mac/
|   `-- win/
`-- src/
    |-- clap/
    |-- glfw/
    |-- gui.cpp
    |-- imgui/
    |-- imgui_base.cpp
    |-- imgui_lin.cpp
    |-- imgui_mac.mm
    |-- imgui_win.cpp
    |-- main.cpp
    |-- main.h
    |-- plugin.cpp
    |-- plugin_impl_0.cpp
    `-- plugin_impl_1.cpp

</code></pre>

<p>To build these examples, all we have to do(on Linux) is navigate to <code class="language-plaintext highlighter-rouge">build/lin</code> and execute the makefile there  with <code class="language-plaintext highlighter-rouge">make</code>.</p>

<p><img src="/assets/images/blog-images/audio-plugins/part-one/simple-clap-example-1.gif" alt="Simple CLAP Example 1" /> 
<img src="/assets/images/blog-images/audio-plugins/part-one/simple-clap-example-2.gif" alt="Simple CLAP Example 2" /></p>

<p>The resulting <code class="language-plaintext highlighter-rouge">.clap</code> file should show up right next to the makefile. If you’d like to try these plugins out, you can add our current directory to your host’s CLAP path or copy the generated CLAP file to <code class="language-plaintext highlighter-rouge">usr/local/lib</code> - the default path for CLAP plugins.</p>

<hr />
<h4 id="understanding-the-examples">Understanding the Examples</h4>

<hr />
<h5 id="the-main-file">The <code class="language-plaintext highlighter-rouge">main</code> File</h5>
<p>You can see in <code class="language-plaintext highlighter-rouge">main.h</code> that the only <code class="language-plaintext highlighter-rouge">#include</code> is <code class="language-plaintext highlighter-rouge">clap.h</code>, which allows us to work with CLAP. This is also where our customized plugin struct is defined (With members <code class="language-plaintext highlighter-rouge">clap_plugin</code>, <code class="language-plaintext highlighter-rouge">clap_plugin_params</code>, and <code class="language-plaintext highlighter-rouge">clap_plugin_gui</code> from the included <code class="language-plaintext highlighter-rouge">clap.h</code>. If you would like to know more about these ‘extensions’ (params &amp; gui), check out the well-documented <a href="https://github.com/free-audio/clap/tree/main/include/clap/ext">source code files</a>.</p>

<hr />
<h5 id="the-plugin-file">The <code class="language-plaintext highlighter-rouge">plugin</code> File</h5>
<p>We don’t really have to worry about this file - it’s where our plugins are constructed and passthroughs are located. Genericized plugin, GUI, and param calls are also set up here.</p>

<hr />
<h5 id="the-plugin_impl-files">The <code class="language-plaintext highlighter-rouge">plugin_impl</code> Files</h5>
<p>These are our actual plugins! Here is where we describe our plugins and their parameters, as well as how they behave, and what they look like (at least in this example).
Let’s take a look at the tone generator plugin once more:
<img src="/assets/images/blog-images/audio-plugins/part-one/simple-clap-example-2.gif" alt="Simple CLAP Example 2" /></p>

<p>You can see how the turning of knobs is continually processed in the <code class="language-plaintext highlighter-rouge">process</code> functions. Things are kicked off with this one:</p>
<pre><code class="language-cpp">
template &lt;class T&gt;
  clap_process_status \_plugin\_impl__process(const clap_process &#42;process,
    int num_channels, int start_frame, int end_frame,
    double *start_param_values, double *end_param_values,
    T **out)
  {
    if (!out) return CLAP_PROCESS_ERROR;

    double start_vol = start_param_values[PARAM_VOLUME];
    double end_vol = end_param_values[PARAM_VOLUME];
    double d_vol = (end_vol-start_vol) / (double)(end_frame-start_frame);

    double start_pitch = start_param_values[PARAM_PITCH];
    double end_pitch = end_param_values[PARAM_PITCH];
    double start_phase=m_phase, d_phase=0.0;
    if (end_pitch &gt;= 0.0)
    {
      if (start_pitch &lt; 0.0) start_phase=0.0;
      else start_pitch += start_param_values[PARAM_DETUNE]*0.01;
      end_pitch += end_param_values[PARAM_DETUNE]*0.01;
      double freq = 440.0 * pow(2.0, (end_pitch-57.0)/12.0);
      d_phase = 2.0 * _PI * freq / (double)m_srate;
    }

    for (int c=0; c &lt; num_channels; ++c)
    {
      T *cout=out[c];
      if (!cout) return CLAP_PROCESS_ERROR;

      if (d_phase &gt; 0.0)
      {
        double vol = start_vol;
        double phase = start_phase;
        for (int i=start_frame; i &lt; end_frame; ++i)
        {
          cout[i] = sin(phase)*vol;
          phase += d_phase;
          vol += d_vol;
        }
      }
      else
      {
        memset(cout+start_frame, 0, (end_frame-start_frame)*sizeof(T));
      }
    }

    m_phase += (double)(end_frame-start_frame)*d_phase;

    return CLAP_PROCESS_CONTINUE;
  }

</code></pre>

<p>And they continue being kicked off with this one:</p>
<pre><code class="language-cpp">
clap_process_status plugin_impl__process(const clap_process &#42;process)
  {
    double cur_param_values[NUM_PARAMS];
    for (int i=0; i &lt; NUM_PARAMS; ++i)
    {
      cur_param_values[i]=m_param_values[i];
    }

    clap_process_status s = -1;
    if (process &amp;&amp; process-&gt;audio_inputs_count == 0 &amp;&amp;
      process-&gt;audio_outputs_count == 1 &amp;&amp; process-&gt;audio_outputs[0].channel_count == 2)
    {
      // handling incoming parameter changes and slicing the process call
      // on the time axis would happen here.

      if (process-&gt;audio_outputs[0].data32)
      {
        s = _plugin_impl__process(process, 2, 0, process-&gt;frames_count,
          m_last_param_values, cur_param_values,
          process-&gt;audio_outputs[0].data32);
      }
      else if (process-&gt;audio_outputs[0].data64)
      {
        s = _plugin_impl__process(process, 2, 0, process-&gt;frames_count,
          m_last_param_values, cur_param_values,
          process-&gt;audio_outputs[0].data64);
      }
    }

    for (int i=0; i &lt; NUM_PARAMS; ++i)
    {
      m_last_param_values[i]=m_param_values[i];
    }

    if (s &lt; 0) s = CLAP_PROCESS_ERROR;
    return s;
  }
</code></pre>

<p>The other important classes are all the <code class="language-plaintext highlighter-rouge">ImGui</code> ones, like <code class="language-plaintext highlighter-rouge">draw()</code>.</p>

<hr />
<h5 id="the-gui-files">The <code class="language-plaintext highlighter-rouge">\*gui\*</code> Files</h5>
<p>These files are where our Plugin struct member actually interfaces with OpenGL through <a href="https://github.com/ocornut/imgui">Dear Imgui</a> and <a href="https://www.glfw.org/">GLFW</a></p>

<hr />
<h5 id="the-gui">The GUI</h5>
<p>The developer of these simpler plugins - schwaaa - explains why he’s using <a href="https://github.com/ocornut/imgui">Dear Imgui</a>:</p>

<blockquote>
  <p>The UI for this example is presented via <a href="https://github.com/ocornut/imgui">Dear ImGui</a>. The ImGui concept is easy to misunderstand so please read <a href="https://github.com/ocornut/imgui#readme">the Dear ImGui README</a>. The key concept is that the caller (this plugin) does not retain any state related to the UI. This allows developers to quickly prototype controls.</p>

  <p>100% of the plugin’s UI code is in <code class="language-plaintext highlighter-rouge">plugin_impl__draw()</code>, a single small function. For example, this is the code for the volume slider, which draws the control, handles mouse and keyboard input, and links the control to the voldb variable:</p>

  <p><code class="language-plaintext highlighter-rouge">ImGui::SliderFloat("Volume", &amp;voldb, -60.0f, 12.0f, "%+.1f dB", 1.0f);</code></p>

  <p>You have some control over the appearance and placement of the control, but the primary design goal is simplicity and ease of programmer use. ImGui is not typically used for end-user UI.</p>

  <p>ImGui is helpful for this example because:</p>
  <ul>
    <li>Permissively licensed</li>
    <li>No external dependencies</li>
    <li>Very easy to interact with</li>
  </ul>
</blockquote>

<hr />
<h5 id="the-makefile">The Makefile</h5>
<p>You may have noticed that a single <code class="language-plaintext highlighter-rouge">.clap</code> file contains two plugins. You can step through the <a href="https://github.com/schwaaa/clap-imgui/blob/main/build/lin/Makefile">makefile</a> to see how this happens(and here’s a <a href="https://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/">makefile refresher</a>, if you’re in need), and how we end up with our plugins from dozens of source files. The most important line is where the actual compilation takes place (line 57).</p>

<hr />
<h3 id="closing-thoughts">Closing Thoughts</h3>
<p>Creating audio plugins from scratch turned out to be a bit more convoluted than I expected. I was especially hoping for more in the way of documentation. The next post in this series will be focused on creating our own arpeggiator, and the following on creating a sampler.</p>]]></content><author><name>Evan</name></author><category term="c++" /><category term="music" /><summary type="html"><![CDATA[What does it take to build an audio plugin?]]></summary></entry><entry><title type="html">Semi-Automating Microsoft Edge Searches</title><link href="eldun.github.io/2021/03/16/automating-microsoft-edge-searches.html" rel="alternate" type="text/html" title="Semi-Automating Microsoft Edge Searches" /><published>2021-03-16T00:00:00-04:00</published><updated>2021-03-16T00:00:00-04:00</updated><id>eldun.github.io/2021/03/16/automating-microsoft-edge-searches</id><content type="html" xml:base="eldun.github.io/2021/03/16/automating-microsoft-edge-searches.html"><![CDATA[<script src="/js/post-scripts/automating-edge-searches/search.js" type="text/javascript"></script>

<h2 id="introduction"><a id="introduction"></a>Introduction</h2>

<p>First off, I’d like to say that I have nothing against Edge. It’s beautiful and zippy. However, when I attempted to accrue search points naturally, I found myself cursing Bing’s algorithm (StackOverflow on the second page?? GeeksForGeeks at the top?!??). And so here we are, automatically search made-up words in pursuit of the ultimate hypothetical prize:</p>

<p><img src="/assets/images/blog-images/automating-edge-searches/goal.png" alt="The ultimate hypothetical prize." /></p>

<hr />

<h2 id="word-generation"><a id="word-generation"></a>Word Generation</h2>

<p>The first order of business was generating plausible words. I’m not sure if Bing ignores nonsense words. In fact, I don’t know if there’s any critera that Bing filters by. Anyway, to generate words, I just sampled a bit of code from <a href="https://j11y.io/javascript/random-word-generator/">here</a>, which simply alternates vowels and consonants.</p>

<pre><code class="language-javascript">// From https://j11y.io/javascript/random-word-generator/
function createRandomWord(length) {
  var consonants = "bcdfghjklmnpqrstvwxyz",
    vowels = "aeiou",
    rand = function (limit) {
      return Math.floor(Math.random() * limit);
    },
    i,
    word = "",
    length = parseInt(length, 10),
    consonants = consonants.split(""),
    vowels = vowels.split("");
  for (i = 0; i &lt; length / 2; i++) {
    var randConsonant = consonants[rand(consonants.length)],
      randVowel = vowels[rand(vowels.length)];
    word += i === 0 ? randConsonant.toUpperCase() : randConsonant;
    word += i * 2 &lt; length - 1 ? randVowel : "";
  }
  return word;
}</code></pre>
<pre><code class="language-javascript">console.log(createRandomWord(5));
console.log(createRandomWord(10));
console.log(createRandomWord(20));

Xajiq
Sexetelufa
Vazocolebaboxugosiqi</code></pre>

<hr />

<h2 id="searching"><a id="searching"></a>Searching</h2>

<p>Well, we’re almost done. Add the listener to the search button that’s in the html:</p>

<pre><code class="language-javascript">document.addEventListener("DOMContentLoaded", init, false);
function init() {
  var button = document.getElementById("bing-search-button");
  button.addEventListener("click", startSearch, true);
}</code></pre>

<p><code class="language-plaintext highlighter-rouge">startSearch</code> is called, and <code class="language-plaintext highlighter-rouge">continueSearch</code> is called until the max number of searches is reached (typically 50ish):</p>

<pre><code class="language-javascript">function startSearch() {
  let searchCount = 0;
  let randomWord = () =&gt; createRandomWord(Math.ceil(Math.random() * 14) + 3);
  let tab = window.open(`https://www.bing.com/search?q=${randomWord()}`);
  let tab2 = window.open(`https://www.bing.com/search?q=${randomWord()}`);

  setTimeout(continueSearch, 2000, searchCount);
  

  function continueSearch(searchCount) {
    if (searchCount &lt; MAX_BING_SEARCHES) {
      tab.location = `https://www.bing.com/search?q=${randomWord()}`;
      tab2.location = `https://www.bing.com/search?q=${randomWord()}`;
      searchCount++;

      setTimeout(continueSearch, 5000, searchCount);
    }
  }
}</code></pre>

<p>Note that two tabs are opened - one for desktop searches, and one for mobile. I couldn’t find a simpler way to mock a mobile <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent">user agent</a> than just hitting F12 and toggling mobile device emulation (if you’re on desktop) or requesting the desktop site (if you’re on mobile).</p>

<p><img src="/assets/images/blog-images/automating-edge-searches/device-emulation.png" alt="Toggling Mobile Device Emulation" /></p>

<p>In order for both tabs to open, you’ll have to allow pop-ups from eldun.github.io.</p>

<hr />

<h2 id="the-result"><a id="the-result"></a>The Result</h2>

<p>I try to click this button everyday:</p>

<p><button class="btn" id="bing-search-button">Start Auto-Search</button></p>]]></content><author><name>Evan</name></author><category term="web" /><category term="javascript" /><category term="automation" /><summary type="html"><![CDATA[Microsoft has a suprisingly worthwhile rewards program - in addition to their three daily tasks (which range in value from 5 to 50 points each), there are a possible 250 points(~$1.25) to be earned from Bing searches within the Edge browser. This is my quick and dirty solution for snagging those points without manually searching 50 queries in Edge every day.]]></summary></entry><entry><title type="html">Ray Tracing in One Weekend:</title><link href="eldun.github.io/2020/06/19/ray-tracing-in-one-weekend-part-two.html" rel="alternate" type="text/html" title="Ray Tracing in One Weekend:" /><published>2020-06-19T00:00:00-04:00</published><updated>2020-06-19T00:00:00-04:00</updated><id>eldun.github.io/2020/06/19/ray-tracing-in-one-weekend-part-two</id><content type="html" xml:base="eldun.github.io/2020/06/19/ray-tracing-in-one-weekend-part-two.html"><![CDATA[<h2 id="image-output"><a id="image-output"></a>Image Output</h2>
<p>Of course, the first step with producing a pretty path traced image is to produce an image. The method suggested by Peter is a simple plaintext <code class="language-plaintext highlighter-rouge">.ppm</code> file. The following is an example snippet and image from <a href="https://en.wikipedia.org/wiki/Netpbm#PPM_example">Wikipedia</a>:</p>

<div class="row">
<pre><code class="language-shell">P3
3 2
255
# The part above is the header
# "P3" means this is an RGB color image in ASCII
# "3 2" is the width and height of the image in pixels
# "255" is the maximum value for each color
# The part below is image data: RGB triplets
255   0   0  # red
  0 255   0  # green
  0   0 255  # blue
255 255   0  # yellow
255 255 255  # white
  0   0   0  # black</code></pre>
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/ppm-example-output.png" />

</div>

<p>The code for creating a <code class="language-plaintext highlighter-rouge">.ppm</code> file is as follows:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value
	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			float r = float(i) / float(nx);
			float g = float(j) / float(ny);
			float b = 0.2;
			int ir = int(255.99 * r);
			int ig = int(255.99 * g);
			int ib = int(255.99 * b);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>Note:</p>

<ul>
  <li>Pixels are written from left to right.</li>
  <li>Rows of pixels are written top to bottom.</li>
  <li>In this simple example, from left to right, red goes from 0 to 255. Green goes from 0 to 255, bottom to top. As such, the top right corner should be yellow.</li>
</ul>

<p>Now to compile and redirect the output of our program to a file:</p>
<pre><code class="language-shell">g++ main.cpp
./a.out &gt; hello.ppm</code></pre>

<p>You may have to use a <a href="http://www.cs.rhodes.edu/welshc/COMP141_F16/ppmReader.html">web tool</a> or download a file viewer (I use <a href="https://www.irfanview.com/">IrfanView</a>) to view the <code class="language-plaintext highlighter-rouge">.ppm</code> file as an image. Here’s my resulting image and raw contents of the file:</p>

<p><span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hello-world-ppm.png" alt="The &quot;Hello World&quot; of our path tracer" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/hello-world-ppm-raw.png" alt="The &quot;Hello World&quot; of our path tracer" />
</span></p>

<hr />

<h2 id="timing-execution"><a id="timing-execution"></a>Timing Execution</h2>
<p>Eventually, our program is going to chug when it comes to producing an image. It’s nice to have a total running time output. This is optional, and you can <a href="#vec3-class">skip</a> it if you please.</p>

<p>If you want, you could just run our program in the terminal prepended with <code class="language-plaintext highlighter-rouge">time</code>. Here’s an example of the utility:</p>

<pre><code class="shell">eldun@Evan:/mnt/c/Users/Ev/source/Projects/PathTracer/PathTracer$ time sleep 1

real    0m1.019s
user    0m0.016s
sys     0m0.000s</code></pre>

<p>Otherwise, you can <code class="language-plaintext highlighter-rouge">#include &lt;chrono&gt;</code> (for timing) and <code class="language-plaintext highlighter-rouge">#include &lt;iomanip&gt;</code> (for formatting) in main (or anywhere) to time more specific parts of the program:</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;
<span class="highlight-green">#include &lt;chrono&gt;
#include &lt;iomanip&gt;
</span>

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

   	<span class="highlight-green">	auto start = std::chrono::high_resolution_clock::now();	   </span>

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			float r = float(i) / float(nx);
			float g = float(j) / float(ny);
			float b = 0.2;
			int ir = int(255.99 * r);
			int ig = int(255.99 * g);
			int ib = int(255.99 * b);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	<span class="highlight-green">	auto stop = std::chrono::high_resolution_clock::now(); 


	auto hours = std::chrono::duration_cast&lt;std::chrono::hours&gt;(stop - start);
	auto minutes = std::chrono::duration_cast&lt;std::chrono::minutes&gt;(stop - start) - hours;
	auto seconds = std::chrono::duration_cast&lt;std::chrono::seconds&gt;(stop - start) - hours - minutes;
    	std::cerr &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; "\nDone in:" &lt;&lt; std::endl &lt;&lt; 
	"\t" &lt;&lt; hours.count() &lt;&lt; " hours" &lt;&lt; std::endl &lt;&lt;
	"\t" &lt;&lt; minutes.count() &lt;&lt; " minutes" &lt;&lt; std::endl &lt;&lt;
	"\t" &lt;&lt; seconds.count() &lt;&lt; " seconds." &lt;&lt; std::endl; </span>

}</code></pre>

<hr />

<h2 id="vec3-class"><a id="vec3-class"></a>Vec3 Class</h2>
<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/vectors/vector.png" alt="Vector" />
Vectors! I feel like I haven’t used these since high school math but they are <strong>lovely</strong>. If you need or want a refresher on vectors, make sure to read <a href="#vector-refresher">this section</a>. According to Peter Shirley, almost all graphics programs have some class(es) for storing geometric vectors and colors. In many cases, the vectors are four-dimensional to represent homogenous coordinates for geometry, or to represent the alpha transparency channel for color values. We’ll be using three-dimensional coordinates, as that’s all we need to represent direction, color, location, offset, etc.</p>

<h3 id="vector-refresher"><a id="vector-refresher"></a>Vector Refresher</h3>
<p>Need a vector refresher? If so, check out <a href="https://www.mathsisfun.com/algebra/vectors.html">this rundown</a> at mathisfun.com. It’s the best I’ve found.
All the operations within the code above are covered the mathisfun post. Take particular note of <a href="https://www.mathsisfun.com/algebra/vector-unit.html"><code class="language-plaintext highlighter-rouge">make_unit_vector()</code></a>, <a href="https://www.mathsisfun.com/algebra/vectors-dot-product.html"><code class="language-plaintext highlighter-rouge">dot()</code></a>, and <a href="https://www.mathsisfun.com/algebra/vectors-cross-product.html"><code class="language-plaintext highlighter-rouge">cross()</code></a>.</p>

<p>Here are the constructors and declarations of the functions we’ll be using within <code class="language-plaintext highlighter-rouge">vec3.h</code>.</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">#ifndef VEC3H
#define VEC3H

#include &lt;math.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;iostream&gt;

// 3 dimensional vectors will be used for colors, locations, directions, offsets, etc.
class vec3 {
public:
	vec3() {}
	vec3(double e0, double e1, double e2) { e[0] = e0; e[1] = e1; e[2] = e2; }

	inline double x() const { return e[0]; }
	inline double y() const { return e[1]; }
	inline double z() const { return e[2]; }
	inline double r() const { return e[0]; }
	inline double g() const { return e[1]; }
	inline double b() const { return e[2]; }

	// return reference to current vec3 object
	inline const vec3&amp; operator+() const { return *this; }

	// return opposite of vector when using '-'
	inline vec3 operator-() const { return vec3(-e[0], -e[1], -e[2]); }

	// return value or reference to value of vec3 at index i ( I believe)
	inline double operator[](int i) const { return e[i]; }
	inline double&amp; operator[](int i) { return e[i]; };

	inline vec3&amp; operator+=(const vec3&amp; v2);
	inline vec3&amp; operator-=(const vec3&amp; v2);
	inline vec3&amp; operator*=(const vec3&amp; v2);
	inline vec3&amp; operator/=(const vec3&amp; v2);
	inline vec3&amp; operator*=(const double t);
	inline vec3&amp; operator/=(const double t);

	inline double length() const {
		return sqrt(e[0]*e[0] + e[1]*e[1] + e[2]*e[2]);
	}
	inline double squared_length() const {
		return e[0]*e[0] + e[1]*e[1] + e[2]*e[2];
	}
	inline void make_unit_vector();

	double e[3];
};

...</code></pre>

<h3 id="vec3-definitions"><a id="vec3-definitions"></a>Vec3 Definitions</h3>
<p>The next step in our vector class is to define our functions. Be very careful here! This is where I had a few minor typo issues that mangled the final image later in the project. It’s not hard to see why; these are the lowest-level operations of vectors, which will simulate our light rays and their properties.</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">...

// input output overloading
inline std::istream&amp; operator&gt;&gt;(std::istream&amp; is, vec3&amp; t) {
	is &gt;&gt; t.e[0] &gt;&gt; t.e[1] &gt;&gt; t.e[2];
	return is;
}

inline std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const vec3&amp; t) {
	os &lt;&lt; t.e[0] &lt;&lt; " " &lt;&lt; t.e[1] &lt;&lt; " " &lt;&lt; t.e[2];
	return os;
}


inline void vec3::make_unit_vector() {
	double k = 1.0 / sqrt(e[0]*e[0] + e[1]*e[1] + e[2]*e[2]);
	e[0] *= k;
	e[1] *= k;
	e[2] *= k;
}

inline vec3 operator+(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] + v2.e[0], v1.e[1] + v2.e[1], v1.e[2] + v2.e[2]);
}

inline vec3 operator-(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] - v2.e[0], v1.e[1] - v2.e[1], v1.e[2] - v2.e[2]);
}

inline vec3 operator*(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] * v2.e[0], v1.e[1] * v2.e[1], v1.e[2] * v2.e[2]);
}

inline vec3 operator/(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[0] / v2.e[0], v1.e[1] / v2.e[1], v1.e[2] / v2.e[2]);
}

inline vec3 operator*(double t, const vec3&amp; v) {
	return vec3(t * v.e[0], t * v.e[1], t * v.e[2]);
}

inline vec3 operator/(const vec3 v, double t) {
	return vec3(v.e[0] / t, v.e[1] / t, v.e[2] / t);
}

inline vec3 operator*(const vec3&amp; v, double t) {
	return vec3(t * v.e[0], t * v.e[1], t * v.e[2]);
}

// Dot product
inline double dot(const vec3&amp; v1, const vec3&amp; v2) {
	return
		v1.e[0] * v2.e[0]
		+ v1.e[1] * v2.e[1]
		+ v1.e[2] * v2.e[2];
}

// Cross product
inline vec3 cross(const vec3&amp; v1, const vec3&amp; v2) {
	return vec3(v1.e[1] * v2.e[2] - v1.e[2] * v2.e[1],
				v1.e[2] * v2.e[0] - v1.e[0] * v2.e[2],
				v1.e[0] * v2.e[1] - v1.e[1] * v2.e[0]);
}

inline vec3&amp; vec3::operator+=(const vec3&amp; v) {
	e[0] += v.e[0];
	e[1] += v.e[1];
	e[2] += v.e[2];
	return *this;
}
inline vec3&amp; vec3::operator-=(const vec3&amp; v) {
	e[0] -= v.e[0];
	e[1] -= v.e[1];
	e[2] -= v.e[2];
	return *this;
}

inline vec3&amp; vec3::operator*=(const vec3&amp; v) {
	e[0] *= v.e[0];
	e[1] *= v.e[1];
	e[2] *= v.e[2];
	return *this;
}

inline vec3&amp; vec3::operator/=(const vec3&amp; v) {
	e[0] /= v.e[0];
	e[1] /= v.e[1];
	e[2] /= v.e[2];
	return *this;
}

inline vec3&amp; vec3::operator*=(const double t) {
	e[0] *= t;
	e[1] *= t;
	e[2] *= t;
	return *this;
}

inline vec3&amp; vec3::operator/=(const double t) {
	double k = 1.0 / t;

	e[0] *= k;
	e[1] *= k;
	e[2] *= k;
	return *this;
}

inline vec3 unit_vector(vec3 v) {
	return v / v.length();
}

#endif // !VEC3H</code></pre>

<p>Make sure to include our new vec3.h in main.cpp.</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;

<span class="highlight-green">
#include "vec3.h"
</span>

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value
	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			<span class="highlight-green">
			vec3 col(float(i) / float(nx), float(j) / float(ny), 0.2);
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			</span>
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}
</code></pre>

<hr />

<h2 id="rays"><a id="rays"></a>Rays</h2>
<p>Ray tracers need rays! These are what will be colliding with objects in the scene. Rays have an origin, a direction, and can be described by the following formula:</p>

<p><strong><em>P</em></strong>(<em>t</em>) = <strong><em>A</em></strong> + <em>t</em><strong><em>B</em></strong></p>

<ul>
  <li><strong><em>p</em></strong> is a point on the ray.</li>
  <li><strong><em>A</em></strong> is the ray origin.</li>
  <li><strong><em>B</em></strong> is the direction of the ray.</li>
  <li>The ray parameter <em>t</em> is a real number (positive or negative) that moves <strong><em>p</em></strong>(t) along the ray.</li>
</ul>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.lerp.png" alt="Our Ray (Illustration from Peter Shirley's book)" /></p>

<p>Here’s the header file for our ray class:</p>

<p><code class="language-plaintext highlighter-rouge">ray.h:</code></p>
<pre><code class="language-cpp">#ifndef RAYH
#define RAYH
#include "Vec3.h"

class ray
{
public:
	ray() {}
	ray(const vec3&amp; a, const vec3&amp; b) { A = a; B = b; }
	vec3 origin() const		{ return A; }
	vec3 direction() const	{ return B; }
	vec3 point_at_parameter(double t) const { return A + t * B; }

	vec3 A;
	vec3 B;
};

#endif // !RAYH</code></pre>

<h3 id="sending-rays-from-the-camera"><a id="sending-rays-from-the-camera"></a>Sending Rays from the Camera</h3>
<p>Put simply, our ray tracer will send rays through pixels and compute the color seen for each ray. The steps for doing so are as follows:</p>

<ol>
  <li>Calculate the ray from camera to pixel.</li>
  <li>Determine which objects the ray intersects.</li>
  <li>Compute a color for the intersection.</li>
</ol>

<p>We will need a “viewport” of sorts to pass rays through from our “camera.” Since we’re using standard square pixel spacing, the viewport will have the same aspect ratio as our rendered image. Shirley sets the height of the viewport to two units in his book, and we’ll do the same.</p>

<p>Using Peter Shirley’s example, we’re going to set the camera at (0,0,0), and look towards the negative z-axis. The viewport will be traversed with rays from left-to-right, bottom-to-top. Variables u and v will be the offset vectors used to move the camera ray along the viewport:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.cam-geom.png" alt="Camera Geometry (Illustration from Peter Shirley's book)" /></p>

<p>Here’s our code for the camera, as well as rendering a blue-to-white gradient:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "ray.h"

/*
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
*/
vec3 color(const ray&amp; r) {
	vec3 unit_direction = unit_vector(r.direction());
	double t = 0.5 * (unit_direction.y() + 1.0);
	return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);

}

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// The values below are derived from making the "camera"/ray origin coordinates (0, 0, 0) relative to the canvas.
	vec3 lower_left_corner(-2.0, -1.0, -1.0);
	vec3 horizontal(4.0, 0.0, 0.0);
	vec3 vertical(0.0, 2.0, 0.0);
	vec3 origin(0.0, 0.0, 0.0);
	for (int j = ny - 1; j &gt;= 0; j--) {
		std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			double u = double(i) / double(nx);
			double v = double(j) / double(ny);

			// Approximate pixel centers on the canvas for each ray r
			ray r(origin, lower_left_corner + u * horizontal + v * vertical);

			vec3 col = color(r);
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>The result:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/gradient.png" alt="Linear Gradient" /></p>

<p>We can move the camera code into <code class="language-plaintext highlighter-rouge">camera.h</code>.</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code>:</p>
<pre><code class="language-cpp">
#ifndef CAMERAH
#define CAMERAH

#include "ray.h"

class camera {
public:

	// The values below are derived from making the "camera" / ray origin coordinates(0, 0, 0) relative to the canvas.
	camera() {
		lower_left_corner = vec3(-2.0, -1.0, -1.0);
		horizontal = vec3(4.0, 0.0, 0.0);
		vertical = vec3(0.0, 2.0, 0.0);
		origin = vec3(0.0, 0.0, 0.0);
	}
	ray get_ray(double u, double v) { return ray(origin, lower_left_corner + u * horizontal + v * vertical - origin); }

	vec3 origin;
	vec3 lower_left_corner;
	vec3 horizontal;
	vec3 vertical;
};

#endif // !CAMERAH
</code></pre>

<hr />

<h2 id="introducing-spheres"><a id="introducing-spheres"></a>Introducing Spheres</h2>
<h3 id="describing-a-sphere"><a id="describing-a-sphere"></a>Describing a Sphere</h3>
<p>We have a beautiful sky-like gradient. Let’s add a sphere! Spheres are popular in ray tracers because they’re mathematically simple.</p>

<p>A sphere centered at the origin of radius $R$ is $x^2 + y^2 + z^2 = R^2$.</p>

<ul>
  <li>This means that if a point $(x,y,z)$ is on a sphere, $x^2 + y^2 + z^2 = R^2$.</li>
  <li>If the point is inside the sphere, $x^2 + y^2 + z^2 &lt; R^2$.</li>
  <li>If the point is outside the sphere, $x^2 + y^2 + z^2 &gt; R^2$</li>
</ul>

<p>If the sphere center isn’t at the origin, the formula is:</p>

\[(x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2 = r^2\]

<p>It’s best if formulas are kept under the hood in the vec3 class.</p>

<p>The vector from center $\mathbf{C} = (C_x,C_y,C_z)$ to point $\mathbf{P} = (x,y,z)$ is $(\mathbf{P} - \mathbf{C})$, and therefore</p>

\[(\mathbf{P} - \mathbf{C}) \cdot (\mathbf{P} - \mathbf{C}) = (x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2\]

<p>Therefore, the equation of a sphere in vector form is:</p>

\[(\mathbf{P} - \mathbf{C}) \cdot (\mathbf{P} - \mathbf{C}) = r^2\]

<p>Any point $\mathbf{P}$ that satisfies this equation is on the sphere.
We’re going to find out if a given ray <em>ever</em> hits the sphere. If it does, there is a value <em>t</em> for which P(t) satisfies this equation:</p>

\[(\mathbf{P}(t) - \mathbf{C}) \cdot (\mathbf{P}(t) - \mathbf{C}) = r^2\]

<p>The same formula, expanded:</p>

\[(\mathbf{A} + t \mathbf{b} - \mathbf{C}) \cdot (\mathbf{A} + t \mathbf{b} - \mathbf{C}) = r^2\]

<p>and again:</p>

\[t^2 \mathbf{b} \cdot \mathbf{b} + 2t \mathbf{b} \cdot (\mathbf{A}-\mathbf{C}) + (\mathbf{A}-\mathbf{C}) \cdot (\mathbf{A}-\mathbf{C}) - r^2 = 0\]

<p>The unknown variable is <em>t</em>, and this is a quadratic equation. Solving for <em>t</em> will lead to a square root operation (aka the discriminant) that is either positive (two real solutions), negative (no real solutions), or zero (one real solution):</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.ray-sphere.png" alt="Ray-Sphere Intersections(Illustration from Peter Shirley's book)" /></p>

<h3 id="placing-a-sphere"><a id="placing-a-sphere"></a>Placing a Sphere</h3>

<p>Prepend <code class="language-plaintext highlighter-rouge">main.cpp</code>’s main function with the following to mathematically hard-code a sphere to be hit by rays:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">...

bool hit_sphere(const vec3&amp; center, double radius, const ray&amp; r) {
	vec3 oc = r.origin() - center;
	double a = dot(r.direction(), r.direction());
	double b = 2.0 * dot(oc, r.direction());
	double c = dot(oc, oc) - radius * radius;
	double discriminant = b*b - 4*a*c;
	return (discriminant &gt; 0);
}

/*
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
*/
vec3 color(const ray&amp; r) {
	if (hit_sphere(vec3(0, 0, -1), 0.5, r))
		return vec3(1, 0, 0);
	vec3 unit_direction = unit_vector(r.direction());
	double t = 0.5 * (unit_direction.y() + 1.0);
	return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
}

...</code></pre>

<p>The result:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/red-sphere.png" alt="Ray traced sphere" /></p>

<!-- Be aware that if the sphere center is change to z= +1, we'll still see the same image. We should not be seeing objects behind us. This will be fixed in the next section. -->

<hr />

<h2 id="surface-normals"><a id="surface-normals"></a>Surface Normals</h2>

<p>Our sphere looks like a circle. To make it more obvious that it <em>is</em> a sphere, we’ll add surface normals to the face. Surface normals are simply vectors that are perpendicular to the surface of an object.</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/Normal_vectors_on_a_curved_surface.svg" alt="Surface Normal" /></p>

<p>In our case, the outward normal is the hitpoint minus the center:</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/fig.sphere-normal.png" alt="Surface Normal(from Shirley's book)" /></p>

<p>Since we don’t have any lights, we can visualize the normals with a color map.</p>

<p>Our <code class="language-plaintext highlighter-rouge">main.cpp</code> file will now look something like this:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "ray.h"

double hit_sphere(const vec3&amp; center, double radius, const ray&amp; r) {
	vec3 oc = r.origin() - center;
	double a = dot(r.direction(), r.direction());
	double b = 2.0 * dot(oc, r.direction());
	double c = dot(oc, oc) - radius * radius;
	double discriminant = (b*b) - (4*a*c);
	if (discriminant &lt; 0) {
		return -1.0;
	}

<span class="highlight-green">
	else {
		return (-b - sqrt(discriminant)) / (2.0 * a);
</span>}
}

/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
* Draw sphere and surface normals
*/
vec3 color(const ray&amp; r) {
	double t = hit_sphere(vec3(0, 0, -1), 0.5, r); // does the ray hit the values of a sphere placed at (0,0,-1) with a radius of .5?
	if (t &gt; 0.0) { // sphere hit
		vec3 N = unit_vector(r.point_at_parameter(t) - vec3(0, 0, -1)); // N (the normal) is calculated
		return 0.5 * (vec3(N.x() + 1, N.y() + 1, N.z() + 1)); // RGB values assigned based on xyz values
	}
	vec3 unit_direction = unit_vector(r.direction());
	t = 0.5 * (unit_direction.y() + 1.0);
	return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);

}

...</code></pre>

<p>Our resulting image:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/surface-normals-render.png" alt="Sphere with Normals" /></p>

<h3 id="simplifying-ray-sphere-intersection"><a id="simplifying-ray-sphere-intersection"></a>Simplifying Ray-Sphere Intersection</h3>

<p>As it turns out, we can simplify ray-sphere intersection. Here’s our original equation:</p>
<pre><code class="language-cpp">
vec3 oc = r.origin() - center;
auto a = dot(r.direction(), r.direction());
auto b = 2.0 * dot(oc, r.direction());
auto c = dot(oc, oc) - radius*radius;
auto discriminant = b*b - 4*a*c;
</code></pre>

<p>The dot product of a vector with itself is equal to the squared length of that vector.</p>

<p>Additionally, the equation for b has a factor of two in it. Consider the quadratic equation if b = 2h:</p>

\[\frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\]

\[= \frac{-2h \pm \sqrt{(2h)^2 - 4ac}}{2a}\]

\[= \frac{-2h \pm 2\sqrt{h^2 - ac}}{2a}\]

\[= \frac{-h \pm \sqrt{h^2 - ac}}{a}\]

<p>As such, we can refactor our code like so:</p>
<pre><code class="language-cpp">vec3 oc = r.origin() - center;
auto a = r.direction().length_squared();
auto half_b = dot(oc, r.direction());
auto c = oc.length_squared() - radius*radius;
auto discriminant = half_b*half_b - a*c;

if (discriminant &lt; 0) {
    return -1.0;
} else {
    return (-half_b - sqrt(discriminant) ) / a;
}</code></pre>

<p>Cool! But it could be cooler. We need more spheres. The cleanest way to accomplish this is to create an abstract class - a class that must be overwritten by derived classes - of hittable objects.</p>

<hr />

<h2 id="multiple-spheres-with-the-hittable-class"><a id="multiple-spheres"></a>Multiple Spheres with the Hittable Class</h2>

<p>Our hittable abstract class will have a “hit” function that will be passed a ray and a record containing information about the hit, such as the time(which will be added with motion blur later in this series), position, and the surface normal:</p>

<p><code class="language-plaintext highlighter-rouge">hittable.h:</code></p>
<pre><code class="language-cpp">#ifndef HITTABLEH
#define HITTABLEH

#include "ray.h"


struct hit_record {
	double t;
	vec3 p;
	vec3 normal;
};

/* 
* A class for objects rays can hit.
*/
class hittable {
public:
	virtual bool hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const = 0;
};

#endif // !HITTABLEH</code></pre>

<hr />

<h2 id="front-faces-versus-back-faces"><a id="front-faces-versus-back-faces"></a>Front Faces Versus Back Faces</h2>
<p>A question we have to ask ourselves about our normals is whether they should always point outward. Right now, the normal will always be in the <em>direction of the center to the intersection point</em> - outward. So if the ray intersects from the outside, the normal is against the ray. If the ray intersects from the inside (like in a glass ball), the normal would be pointing in the same direction of the ray. The alternative option is to have the normal always point against the ray.</p>

<p>If we decide to always have the normal point outward, we need to determine what side the ray is on when we color it. If they face the same direction, the ray is inside the object traveling outward. If they’re opposite, the ray is outside traveling inward. We can determine this by taking the dot product of the ray and the normal - if the dot is positive, the ray is inside traveling outward.</p>

<pre><code class="language-cpp">if (dot(ray_direction, outward_normal) &gt; 0.0) {
    // ray is inside the sphere
    ...
} else {
    // ray is outside the sphere
    ...
}</code></pre>

<p>Suppose we take the other option: always having the normals point against the ray. We would have to store what side of the surface the ray is on:</p>
<pre><code class="language-cpp">bool front_face;
if (dot(ray_direction, outward_normal) &gt; 0.0) {
    // ray is inside the sphere
    normal = -outward_normal;
    front_face = false;
}
else {
    // ray is outside the sphere
    normal = outward_normal;
    front_face = true;
}
</code></pre>

<p>You can choose whichever method you please, but Shirley’s book recommends the “outward” boolean method, as we will have more material types than geometric types for the time being.</p>

<p>Following the suggestion of Shirley, we’ll add a <code class="language-plaintext highlighter-rouge">front_face</code> boolean to the <code class="language-plaintext highlighter-rouge">hittable.h</code> <code class="language-plaintext highlighter-rouge">hit_record</code> struct, as well as a function to solve the calculation:</p>

<p><code class="language-plaintext highlighter-rouge">hittable.h</code>:</p>
<pre><code class="language-cpp">#ifndef HITTABLEH
#define HITTABLEH

#include "ray.h"


struct hit_record {
	double t; // parameter of the ray that locates the intersection point
	vec3 p; // intersection point
	vec3 normal;
	<span class="highlight-green">bool front_face;
	
	inline void set_face_normal(const ray&amp; r, const vec3&amp; outward_normal) {
    		front_face = dot(r.direction(), outward_normal) &lt; 0;
    		normal = front_face ? outward_normal :-outward_normal;
	} </span>
};

class hittable {
public: 
	virtual bool hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const = 0;
};

#endif // !HITTABLEH</code></pre>

<p>And now to update our sphere header with the simplified ray intersection and the outward normal calculations):</p>

<p><code class="language-plaintext highlighter-rouge">sphere.h:</code></p>
<pre><code class="language-cpp">#ifndef SPHEREH
#define SPHEREH

#include "hittable.h"

class sphere : public hittable {
public:
	sphere() {}
	sphere(vec3 cen, float r) : center(cen), radius(r) {};
	virtual bool hit(const ray&amp; r, double tmin, double tmax, hit_record&amp; rec) const;
	vec3 center;
	double radius;
};

bool sphere::hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const {
	vec3 oc = r.origin() - center; // Vector from center to ray origin
	double a = r.direction().length_squared();
	double halfB = dot(oc, r.direction());
	double c = oc.length_squared() - radius*radius;
	double discriminant = (halfB * halfB) - (a * c);
	if (discriminant &gt; 0.0) {
        auto root = sqrt(discriminant);

		auto temp = (-halfB - root)/a;

		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
<span class="highlight-green">vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);</span>
			return true;
		}
		temp = (-halfB + root / a;
		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
<span class="highlight-green">vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);</span>
			return true;
		}
	}
	return false;
}

#endif // !SPHEREH
</code></pre>

<p>As well as a new file for a list of hittable objects:</p>

<p><code class="language-plaintext highlighter-rouge">hittableList.h:</code></p>
<pre><code class="language-cpp">#ifndef HITTABLELISTH
#define HITTABLELISTH

#include "hittable.h"

class hittable_list : public hittable {
public:
	hittable_list() {}
	hittable_list(hittable** l, int n) { list = l; list_size = n; }
	virtual bool hit(const ray&amp; r, double tmin, double tmax, hit_record&amp; rec) const;
	hittable** list;
	int list_size;
};

bool hittable_list::hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const {
	hit_record temp_rec;
	bool hit_anything = false;
	double closest_so_far = t_max;
	for (int i = 0; i &lt; list_size; i++) {
		if (list[i]-&gt;hit(r, t_min, closest_so_far, temp_rec)) {
			hit_anything = true;
			closest_so_far = temp_rec.t;
			rec = temp_rec;
		}
	}
	return hit_anything;
}

#endif // !HITTABLELISTH</code></pre>

<p>And the modified <code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "sphere.h"
#include "hittableList.h"
#include "float.h"


/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
* Draw sphere and surface normals
*/
vec3 color(const ray&amp; r, hittable * world) {
	hit_record rec;
	if (world-&gt;hit(r, 0.0, DBL_MAX, rec)) {
		return 0.5 * vec3(rec.normal.x() + 1, rec.normal.y() + 1, rec.normal.z() + 1); // return a vector with values between 0 and 1 (based on xyz) to be converted to rgb values
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {
	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// The values below are derived from making the "camera"/ray origin coordinates (0, 0, 0) relative to the canvas. 
	// See the included file "TracingIllustration.png" for a visual representation.
	vec3 lower_left_corner(-2.0, -1.0, -1.0);
	vec3 horizontal(4.0, 0.0, 0.0);
	vec3 vertical(0.0, 2.0, 0.0);
	vec3 origin(0.0, 0.0, 0.0);

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			double u = double(i) / double(nx);
			double v = double(j) / double(ny);

			// Approximate pixel centers on the canvas for each ray r
			ray r(origin, lower_left_corner + u * horizontal + v * vertical);

			vec3 p = r.point_at_parameter(2.0);
			vec3 col = color(r, world);
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
	std::cerr &lt;&lt; "\nDone.\n";
}
</code></pre>

<p>The result:
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables.png" alt="Sphere hittables" /></p>

<p>Stunning. A little bit jaggedy, though, don’t you think? This effect is known as “aliasing.” If you wanted to, you could increase the resolution of our scene for higher fidelity. Another interesting method for better image quality falls under the umbrella term “anti-aliasing.”</p>

<hr />

<h2 id="anti-aliasing"><a id="anti-aliasing"></a>Anti-Aliasing</h2>
<p>Anti-aliasing encompasses a whole slew of methods to combat “jaggies” - from multi-sampling to super-sampling, approximation(FXAA) to temporal, or - more recently - deep learning anti-aliasing. Each of these methods has pros and cons depending on the type of scene portrayed, performance targets, and even scene movement. Usually, there’s a trade-off between image quality and speed. We’ve entered a fascinating time for graphics where raw pixel count (True 4K this! Real 4K that!) is becoming less important - thanks to some incredible leaps in upscaling and anti-aliasing.</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/anti-aliasing.png" alt="Small sample of anti-aliasing methods" /></p>

<p>If you want to learn more, I highly suggest watching <a href="https://www.youtube.com/watch?v=NbrA4Nxd8Vo">this video</a> from one of my favorite YouTube channels(<a href="https://www.youtube.com/user/DigitalFoundry">Digital Foundry</a>), or reading <a href="https://techguided.com/what-is-anti-aliasing/">this blog post</a> from techguided.com.</p>

<p>We’re going to be using multisample anti-aliasing (MSAA) in our ray tracer. As you may have supposed, multisampling, in this case, means taking multiple sub-pixel samples from each pixel and averaging the color across the whole pixel. Here’s an example - the raw triangle on the left, and the triangle with four samples per pixel on the right:</p>

<div class="captioned-image">
<span class="row">
![No MSAA](/assets/images/blog-images/path-tracer/the-first-weekend/no-msaa.png)
![MSAA 4x](/assets/images/blog-images/path-tracer/the-first-weekend/msaa.png)
</span>
[Source](https://developer.apple.com/documentation/metal/gpu_features/understanding_gpu_family_4/about_enhanced_msaa_and_imageblock_sample_coverage_control)
</div>

<p>Instead of taking perfectly spaced samples of pixels like in the example above, we’ll be taking random samples of pixels. For that, we’ll need a way of generating random numbers (you can do it however you please):</p>

<p><code class="language-plaintext highlighter-rouge">random.h:</code></p>
<pre><code class="language-cpp">#ifndef RANDOMH
#define RANDOMH

#include &lt;cstdlib&gt;

inline double random_double() {
    // Returns a random real in [0,1).
    return rand() / (RAND_MAX + 1.0);
}

inline double random_double(double min, double max) {
    // Returns a random real in [min,max).
    return min + (max-min)*random_double();
}

#endif // !RANDOMH</code></pre>

<h3 id="adding-anti-aliasing-to-the-camera"><a id="adding-anti-aliasing-to-the-camera"></a>Adding Anti-Aliasing to the Camera</h3>

<p>Next, we’ll create a camera class to manage the virtual camera and scene sampling:</p>

<pre><code class="language-cpp">#ifndef CAMERAH
#define CAMERAH

#include "ray.h"

class camera {
public:

	// The values below are derived from making the "camera" / ray origin coordinates(0, 0, 0) relative to the canvas.
	camera() {
		lower_left_corner = vec3(-2.0, -1.0, -1.0);
		horizontal = vec3(4.0, 0.0, 0.0);
		vertical = vec3(0.0, 2.0, 0.0);
		origin = vec3(0.0, 0.0, 0.0);
	}
	ray get_ray(double u, double v) { return ray(origin, lower_left_corner + u * horizontal + v * vertical - origin); }

	vec3 origin;
	vec3 lower_left_corner;
	vec3 horizontal;
	vec3 vertical;
};

#endif // !CAMERAH</code></pre>

<p>And our resulting main method:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include "sphere.h"
#include "hittableList.h"
#include "camera.h"
#include "random.h"


/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
*/
vec3 color(const ray&amp; r, hittable * world) {
	hit_record rec;
	if (world-&gt;hit(r, 0.0, DBL, rec)) {
		return 0.5 * vec3(rec.normal.x() + 1, rec.normal.y() + 1, rec.normal.z() + 1); // return a vector with values between 0 and 1 (based on xyz) to be converted to rgb values
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {

	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	int ns = 100; // Number of samples for each pixel for anti-aliasing
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 1)) / double(nx);
				double v = (j + random_double(0.0, 1)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				col += color(r, world);
			}

			col /= double(ns); // Average the color between objects/background
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>Keep in mind - these images are only 200x100.
The difference is clear. And blurry. Haha:</p>

<p><span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables.png" alt="Sphere hittables" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables-msaa.png" alt="Sphere hittables" />
</span>
<span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables-zoom.png" alt="Sphere hittables" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/hittables-msaa-zoom.png" alt="Sphere hittables" />
</span></p>

<hr />

<h2 id="diffuse-materials"><a id="diffuse-materials"></a>Diffuse Materials</h2>

<p>Our ball is pretty, but lacks texture. Let’s add diffuse materials!</p>

<p>Diffuse materials reflect light from their surface such that an incident ray is scattered ay many angles, rather than just one (which is the case with specular reflection):</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/diffuse.png" alt="Diffuse reflection" />
<em>An example of light reflecting off of a diffuse surface (<a href="https://en.wikipedia.org/wiki/Diffuse_reflection">source</a>)</em>
</span></p>

<p>From Wikipedia:</p>
<blockquote>
  <p>The visibility of objects, excluding light-emitting ones, is primarily caused by diffuse reflection of light: it is diffusely-scattered light that forms the image of the object in the observer’s eye.</p>
</blockquote>

<p>Diffuse materials also modulate the color of their surroundings with their intrinsic color. In our ray tracer, we’re going to simulate diffuse materials by randomizing ray reflections upon hitting a diffuse object. For example, if we were to shoot three rays into the space between two diffuse surfaces, we might see a result like this:</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/diffuse.png" alt="Diffuse reflection" />
<em>How rays might behave in our ray tracer upon hitting a diffuse surface (<a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">source</a>)</em></span></p>

<p>In addition to being reflected, some rays could also be absorbed. Naturally, the darker the surface of a given object, the more likely absorption will take place… which is why that object looks dark. Take Vantablack, one of the darkest substances known. It’s made up of carbon nanotubes, and is essentially a very fine shag carpet. Light gets lost (or diffused) within this forest of tubes to create a pretty striking diffuse material:</p>

<p><span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/vantablack-zoom.png" alt="Vantablack" />
<!-- *[source](https://en.wikipedia.org/wiki/Vantablack)* -->
</span>
<span class="row">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/vantablack.png" alt="Vantablack" />
<!-- *[source](https://www.techbriefs.com/component/content/article/tb/supplements/pit/features/applications/27558)* -->
</span></p>

<p>When I was originally reading Peter Shirley’s guide, he described an incorrect (but close) approximation of ideal Lambertian reflectance(think unfinished wood or charcoal - no shiny specular highlights). We’ll go through how I originally did it, and then modify the code to make matte surfaces more true-to-life, thanks to an update to his book.</p>

<h3 id="the-math-of-diffuse-materials"><a id="the-math-of-diffuse-materials"></a>The Math of Diffuse Materials</h3>

<p>First of all, we need to form a unit sphere tangent to the hitpoint <strong>p</strong> on the scene object. The center of this sphere will be the coordinates at the end of the surface normal <strong>n</strong>. Be aware that there are two spheres tangential to the collided sphere - one inside the object(<strong>p</strong> - <strong>n</strong>), and one outside(<strong>p</strong> + <strong>n</strong>). we’ll pick the tangent sphere that’s on the same side of the surface as the ray origin. Next, we’ll select a random point <strong>s</strong> in the tangent unit sphere and send a ray from the hit point <strong>p</strong> to the random point <strong>s</strong> - which results in the vector <strong>s</strong> - <strong>p</strong>.</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/ray-tracing-diffuse.png" alt="Diffuse material illustration" />
<em>Generation of random diffuse bounce ray (<a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">source</a>)</em>
</span></p>

<p>Now we need a way to pick the aforementioned random point <strong>s</strong>. Following Shirley’s lead, we’ll use a rejection method; picking a random point in the unit cube where x, y, and z all range from -1 to 1. If the point is outside the sphere (x<sup>2</sup> + y<sup>2</sup> + z<sup>2</sup> &gt; 1), we reject it and try again:</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">vec3 random_unit_sphere_coordinate() {
	vec3 p;
	do {
		p = 2.0 * vec3(random_double(0, 1), random_double(0, 1), random_double(0, 1)) - vec3(1, 1, 1);
	} while (p.squared_length() &gt;= 1.0);
	return p;
}</code></pre>

<p>Now we have to update our <code class="language-plaintext highlighter-rouge">color</code> function to use the random coordinates:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">vec3 color(const ray&amp; r, hittable * world) {
	hit_record rec;
	// Light that reflects off a diffuse surface has its direction randomized.
	// Light may also be absorbed.
	if (world-&gt;hit(r, 0.0, DBL_MAX, rec)) {
		<span class="highlight-green">
		vec3 target = rec.p + rec.normal + random_unit_sphere_coordinate(); 
		return 0.5 * color(ray(rec.p, target - rec.p), world); // light is absorbed continually by the sphere or reflected into the world.
		</span>
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}
...</code></pre>

<p>Notice that our new code is recursive and will only stop recursing when the ray fails to hit any object. In some scenes (or some unlucky sequences of random numbers), this could wreak havoc on performance. For that reason, we’ll enforce a bounce limit:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code></p>
<pre><code class="language-cpp"><span class="highlight-green"> vec3 color(const ray&amp; r, hittable * world, int depth) {</span>
	hit_record rec;

<span class="highlight-green">	if (depth &lt;= 0)
        return vec3(0,0,0); // Bounce limit reached - return darkness</span>

	// Light that reflects off a diffuse surface has its direction randomized.
	// Light may also be absorbed.
	if (world-&gt;hit(r, 0.0, DBL_MAX, rec)) {
		<span class="highlight-green">	vec3 target = rec.p + rec.normal + random_unit_sphere_coordinate();

	// light is absorbed continually by the sphere or reflected into the world.
	return 0.5 * color(ray(rec.p, target - rec.p), world, depth-1);</span>

	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {

	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	int ns = 100; // Number of samples for each pixel for anti-aliasing
	<span class="highlight-green">	int maxDepth = 50; // Bounce limit</span>
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 1)) / double(nx);
				double v = (j + random_double(0.0, 1)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				<span class="highlight-green">
				col += color(r, world, maxDepth);
				</span>
			}

			col /= double(ns); // Average the color between objects/background
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p>The result:
<span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse.png" alt="Diffuse sphere" />
<em>Diffuse sphere</em>
</span></p>

<h3 id="gamma-correction"><a id="gamma-correction"></a>Gamma Correction</h3>

<p>Our spheres are reflecting 50% of each bounce, so why is our picture so dark? Most image viewers assume images to be “gamma-corrected.” Ours is not. Here’s an explanation of gamma correction from <a href="https://en.wikipedia.org/wiki/Gamma_correction">Wikipedia</a>:</p>

<blockquote>
  <p>Gamma correction, or often simply gamma, is a nonlinear operation used to encode and decode luminance or tristimulus values in video or still image systems.</p>
</blockquote>

<p>You can read more about gamma correction <a href="https://www.cambridgeincolour.com/tutorials/gamma-correction.htm">here</a> if you feel so compelled.</p>

<p>So basically, we need to transform our values before storing them. For a start, we’ll use gamma 2 - which would mean raising the colors to the power of 1/<em>gamma</em> (or.5) - mathematically identical to the square root:</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp:</code></p>
<pre><code class="language-cpp">...

int main() {

	int nx = 200; // Number of horizontal pixels
	int ny = 100; // Number of vertical pixels
	int ns = 10; // Number of samples for each pixel for anti-aliasing
	int maxDepth = 50; // Bounce limit
	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	    std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;
		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 1)) / double(nx);
				double v = (j + random_double(0.0, 1)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				col += color(r, world, maxDepth);
			}

			col /= double(ns); // Average the color between objects/background
<span class="highlight-green">			col = vec3(sqrt(col[0]), sqrt(col[1]), sqrt(col[2]));  // set gamma to 2</span>
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
    std::cerr &lt;&lt; "\nDone.\n";
}</code></pre>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-gamma.png" alt="Diffuse sphere with gamma correction" />
</span></p>

<h3 id="shadow-acne"><a id="shadow-acne"></a>Shadow Acne</h3>

<p>There’s one small issue left to fix, known as shadow acne. Some of the rays hit the sphere (or any object, really) not at t = 0, but rather at something like t = ±0.0000001 due to floating-point approximation. In that case, we’ll just change our hit detection specs in <code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>

<pre><code class="language-cpp">if (world.hit(r, 0.001, DBL_MAX, rec)) {</code></pre>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-shadow-acne.png" alt="Shadow acne sphere" />
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png" alt="Sphere no shadow acne" />
  </div>
</div>
  (Mouseover) Fix shadow acne
</div>

<p>You can view the images separately, as well. Here’s <a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-shadow-acne.png">the one with shadow acne</a> and <a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png">the one without</a>.</p>

<h3 id="true-lambertian-reflection"><a id="true-lambertian-reflection"></a>True Lambertian Reflection</h3>
<p>Recall that Lambertian reflectance is the “ideal” matte surface - the apparent brightness of a Lambertian surface to an observer is the same regardless of the observer’s angle of view.</p>

<p>Here’s Shirley’s explanation of the implementation of true Lambertian reflectance:</p>

<blockquote>
  <p>The rejection method presented here produces random points in the unit ball offset along the surface normal. This corresponds to picking directions on the hemisphere with high probability close to the normal, and a lower probability of scattering rays at grazing angles. This distribution scales by the cos3(ϕ) where ϕ is the angle from the normal. This is useful since light arriving at shallow angles spreads over a larger area, and thus has a lower contribution to the final color.
However, we are interested in a Lambertian distribution, which has a distribution of cos(ϕ). True Lambertian has the probability higher for ray scattering close to the normal, but the distribution is more uniform. This is achieved by picking points on the surface of the unit sphere, offset along the surface normal. Picking points on the sphere can be achieved by picking points in the unit ball, and then normalizing those.</p>
</blockquote>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/rand-unit-vector.png" alt="Generation of random unit vector" /></p>

<p>And our total replacement for <code class="language-plaintext highlighter-rouge">random_unit_sphere_coordinate()</code>:
(use 3.14 as pi for now, we’ll address it in the next section)</p>

<pre><code class="language-cpp">vec3 random_unit_vector() {
    auto a = random_double(0, 2*pi);
    auto z = random_double(-1, 1);
    auto r = sqrt(1 - z*z);
    return vec3(r*cos(a), r*sin(a), z);
}</code></pre>

<p>The result:</p>
<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png" alt="Lambertian approximation" />
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/lambertian.png" alt="True lambertian reflection" />
  </div>
</div>
  (Mouseover)True lambertian reflectance
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/diffuse-fix-shadow-acne.png">Lambertian approximation</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/lambertian.png">True Lambertian</a></li>
</ul>

<p>It’s a subtle difference, but a difference nonetheless. Notice that the shadows are not as pronounced and that both spheres are lighter.</p>

<p>These changes are both due to the more uniform scattering toward the normal. For diffuse objects, they appear lighter because more light bounces toward the camera. For shadows, less light bounces straight up.</p>

<hr />

<h2 id="common-constants-and-utilities"><a id="common-constants-and-utilities"></a>Common Constants and Utilities</h2>

<p>You may have noticed in <code class="language-plaintext highlighter-rouge">random_unit_vector()</code> that <em>pi</em> is not defined. That’s because in Shirley’s newer edition, he creates a general main header file with some constants and utilities:</p>

<pre><code class="language-cpp">#ifndef RTWEEKEND_H
#define RTWEEKEND_H

#include &lt;cmath&gt;
#include &lt;cstdlib&gt;
#include &lt;limits&gt;
#include &lt;memory&gt;


// Usings
using std::shared_ptr;
using std::make_shared;
using std::sqrt;

// Constants
const double infinity = std::numeric_limits&lt;double&gt;::infinity();
const double pi = 3.1415926535897932385;

// Utility Functions
inline double degrees_to_radians(double degrees) {
    return degrees * pi / 180;
}

// Common Headers
#include "ray.h"
#include "vec3.h"

#endif
</code></pre>

<p>And uses it in <code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;cfloat&gt;

<span class="highlight-green">
#include "rtweekend.h"
</span>

#include "sphere.h"
#include "hittableList.h"
#include "camera.h"
#include "random.h"

vec3 random_unit_vector() {
	auto a = random_double(0, 2*pi);
    auto z = random_double(-1, 1);
    auto r = sqrt(1 - z*z);
    return vec3(r*cos(a), r*sin(a), z);
}

/*
* Assign colors to pixels
*
* Background -
* Linearly blends white and blue depending on the value of y coordinate (Linear Blend/Linear Interpolation/lerp).
* Lerps are always of the form: blended_value = (1-t)*start_value + t*end_value.
* t = 0.0 = White
* t = 1.0 = Blue
* 
* Draw sphere and surface normals
*/
vec3 color(const ray&amp; r, hittable * world, int depth) {
	hit_record rec;

	if (depth &lt;= 0)
        return vec3(0,0,0); // Bounce limit reached - return darkness

<span class="highlight-green">	if (world-&gt;hit(r, 0.001, infinity, rec)) { </span>
		vec3 target = rec.p + rec.normal + random_unit_vector(); 
		return 0.5 * color(ray(rec.p, target - rec.p), world, depth-1); // light is absorbed continually by the sphere or reflected into the world.
	}
	else { // background
		vec3 unit_direction = unit_vector(r.direction());
		double t = 0.5 * (unit_direction.y() + 1.0);
		return (1.0 - t) * vec3(1.0, 1.0, 1.0) + t * vec3(0.5, 0.7, 1.0);
	}
}

int main() {

	int nx = 1600; // Number of horizontal pixels
	int ny = 800; // Number of vertical pixels
	int ns = 10; // Number of samples for each pixel for anti-aliasing
	int maxDepth = 50; // Bounce limit

	std::cout &lt;&lt; "P3\n" &lt;&lt; nx &lt;&lt; " " &lt;&lt; ny &lt;&lt; "\n255\n"; // P3 signifies ASCII, 255 signifies max color value

	// Create spheres
	hittable *list[2];
	list[0] = new sphere(vec3(0, 0, -1), 0.5);
	list[1] = new sphere(vec3(0, -100.5, -1), 100);
	hittable* world = new hittable_list(list, 2);
	camera cam;

	for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas
	        std::cerr &lt;&lt; "\rScanlines remaining: " &lt;&lt; j &lt;&lt; ' ' &lt;&lt; std::flush;

		for (int i = 0; i &lt; nx; i++) {
			vec3 col(0, 0, 0);
			for (int s = 0; s &lt; ns; s++) { // Anti-aliasing - get ns samples for each pixel
				double u = (i + random_double(0.0, 0.999)) / double(nx);
				double v = (j + random_double(0.0, 0.999)) / double(ny);
				ray r = cam.get_ray(u, v);
				vec3 p = r.point_at_parameter(2.0);
				col += color(r, world, maxDepth);
			}

			col /= double(ns); // Average the color between objects/background
			col = vec3(sqrt(col[0]), sqrt(col[1]), sqrt(col[2]));  // set gamma to 2
			int ir = int(255.99 * col[0]);
			int ig = int(255.99 * col[1]);
			int ib = int(255.99 * col[2]);
			std::cout &lt;&lt; ir &lt;&lt; " " &lt;&lt; ig &lt;&lt; " " &lt;&lt; ib &lt;&lt; "\n";
		}
	}
}</code></pre>

<p>While we’re here making changes, let’s clean things up a bit by:</p>
<ul>
  <li>Moving <code class="language-plaintext highlighter-rouge">random_unit_vector()</code> to <code class="language-plaintext highlighter-rouge">vec3.h</code></li>
  <li>Moving our <code class="language-plaintext highlighter-rouge">random number generator</code> to <code class="language-plaintext highlighter-rouge">rtweekend.h</code></li>
</ul>

<hr />

<h2 id="metal"><a id="metal"></a>Metal</h2>

<h3 id="abstract-class-for-materials"><a id="abstract-class-for-materials"></a>Abstract Class for Materials</h3>
<p>We’re going to use an abstract material class that encapsulates behavior which will do two things:</p>
<ul>
  <li>Produce a scattered ray (or absorb the incident ray)</li>
  <li>Determine attenuation (reduction of magnitude) of a scattered ray</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">#ifndef MATERIAL_H
#define MATERIAL_H

class material {
    public:
        virtual bool scatter(
            const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered
        ) const = 0;
};

#endif</code></pre>

<h3 id="describing-ray-object-intersections"><a id="describing-ray-object-intersections"></a>Describing Ray-Object Intersections</h3>
<p>The <code class="language-plaintext highlighter-rouge">hit_record</code> struct in <code class="language-plaintext highlighter-rouge">hittable.h</code> is where we’ll be storing whatever information we want about hits. We’ll be adding material to the struct.</p>

<p><code class="language-plaintext highlighter-rouge">hittable.h</code>:</p>
<pre><code class="language-cpp">#ifndef HITTABLEH
#define HITTABLEH

#include "ray.h"

<span class="highlight-green">class material; // forward declaration</span>

struct hit_record {
	double t; // parameter of the ray that locates the intersection point
	vec3 p; // intersection point
	vec3 normal;
	bool front_face;
<span class="highlight-green">	material* material_ptr;</span>

	inline void set_face_normal(const ray&amp; r, const vec3&amp; outward_normal) {
        front_face = dot(r.direction(), outward_normal) &lt; 0;
        normal = front_face ? outward_normal : -outward_normal;
    }
};

/* 
* A class for objects rays can hit.
*/
class hittable {
public: 
	virtual bool hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const = 0;
};

#endif // !HITTABLEH</code></pre>

<p>When a ray hits a surface, the material pointer within the hit struct will point to the material the object was instantiated with. As such, we’ll have to reference the material within our sphere class to be included with the <code class="language-plaintext highlighter-rouge">hit_record</code>.</p>

<p><code class="language-plaintext highlighter-rouge">sphere.h:</code></p>
<pre><code class="language-cpp">#ifndef SPHEREH
#define SPHEREH

#include "hittable.h"

class sphere : public hittable {
public:
	sphere() {}
<span class="highlight-green">	sphere(vec3 cen, float r, material* material) : center(cen), radius(r), material_ptr(material) {};</span>
	virtual bool hit(const ray&amp; r, double tmin, double tmax, hit_record&amp; rec) const;
	vec3 center;
	double radius;
<span class="highlight-green">	material* material_ptr;</span>
};

bool sphere::hit(const ray&amp; r, double t_min, double t_max, hit_record&amp; rec) const {
	vec3 oc = r.origin() - center; // Vector from center to ray origin
	double a = r.direction().length_squared();
	double halfB = dot(oc, r.direction());
	double c = oc.length_squared() - radius*radius;
	double discriminant = (halfB * halfB) - (a * c);
	if (discriminant &gt; 0.0) {
        auto root = sqrt(discriminant);

		auto temp = (-halfB - root) / a;

		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
			vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);
<span class="highlight-green">			rec.material_ptr = material_ptr;</span>
			return true;
		}
		temp = (-halfB + root) / a;
		if (temp &lt; t_max &amp;&amp; temp &gt; t_min) {
			rec.t = temp;
			rec.p = r.point_at_parameter(rec.t);
			vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);
<span class="highlight-green">			rec.material_ptr = material_ptr;</span>
			return true;
		}
	}
	return false;
}

#endif // !SPHEREH</code></pre>

<h3 id="light-scatter"><a id="light-scatter"></a>Light Scatter</h3>
<p>The Lambertian material we modeled previously would either scatter and attenuate by its reflectance <em>R</em>, or scatter with no attenuation but absorb 1 - <em>R</em> of rays, or somewhere in between. This is represented in code as follows:</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code></p>
<pre><code class="language-cpp">...
class lambertian : public material {
    public:
        lambertian(const vec3&amp; a) : albedo(a){};
        virtual bool scatter(const ray&amp; ray_in,
                            const hit_record&amp; rec,
                            vec3&amp; attenuation,
                            ray&amp; scattered) const {
            vec3 scatter_direction = rec.p + rec.normal + random_unit_vector();
            scattered = ray(rec.p, target - rec.p);
            attenuation = albedo;
            return true;
        }
    vec3 albedo; // reflectivity

};
...</code></pre>

<h3 id="metal-reflection"><a id="metal-reflection"></a>Metal Reflection</h3>
<p>Metal is definitely NOT Lambertian - here’s a simple sketch depecting a general mirrored reflection:</p>

<p><span class="captioned-image"> <img src="/assets/images/blog-images/path-tracer/the-first-weekend/metal-reflect.png" alt="Mirrored Reflection" /><em>Metal Reflection (<a href="http://viclw17.github.io/2018/07/30/raytracing-reflecting-materials">source</a>)</em></span></p>

<div class="math-block">
$$
\vec r = \vec v - (-2 * \vert \vec a\vert  * \vec n)
$$

where 

$$
\vert \vec a\vert  = \vert \vec v\vert  * cos(\theta)
$$

since 

$$
dot(\vec v, \vec n) = \vert \vec v\vert \vert \vec n\vert cos(\pi - \theta) = -\vert \vec v\vert cos(\theta)
$$

that means

$$
\vert \vec a\vert  = -dot(\vec v, \vec n)
$$

and

$$
\vec r = \vec v - (2 * dot(\vec v, \vec n) * \vec n)
$$
</div>

<p>In other words, the reflected ray is v + 2a. N is a unit vector, but that might not be the case for v. Also, because v points inward, we’re going have to flip it by negating it. This yields the following formula:</p>

<pre><code class="language-cpp">vec3 reflect(const vec3&amp; v, const vec3&amp; n){
    return v - 2*dot(v,n)*n;
}</code></pre>

<p>We can go ahead and incorporate this formula into our metal material:</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code></p>
<pre><code class="language-cpp">vec3 reflect(const vec3&amp; v, const vec3&amp; n){
    return v - 2*dot(v,n)*n; // v enters the hittable, which is why subtraction is required.
}

class material {
    public:
    virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const = 0;
};

// Matte surface
// Light that reflects off a diffuse surface has its direction randomized.
// Light may also be absorbed. See Diffuse.png for illustration and detailed description
class lambertian : public material {
    public:
        lambertian(const vec3&amp; a) : albedo(a){};
        virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const {
            vec3 target = rec.p + rec.normal + random_unit_sphere_coordinate();
            scattered = ray(rec.p, target - rec.p);
            attenuation = albedo;
            return true;
        }
    vec3 albedo; // reflectivity

};

class metal : public material {
    public:
        metal(const vec3&amp; a) : albedo(a) {}
        virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const {
        vec3 reflected = reflect(unit_vector(ray_in.direction()), rec.normal);
        scattered = ray(rec.p, reflected);
        attenuation = albedo;
        return dot(scattered.direction(), rec.normal) &gt; 0.0;
    }

    vec3 albedo;
};</code></pre>

<p>And of course, we’re going to need to update our <code class="language-plaintext highlighter-rouge">color</code>() function to use our new material:
<code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">vec3 color(const ray&amp; r, hittable *world, int depth) {
    hit_record rec;

    if (depth &lt;= 0) {
        return vec3(0,0,0);
    }  
    if (world-&gt;hit(r, 0.001, DBL_MAX, rec)) {
        ray scattered;
        vec3 attenuation;
		// Scatter formulas vary based on material
        if (rec.material_ptr-&gt;scatter(r, rec, attenuation, scattered)) {
            return attenuation*color(scattered, world, depth-1);
        }
        else {
            return vec3(0,0,0);
        }
    }
    else {
        vec3 unit_direction = unit_vector(r.direction());
        double t = 0.5*(unit_direction.y() + 1.0);
        return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
    }
}</code></pre>

<h3 id="adding-metal-spheres-to-the-scene"><a id="adding-metal-spheres-to-the-scene"></a>Adding Metal Spheres to the Scene</h3>

<p>Now that we’ve got some shiny new spheres, let’s add ‘em to the scene, render ‘em, and check ‘em out:
<code class="language-plaintext highlighter-rouge">main.cpp</code>:</p>
<pre><code class="language-cpp">int main {
	...

	hittable *list[4];
		hittable *list[4];
    	list[0] = new sphere(vec3(0,0,-1), 0.5, new lambertian(vec3(0.8, 0.3, 0.3)));
		list[1] = new sphere(vec3(0,-100.5,-1), 100, new lambertian(vec3(0.8, 0.8, 0.0)));
		list[2] = new sphere(vec3(1,0,-1), 0.5, new metal(vec3(0.8, 0.6, 0.2)));
		list[3] = new sphere(vec3(-1,0,-1), 0.5, new metal(vec3(0.8, 0.8, 0.8)));
		hittable *world = new hittable_list(list,4);

		camera cam(lookFrom, lookAt, vec3(0,1,0), 20,double(nx)/double(ny), aperture, distToFocus);	

		auto start = std::chrono::high_resolution_clock::now();


		for (int j = ny - 1; j &gt;= 0; j--) { // Navigate canvas

		...</code></pre>

<p>You’ll get something like this:</p>

<p><span class="captioned-image"><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal.png" alt="Metal and Lambertian spheres" /><em>Metal and Lambertian spheres</em></span></p>

<p>Feel free to mess around with the color, positioning, and material, as well:</p>

<p><span class="captioned-image"><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal-edit.png" alt="Metal spheres" /><em>Your new album cover</em></span></p>

<h3 id="fuzzy-metal"><a id="fuzzy-metal"></a>Fuzzy Metal</h3>

<p>In addition to perfectly polished metal spheres, we can simulate rough metal as well, with some “fuzziness.” To do so, we just need to append a random vector to the reflected rays:</p>

<p><span class="captioned-image"><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/reflect-fuzzy.png" alt="Fuzzy metal reflections" /><em>Generating fuzzy reflections</em> (<a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html"><em>source</em></a>)</span></p>

<p>The larger the sphere, the fuzzier the reflections will be. If the sphere is too large, we may scatter below the surface of an object. If that happens, we can just have the surface absorb those rays.</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">class metal : public material {
    public:
        metal(const vec3&amp; a, double f) : albedo(a) {
			if (f&lt;1) fuzz = f; else fuzz = 1;
        }
        virtual bool scatter(const ray&amp; ray_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered) const {
        vec3 reflected = reflect(unit_vector(ray_in.direction()), rec.normal);
        scattered = ray(rec.p, reflected);
        attenuation = albedo;
        return dot(scattered.direction(), rec.normal) &gt; 0.0;
    }

    vec3 albedo;
	double fuzz;
};</code></pre>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/all-metal-no-fuzz.png" alt="Metal - no fuzz" />
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal-fuzz.png" alt="Fuzzy Metal" />
  </div>
</div>
(Mouseover) Fuzziness from left to right: .5, 0, and 1
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/all-metal-no-fuzz.png">No fuzz</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/metal-fuzz.png">Fuzz</a></li>
</ul>

<hr />

<h2 id="dielectrics"><a id="dielectrics"></a>Dielectrics</h2>
<p>Dielectrics are materials like glass. When a light ray hits one, the ray splits into a reflected ray and a refracted ray. In this path tracer, we’ll be randomly choosing which ray to simulate, only generating one ray per interaction.</p>

<h3 id="refraction"><a id="refraction"></a>Refraction</h3>
<p>Refraction is the deflection of a ray from a straight path due to passing obliquely from one medium to another.</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/refraction.png" alt="Refraction" />
Notice both the reflected beam (top right) and the refracted beam (bottom right) (<a href="https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/F%C3%A9nyt%C3%B6r%C3%A9s.jpg/1200px-F%C3%A9nyt%C3%B6r%C3%A9s.jpg">source</a>)
</span></p>

<p>The refractive index describes the angle that light propagates through different mediums and is defined as:</p>

\[n={\frac {c}{v}}\]

<p>where <em>c</em> is the speed of light in a vacuum and v is the speed of light in the medium.</p>

<p>For reference, here are some refractive indices:</p>

<table>
  <thead>
    <tr>
      <th>Material</th>
      <th>Refractive Index</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Vacuum</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Air</td>
      <td>1.000293</td>
    </tr>
    <tr>
      <td>Carbon Dioxide</td>
      <td>1.001</td>
    </tr>
    <tr>
      <td>Ice</td>
      <td>1.31</td>
    </tr>
    <tr>
      <td>Water</td>
      <td>1.333</td>
    </tr>
    <tr>
      <td>Kerosene</td>
      <td>1.39</td>
    </tr>
    <tr>
      <td>Vegetable Oil</td>
      <td>1.47</td>
    </tr>
    <tr>
      <td>Window Glass</td>
      <td>1.52</td>
    </tr>
    <tr>
      <td>Amber</td>
      <td>1.55</td>
    </tr>
    <tr>
      <td>Diamond</td>
      <td>2.417</td>
    </tr>
    <tr>
      <td>Germanium</td>
      <td>4.05</td>
    </tr>
  </tbody>
</table>

<h3 id="snells-law"><a id="snells-law"></a>Snell’s Law</h3>
<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/snells-law.svg" alt="Snell's Law" /></p>

<p>Snell’s law states that the ratio of the sines of the angles of incidence and refraction is equivalent to the ratio of phase velocities in the two media, or equivalent to the reciprocal of the ratio of the indices of refraction:</p>

\[{\frac {\sin \theta _{2}}{\sin \theta _{1}}}={\frac {v_{2}}{v_{1}}}={\frac {n_{1}}{n_{2}}}\]

<p>with each θ as the angle measured from the normal of the boundary, v as the velocity of light in the respective medium, n as the refractive index (which is unitless) of the respective medium.</p>

<p>If we render a dielectric object with <code class="language-plaintext highlighter-rouge">ref_index</code> in a vacuum, (since the refractive index is 1 in a vacuum) we get this:</p>

\[\frac {n_{1}}{n_{2}} = \frac {1}{ref\_index}\]

<p>and when the ray shoots back out into the vacuum:</p>

\[\frac {n_{1}}{n_{2}} = {ref\_index}\]

<h3 id="total-internal-reflection"><a id="total-internal-reflection"></a>Total Internal Reflection</h3>
<p>Total internal reflection is an optical phenomenon that occurs when light travels from a medium with a higher refractive index to a lower one, and the angle of incidence is greater than a certain angle (known as the “critical angle”).</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/total-internal-reflection.svg" alt="Total internal reflection" style="background: wheat;" /></p>
<h3 id="calculating-the-refraction-vector"><a id="calculating-the-refraction-vector"></a>Calculating the Refraction Vector</h3>
<p>Now that we’re familiar with the components of refraction, we can calculate the refraction vector geometrically.</p>

<!-- 
![Refraction geometry](/assets/images/blog-images/path-tracer/the-first-weekend/refraction-geometry.png)

We already know [Snell's law](#snells-law):

$$
{\frac {\sin \theta _{2}}{\sin \theta _{1}}}={\frac {v_{2}}{v_{1}}}={\frac {n_{1}}{n_{2}}}
$$

To determine the direction of the refracted ray, we'll have to solve for $\sin\theta_2$.

So we'll re-arrange Snell's law:

$$
\sin\theta_2 = \frac{n_1}{n_2} \cdot \sin\theta
$$

On the side of the surface with the refracted ray, there's a refracted ray $\vec R_2$ and a normal $n_2$, with an angle $theta_2$ between them. What we can do is split $R_2$ into components that are parallel, and components that are perpendicular to $n_2$.

$$
\mathbf{R_2} = \mathbf{R_2}_{\parallel} + \mathbf{R_2}_{\bot}
$$

Solving for $\mathbf{R_2}_{\parallel}$:

$$

$$ -->

<p>We can model the relationships of the vectors with a unit circle to make things easier.</p>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/refraction-vector-circle.png" alt="Refraction unit circle" />
(<em><a href="http://viclw17.github.io/2018/08/05/raytracing-dielectric-materials">source</a></em>)
</span></p>

\[\vec r = \vec A + \vec B\]

<p>We’ll project $\vec r$ onto $\vec M$ to get $\vec A$:</p>

\[\vec A = sin\theta_{2} \cdot \vec M\]

<p>Similarly, we’ll project $\vec r$ onto $- \vec N$ to get $\vec B$:</p>

\[\vec B = cos\theta_{2} \cdot -\vec N\]

<p>We’ll define $\vec M$ as perpendicular($\bot$) to $\vec N$:</p>

\[\vec M = Normalize(\vec C + \vec v) = \frac {(\vec C + \vec v)}{sin\theta_{1}}\]

<p>And we’ll project $\vec r$ onto $- \vec N$ to get $\vec C$:</p>

\[\vec C = cos\theta_{1} \cdot \vec N\]

<p>So, with terms expanded, we go from:</p>

\[\vec r = \vec A + \vec B\]

<p>to:</p>

\[\vec r = sin\theta_{2} \cdot \vec M - cos\theta_{2} \cdot \vec N\]

<p>If we expand and re-arrange the equation, we’ll end up with this:</p>

\[\vec r = \frac {n_{1}}{n_{2}} \cdot (\vec v + cos\theta_{1} \cdot \vec N) - cos\theta_{2} \cdot \vec N\]

<p>After normalizing incidence ray direction $\vec v$ , we can calculate $cos\theta_{1}$ by $dot(\vec v, \vec n) = \vert \vec v\vert \vert \vec n\vert cos\theta_{1} = cos\theta_{1}$.</p>

<p>Since Snell’s law can also be interpreted as:</p>

\[sin\theta_{2} = \frac {n_{1}}{n_{2}} \cdot sin\theta_{1},\]

\[cos^2\theta_{2} = 1 - sin^2\theta_{2} = 1 - \frac {(n_1)^2}{(n_2)^2} \cdot sin^2\theta_{1} = 1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - cos^2\theta_{1}) = 1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - dot(\vec v, \vec n))\]

<p>and the equation can be written as:</p>

\[\vec r = \frac {n_{1}}{n_{2}} \cdot (\vec v - dot(\vec v, \vec n) \cdot \vec N) - \sqrt{1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - dot(\vec v, \vec n))} \cdot \vec N\]

<p>Which means that:</p>

\[cos^2\theta_{2} = 1 - \frac {n_{1}^2}{n_{2}^2} \cdot (1 - dot(\vec v, \vec n))\]

<p>is the discriminant.</p>

<table>
  <thead>
    <tr>
      <th>Discriminant</th>
      <th>Ray Behavior</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$&lt;0$</td>
      <td>Total internal reflection</td>
    </tr>
    <tr>
      <td>$=0$</td>
      <td>Boundary of total reflection - no resultant ray</td>
    </tr>
    <tr>
      <td>$&gt;0$</td>
      <td>Refracted ray $\vec r$</td>
    </tr>
  </tbody>
</table>

<h3 id="coding-the-refraction-vector"><a id="coding-the-refraction-vector"></a>Coding the Refraction Vector</h3>

<ul>
  <li>$\frac {n_{1}}{n_{2}}$ is <code class="language-plaintext highlighter-rouge">n1_over_n2</code></li>
  <li>v is the incidence ray</li>
  <li>n is the surface normal</li>
  <li>refracted is the refracted ray’s direction</li>
</ul>

<pre><code class="language-cpp">bool refract(const vec3&amp; v, const vec3&amp; n, float n1_over_n2, vec3&amp; refracted) {
    vec3 uv = unit_vector(v);
    float dt = dot(uv, n);
    float discriminat = 1.0 - ni_over_nt * ni_over_nt * (1-dt*dt);
    if(discriminat &gt; 0){
        refracted = ni_over_nt * (uv-n*dt) - n*sqrt(discriminat);
        return true;
    }
    else
        return false; // no refracted ray
}</code></pre>

<h3 id="dielectric-reflections"><a id="dielectric-reflections"></a>Dielectric Reflections</h3>
<p>When light strikes a dielectric object, both reflection and refraction may occur. Looking at a puddle at a sharp enough angle makes it look almost like a mirror!</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/fresnel.png" alt="Fresnel puddle" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/partial-transmittance.gif" alt="Patrial transmittance" /></p>

<p>For low-precision applications that don’t involve polarized light, <a href="https://en.wikipedia.org/wiki/Schlick%27s_approximation">Schlick’s approximation</a> will serve our purposes just fine, rather than computing the effective reflection coefficient for every angle.</p>

<p>Schlick’s model states that the specular reflection coefficient can be approximated by:</p>

<p>\(R(\theta_{1})=R_{0}+(1-R_{0})(1-\cos \theta_{1} )^{5}\)
\(R_{0}=\left({\frac {n_{1}-n_{2}}{n_{1}+n_{2}}}\right)^{2}\)</p>

<ul>
  <li>$\theta_1$ is the incident angle.</li>
  <li>$n_1$ and $n_2$ are the refractive indices of the two media.</li>
  <li>$R_0$ is the reflection coefficient for light incoming parallel to the normal.</li>
</ul>

<p>The refractive index of air(1.000293) is often approximated as 1.</p>

\[\frac {n_{1}}{n_{2}} = \frac {1}{n_{dielectric}} \Rightarrow {n_{dielectric}} = \frac {n_{2}}{n_{1}}\]

\[cos\theta_{1} = dot(\vec v, \vec n)\]

<p>We can add Schlick’s approximation to <code class="language-plaintext highlighter-rouge">material.h</code></p>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">float schlick(float cosine, float ref_idx) {
    float r0 = (1 - ref_index) / (1 + ref_index); // ref_index = n2/n1
    r0 = r0 * r0;
    return r0 + (1 - r0) * pow((1 - cosine), 5);
}</code></pre>

<p>If the incident ray produces a refraction ray (which we can check by seeing if <code class="language-plaintext highlighter-rouge">refract()</code> returns true), we are going to calculate the reflective coefficient <code class="language-plaintext highlighter-rouge">reflect_probability</code>. Otherwise, the ray exhibits total internal reflection and the reflective coefficient should be 1.</p>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/refraction-reflection.gif" alt="Refraction and reflection gif" /></p>

<p>We do this by getting whichever value is smaller between:</p>
<ul>
  <li>the dot product of flipped unit incident ray and the normal of the hit point OR</li>
  <li>1.0 (Total internal reflection)</li>
</ul>

<p>You may be wondering how we are to represent the refraction and reflection of light if we can only pick one scattered ray. The answer is multi-sampling - averaging the color between samples that may have been reflected or refracted.</p>

<p>To get an accurate result, we’ll use our random number generator. We’ll generate a number between 0.0 and 1.0. If the resulting number is smaller than the reflective coefficient, the ray will be reflected. Otherwise, it will be refracted.</p>

<p>We can now bundle everything up into our dielectric material’s <code class="language-plaintext highlighter-rouge">scatter()</code> method:</p>

<p><code class="language-plaintext highlighter-rouge">material.h</code>:</p>
<pre><code class="language-cpp">...

class dielectric : public material {
    public:
        dielectric(vec3 a, double ri) : albedo(a), ref_idx(ri) {}

        virtual bool scatter(
            const ray&amp; r_in, const hit_record&amp; rec, vec3&amp; attenuation, ray&amp; scattered
        ) const {

            attenuation = albedo; // color

            double n1_over_n2 = (rec.front_face) ? (1.0 / ref_idx) : (ref_idx);

            vec3 unit_direction = unit_vector(r_in.direction());
            
            double cosine = dot(-unit_direction, rec.normal);
            double reflect_random = random_double(0,1);
            double reflect_probability;

            vec3 refracted;
            vec3 reflected;

			// refracted ray exists
            if (refract(unit_direction, rec.normal, n1_over_n2, refracted)) {
                reflect_probability = schlick(cosine, ref_idx);

                if (reflect_random &lt; reflect_probability) {
                    vec3 reflected = reflect(unit_direction, rec.normal);
                    scattered = ray(rec.p, reflected);
                    return true;
                }
                scattered = ray(rec.p, refracted);
                return true;
            }

            else {
                reflected = reflect(unit_direction, rec.normal);
                scattered = ray(rec.p, reflected);
                return true;
            }
        }
    public:
        double ref_idx;
        vec3 albedo;
};</code></pre>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/no-fresnel.png" alt="Dielectric without Fresnel" />
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/fresnel.png" alt="Dielectric with Fresnel" />
  </div>
</div>
  (Mouseover) Implementation of Fresnel reflections
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/no-fresnel.png">No fresnel</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/fresnel.png">Fresnel</a></li>
</ul>

<h3 id="hollow-dielectric-spheres"><a id="hollow-dielectric-spheres"></a>Hollow Dielectric Spheres</h3>

<p>Bonus fun fact! We can create a hollow glass sphere by creating a smaller sphere with a <em>negative</em> radius <em>inside</em> our existing sphere! The geometry is unaffected, but the normal points inward.</p>

<div class="captioned-image">
<div class="container">
  <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-solid.png" alt="Solid Dielectric" />
  <div class="overlay">
    <img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-hollow.png" alt="Hollow Dielectric" />
  </div>
</div>
  (Mouseover) Hollow dielectric sphere
</div>

<ul>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-solid.png">Solid</a></li>
  <li><a href="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dielectric-hollow.png">Hollow</a></li>
</ul>

<p>And of course, you can change the color of your pretty new dielectric sphere if you please.
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/purple-dielectric.png" alt="Purple dielectric" /></p>

<hr />

<h2 id="camera-modeling"><a id="camera-modeling"></a>Camera Modeling</h2>
<p>Up until now, we’ve been using a very simple camera (though I have changed framing a little bit for illustrating certain renders). Our simple camera was described way back in chapter 4 of Shirley’s book, and it had fixed world position at the origin (0, 0, 0), a fixed image plane (or near-clipping plane) size, and the plane was position at (0, 0, -1), effectively setting our sights along the negative z-axis.</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code>:</p>
<pre><code class="language-cpp">#ifndef CAMERAH
#define CAMERAH

#include "ray.h"

class camera {
public:

	// The values below are derived from making the "camera" / ray origin coordinates(0, 0, 0) relative to the canvas.
	camera() {
		lower_left_corner = vec3(-2.0, -1.0, -1.0);
		horizontal = vec3(4.0, 0.0, 0.0);
		vertical = vec3(0.0, 2.0, 0.0);
		origin = vec3(0.0, 0.0, 0.0);
	}
	ray get_ray(double u, double v) { return ray(origin, lower_left_corner + u * horizontal + v * vertical - origin); }

	vec3 origin;
	vec3 lower_left_corner;
	vec3 horizontal;
	vec3 vertical;
};

#endif // !CAMERAH</code></pre>

<p>We’re going to expand the capability of the camera and make it more flexible by defining a few different variables:</p>

<ul>
  <li>Camera position <code class="language-plaintext highlighter-rouge">look_from</code></li>
  <li>Camera objective <code class="language-plaintext highlighter-rouge">look_at</code></li>
  <li>Vector describing which way is “up” <code class="language-plaintext highlighter-rouge">vup</code></li>
  <li>Vertical field-of-view <code class="language-plaintext highlighter-rouge">vfov</code></li>
  <li>Image plane aspect ratio <code class="language-plaintext highlighter-rouge">aspect_ratio</code></li>
  <li>Camera lens size <code class="language-plaintext highlighter-rouge">aperture</code></li>
  <li>Image plane to camera distance <code class="language-plaintext highlighter-rouge">focus_distance</code></li>
</ul>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/camera-model.png" alt="Camera visualization" /></p>

<p>If the angle of the camera lens is <code class="language-plaintext highlighter-rouge">theta</code>, we can see that <code class="language-plaintext highlighter-rouge">half_height</code> = <code class="language-plaintext highlighter-rouge">tan(theta/2)</code>.</p>

\[tan(\frac {\theta}{2}) = \frac{opposite}{adjacent} = \frac {half\_height}{1}\]

<p><code class="language-plaintext highlighter-rouge">theta</code> is the vertical field of view <code class="language-plaintext highlighter-rouge">vfov</code>. For the convenience of calculation, we can convert theta to radians and define it as <code class="language-plaintext highlighter-rouge">vfov * pi / 180</code>.</p>

<p>Keeping other camera settings the same, we can rewrite our camera:
<code class="language-plaintext highlighter-rouge">camera.h</code></p>
<pre><code class="language-cpp">...
lower_left_corner(-half_width, -half_height,-1.0);
horizontal(2*half_width, 0.0, 0.0); // horizontal range
vertical(0.0, 2*half_height, 0.0);  // vertical range
origin = (0,0,0);
...</code></pre>

<p>In world space(three dimensions), the vectors</p>

\[e1 = (1,0,0),\]

\[e2 = (0,1,0),\]

\[e3 = (0,0,1)\]

<p>form the <a href="https://en.wikipedia.org/wiki/Standard_basis">standard basis</a>.</p>

<p>The standard basis (for a Euclidean space) is an <a href="https://en.wikipedia.org/wiki/Orthonormal_basis">orthonormal basis</a>, where the relevant inner product is the dot product of vectors. Put simply, this means the vectors are orthogonal: at right angles to each other; and normal: all of the same length 1.</p>

<p>All vectors (x,y,z) in world space can be expressed as a sum of the scaled basis vectors.</p>

\[{\displaystyle (x,y,z)=xe_{1}+ye_{2}+ze_{3}}\]

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/standard-basis.svg" alt="Standard basis" />
Every vector $a$ in three dimensions is a linear combination of the standard basis vectors $i$, $j$, and $k$.
</span></p>

<p>Therefore, the previous camera code should be revised to</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code></p>
<pre><code class="language-cpp">lower_left_corner= origin - half_width * e1 - half_height * e2 - e3
horizontal = 2 * half_width * e1
vertical = 2 *half_height * e2</code></pre>

<p>However, if we want to move our “camera” to position <code class="language-plaintext highlighter-rouge">look_from</code> pointing to <code class="language-plaintext highlighter-rouge">look_at</code>, we have to build a new orthonormal basis for camera space with vectors u, v, and w:</p>

<pre><code class="language-cpp">w = unit_vector(lookfrom - lookat) // similar to the Z axis
u = unit_vector(cross(vup, w)) // similar to the X axis
v = cross(w, u) // similar to the Y axis</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/orthonormal-basis.png" alt="New orthonormal basis" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/orthonormal-basis.png" alt="Shirley orthonormal basis" /></p>

<p>The <code class="language-plaintext highlighter-rouge">vup</code> vector describes which direction is up for the camera. You can also think of this as tilt in any (x,y,z).</p>

<div class="row-fill">
	<div class="captioned-image">
	<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/vup-010.png" />
	vup (0,1,0)
	</div>
	<div class="captioned-image">
	<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/vup-0neg10.png" />
	vup (0,-1,0)
	</div>
	<div class="captioned-image">
	<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/vup-110.png" />
	vup (1,1,0)
	</div>
</div>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/shirley/orthonormal-vup.png" alt="Shirley vector-up" /></p>

<pre><code class="language-cpp">class camera {
public:
    camera(vec3 look_from, vec3 look_at, vec3 vup, float vfov, float aspect_ratio) {
        vec3 u, v, w;

        float theta = vfov*pi/180;
        float half_height = tan(theta/2);
        float half_width = aspect_ratio * half_height;
        origin = lookfrom;

        w = unit_vector(look_from - look_at);
        u = unit_vector(cross(vup, w));
        v = cross(w, u);

        lower_left_corner = origin - half_width*u - half_height*v -w;
        horizontal = 2*half_width*u;
        vertical = 2*half_height*v;
    }
    ray get_ray(float s, float t) {return ray(origin, lower_left_corner + s*horizontal + t*vertical - origin);}

    vec3 lower_left_corner;
    vec3 horizontal;
    vec3 vertical;
    vec3 origin;
};</code></pre>

<hr />

<h2 id="depth-of-field"><a id="depth-of-field"></a>Depth of Field</h2>

<p>Depth of field! If you have eyes that kind of work, you’re familiar with it. You can read more about DOF at <a href="https://en.wikipedia.org/wiki/Depth_of_field">Wikipedia</a>. Depth of field (DOF) is the distance between the nearest and farthest objects that are acceptably sharp in an image. The subjects outside the DOF are subject to blur.</p>

<p>From Wikipedia:</p>
<blockquote>
  <p>DOF can be calculated based on focal length, distance to subject, the acceptable “circle of confusion size”, and aperture.</p>
</blockquote>

<p><span class="captioned-image">
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/depth-of-field.png" alt="Depth of field (PBR!)" />
<em>PBR!</em> (<a href="https://www.joepylephotography.com/depth-of-field-joe-pyle-photography/"><em>source</em></a>)
</span></p>

<blockquote>
  <p>In optics, an aperture is a hole or an opening through which light travels. More specifically, the aperture and focal length of an optical system determine the cone angle of a bundle of rays that come to a focus in the image plane.</p>
</blockquote>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/aperture-diagram.gif" alt="Aperture diagram" /></p>

<p>From the beginning, all scene rays have originated from <code class="language-plaintext highlighter-rouge">look_from</code>. To simulate a variable aperature, we’ll generate rays randomly originating from inside a disk centered at <code class="language-plaintext highlighter-rouge">look_from</code>. The larger we make this disc, the stronger the blur will be. With a disk radius of zero, the rays all originate from <code class="language-plaintext highlighter-rouge">look_from</code>, eliminating blur. One such method for generating a point inside a unit disk is as follows:</p>

<p><code class="language-plaintext highlighter-rouge">vec3.h</code>:</p>
<pre><code class="language-cpp">vec3 random_unit_disk_coordinate() {
    while (true) {
        auto p = vec3(random_double(-1,1), random_double(-1,1), 0);
        if (p.length_squared() &gt;= 1) continue;
        return p;
    }
}</code></pre>

<p>Up until now, the focus distance was -1 on the z (or w) axes. We’ll now make it <code class="language-plaintext highlighter-rouge">focus_distance</code> and define our image plane accordingly:</p>

<pre><code class="language-cpp">lower_left_corner = origin - half_width*focus_distance*u - half_height*focus_distance*v - focus_distance*w;
horizontal = 2 * half_width*focus_distance*u;
vertical = 2 * half_height*focus_distance*v;</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/camera-model-summary-aperture.png" alt="Aperture diagram" /></p>

<p>With that, we have a complete camera class:</p>

<p><code class="language-plaintext highlighter-rouge">camera.h</code></p>
<pre><code class="language-cpp">class camera {
    public:
        camera(vec3 look_from, vec3 look_at, vec3 vUp, double vFov, double aspect_ratio, double aperture, double focus_distance) {
            
            lens_radius = aperture / 2;
            
            double theta = vFov*pi/180;
            double half_height = tan(theta/2);
            double half_width = aspect_ratio * half_height;
            origin = look_from;
            
            w = unit_vector(look_from - look_at);
            u = unit_vector(cross(vUp, w));
            v = cross(w, u);

            lowerLeftCorner = origin
                              - half_width * focus_distance * u
                              - half_height * focus_distance * v
                              - focus_distance * w;
            horizontal = 2*half_width*focus_distance*u;
            vertical = 2*half_height*focus_distance*v;
        }

        ray get_ray(double s, double t) {
            vec3 rd = lens_radius*random_unit_disk_coordinate();
            vec3 offset = u * rd.x() + v * rd.y();

            return ray(origin + offset,
                       lowerLeftCorner + s*horizontal + t*vertical - origin - offset);
        }

        vec3 origin;
        vec3 lowerLeftCorner;
        vec3 horizontal;
        vec3 vertical;
        vec3 u, v, w;
        double lens_radius;
};</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dof-0.png" alt="Aperture 0" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dof-point-2.png" alt="Aperture 0.2" />
<img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/dof-point-5.png" alt="Aperture 0.5" /></p>

<hr />

<h2 id="final-scene"><a id="final-scene"></a>Final Scene</h2>

<p>Lastly, we’ll create a random scene of spheres. Feel free to customize. And be aware - this may take some time to render!</p>

<p><code class="language-plaintext highlighter-rouge">main.cpp</code></p>
<pre><code class="language-cpp">...
hittable *random_scene() {
    int n = 500;
    hittable **list = new hittable*[n+1];
    list[0] =  new sphere(vec3(0,-1000,0), 1000, new lambertian(vec3(0.5, 0.5, 0.5))); // "Ground"
    int i = 1;
    for (int a = -11; a &lt; 11; a++) {
        for (int b = -11; b &lt; 11; b++) {
            double randomMaterial = random_double(0,1);
            vec3 center(a+0.9*random_double(0,1),0.2,b+0.9*random_double(0,1));
            if ((center-vec3(4,0.2,0)).length() &gt; 0.9) {
                if (randomMaterial &lt; 0.68) {  // diffuse
                    list[i++] = new sphere(center, 0.2,
                        new lambertian(vec3(random_double(0,1)*random_double(0,1),
                                            random_double(0,1)*random_double(0,1),
                                            random_double(0,1)*random_double(0,1))
                        )
                    );
                }
                else if (randomMaterial &lt; 0.87) { // metal
                    list[i++] = new sphere(center, 0.2,
                            new metal(vec3(0.5*(1 + random_double(0,1)),
                                           0.5*(1 + random_double(0,1)),
                                           0.5*(1 + random_double(0,1))),
                                      0.5*random_double(0,1)));
                }
                else {  // glass
                    list[i++] = new sphere(center, 0.2, new dielectric(vec3(random_double(0,1),random_double(0,1),random_double(0,1)), 1.5));
                }
            }
        }
    }

    list[i++] = new sphere(vec3(0, 1, 0), 1.0, new dielectric(vec3(1.0,1.0,1.0), 1.5));
    list[i++] = new sphere(vec3(-4, 1, 0), 1.0, new lambertian(vec3(0.4, 0.2, 0.1)));
    list[i++] = new sphere(vec3(4, 1, 0), 1.0, new metal(vec3(1.0, 1.0, 1.0), 0.0));

    return new hittable_list(list,i);
}

int main() {

...

double aperture = 0.2; // bigger = blurrier

    hittable *world = random_scene();

	camera cam(lookFrom, lookAt, vec3(0,1,0), 20,double(nx)/double(ny), aperture, distToFocus);
...
}
...</code></pre>

<p><img src="/assets/images/blog-images/path-tracer/the-first-weekend/renders/final-render-1.png" alt="Final render" /></p>]]></content><author><name>Evan</name></author><category term="graphics" /><category term="ray-tracing-in-one-weekend" /><category term="c++" /><summary type="html"><![CDATA[Now that we're familiar with ray tracing through [my introduction]({{ site.url }}/2020/05/20/ray-tracing-in-one-weekend-part-one.html#post-title), we can delve into the titular first section of Peter Shirley's book.]]></summary></entry><entry><title type="html">Ray Tracing in One Weekend:</title><link href="eldun.github.io/2020/05/20/ray-tracing-in-one-weekend-part-one.html" rel="alternate" type="text/html" title="Ray Tracing in One Weekend:" /><published>2020-05-20T00:00:00-04:00</published><updated>2020-05-20T00:00:00-04:00</updated><id>eldun.github.io/2020/05/20/ray-tracing-in-one-weekend-part-one</id><content type="html" xml:base="eldun.github.io/2020/05/20/ray-tracing-in-one-weekend-part-one.html"><![CDATA[<h2 id="what-is-ray-tracing"><a id="what-is-ray-tracing"></a>What is Ray Tracing?</h2>

<div class="row-fill">
<img alt="Minecraft (2011 Initial Release, 2020 Path Tracing Update)" src="/assets/images/blog-images/path-tracer/introduction/minecraft-ray-tracing-off.png" />
<img alt="Minecraft (2011 Initial Release, 2020 Path Tracing Update)" src="/assets/images/blog-images/path-tracer/introduction/minecraft-ray-tracing-on.png" />
</div>

<p>Put simply, ray tracing is a rendering technique that can accurately simulate the lighting of a scene.</p>

<p>Ray tracing generates an image by determining the color of each pixel of the image in a mathematically formulated scene. In the simplest example, the color of each one of these pixels is determined by sending a ray from the camera into the scene, and back to its light source.</p>

<p>The ray will potentially collide with - and bounce off of - scene objects. Each of these scene objects has intrinsic properties, such as reflectivity, refractive index, and roughness. The interactions of the ray among all these objects are combined with the light source to determine the final color and intensity of the pixel. This process is repeated for the whole image, pixel by pixel.</p>

<p>You may be thinking: “Why send a ray from the camera instead of from the light source?”
The reason is a matter of efficiency; rather than sending rays out from the light in all directions, it’s much more effective to only trace the bare minimum - which in our simplistic example - is one ray for each pixel of the image.</p>

<p><img src="/assets/images/blog-images/path-tracer/introduction/ray-tracing-put-simply.png" alt="Ray Tracing Illustration" /></p>

<hr />

<h2 id="what-is-path-tracing"><a id="what-is-path-tracing"></a>What is Path Tracing?</h2>
<p><img src="/assets/images/blog-images/path-tracer/introduction/path-traced-dragon.png" alt="Path traced glass dragon" /></p>

<p>Path tracing is similar to ray tracing, but much more intensive. Hundreds to thousands of rays are traced through each pixel of the image, with numerous bounces off of - or through - objects, before reaching the light source (or hitting a specified “bounce limit”) to generate more accurate color and lighting information.</p>

<p><img src="/assets/images/blog-images/path-tracer/introduction/minecraft-ray-tracing-on-compilation.png" alt="Minecraft (2011 Initial Release, 2020 Path Tracing Update)" /></p>

<p>Path tracing, as you may have guessed, is more accurate a simulation than ray tracing, simulating soft shadows, caustics, and global illumination. However, it is more “brute-force.” Without enough rays through each pixel or simulated bounces for each ray, the final image will be ridden with noise.</p>

<p>Also - fun fact: I learned recently path tracing requires light sources to have physical size instead of “point lights” used in ray tracing or rasterized graphics… which brings me to my next point…</p>

<hr />

<h2 id="what-is-rasterization"><a id="what-is-rasterization"></a>What is Rasterization?</h2>
<p>Rasterization is the <a href="#an-abbreviated-graphics-timeline">vast majority of games</a> have used in to display 3D scenes on 2D screens. Using rasterization, objects are represented with virtual triangles (aka polygons). These triangles all have corners, and these vertices contain data of such attributes as position, color, texture, and the surface normal (orientation).</p>

<p>The triangles are eventually converted to pixels when being rendered. Each pixel can be assigned an initial color value from the data stored in the triangle vertices. Further pixel processing or “shading” including changing color based on how lights in the scene hit, and applying one or more textures, combine to generate the final color applied to a pixel.</p>

<p><img src="/assets/images/blog-images/path-tracer/introduction/raster.png" alt="Vertices being converted to pixels" /></p>

<p>Rasterization is used in real-time computer graphics and while still computationally intensive, it is less so compared to ray tracing.</p>

<div class="captioned-image">
The following images are from rasterized game engines
<div class="row-fill three-images">
    <!-- ![Nintendo 64 (1996 (North America))](/assets/images/blog-images/path-tracer/introduction/n64.png) -->
    <img alt="Super Mario 64 (1996)" src="/assets/images/blog-images/path-tracer/introduction/mario.png" />
    <img alt="F-Zero X (1998)" src="/assets/images/blog-images/path-tracer/introduction/f-zero-x.png" />
    <img alt="The Legend of Zelda: Ocarina of Time (1998)" src="/assets/images/blog-images/path-tracer/introduction/zelda.png" />
</div>
<div class="row-fill">
    <img alt="Solid Snake of Metal Gear Solid (1998)" src="/assets/images/blog-images/path-tracer/introduction/metal-gear-solid.png" />
    <img alt="Crash Bandicoot (1996)" src="/assets/images/blog-images/path-tracer/introduction/crash-bandicoot.png" />
    <img alt="Tekken (1995)" src="/assets/images/blog-images/path-tracer/introduction/tekken.png" />
</div>
<img alt="Assassin's Creed Unity (2015)" src="/assets/images/blog-images/path-tracer/introduction/ac-unity-rasterized.png" />
</div>

<hr />

<h2 id="a-happy-medium-for-now"><a id="a-happy-medium"></a>A Happy Medium (For Now)</h2>
<p>Rasterization and ray tracing can be combined! Rasterization can determine visible objects relatively quickly, while ray tracing can be used to improve the quality of reflections, refractions, and shadows.</p>

<p><img src="/assets/images/blog-images/path-tracer/introduction/hybrid-ray-tracing.png" alt="Hybrid ray tracing" /></p>

<hr />

<h2 id="practical-applications-of-ray-tracing"><a id="practical-applications-of-ray-tracing"></a>Practical Applications of Ray Tracing</h2>
<p>Applications of ray tracing are many and varied:</p>
<ul>
  <li>Real-time rendering (video games)</li>
  <li>Non-real-time rendering (film and television)</li>
  <li>Architecture / lighting design</li>
  <li>Engineering</li>
  <li>Acoustics modeling</li>
  <li>Radio propagation modeling</li>
  <li>Physics simulations</li>
</ul>

<p><span class="row-fill">
<img src="/assets/images/blog-images/path-tracer/introduction/control-boots.png" alt="Control(2019)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/battlefield-five-lobby.png" alt="Battlefield V(2018)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/battlefield-five-street.png" alt="Battlefield V(2018)" />
</span>
<span class="row-fill">
<img src="/assets/images/blog-images/path-tracer/introduction/toy-story-four.png" alt="(2019)" />
</span>
<span class="row-fill two-images">
<img src="/assets/images/blog-images/path-tracer/introduction/building.png" alt="(Ray traced building)" />
<img src="/assets/images/blog-images/path-tracer/introduction/room-acoustics.png" alt="Ray traced acoustics" />
</span>
<span class="row-fill">
<img src="/assets/images/blog-images/path-tracer/introduction/couch.png" alt="(Ray traced building)" />
</span></p>

<hr />

<h2 id="an-abbreviated-graphics-timeline"><a id="an-abbreviated-graphics-timeline"></a>An Abbreviated Graphics Timeline</h2>
<p>I was born in 1995 - an exciting year for computer graphics. <em>Toy Story</em> - the first entirely
computer-animated feature film - would be released. Homer Simpson, of <em>The Simpsons</em> fame, would be computer-animated for a <em>Treehouse of Horror</em> Halloween episode. The Sony Playstation would be released in the United States.</p>

<p><span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/toy-story.png" alt="Toy Story" />
    <img src="/assets/images/blog-images/path-tracer/introduction/homer.png" alt="Homer Simpson" />
</span>
<span class="row-fill">
<img src="/assets/images/blog-images/path-tracer/introduction/playstation-1.png" alt="Playstation" />
</span></p>

<p>Technology had come a long way since 1980’s <em>Battlezone</em>, which with its wireframe vector graphics, was one of the first
big “3D” successes in any medium. Thankfully, I was able to play a version of Battlezone at a young age, and naturally
noticed (and took a major interest in) the increasing fidelity of computer-generated graphics growing up.</p>

<p><span class="row-fill">
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/battlezone.png" alt="Battlezone(1980)" />
</span>
<span class="row-fill five-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/missle-command.png" alt="Missle Command(1980)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/donkey-kong.png" alt="Donkey Kong(1981)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/tempest.png" alt="Tempest(1981)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/pole-position.png" alt="Pole Position(1982)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/mario-bros.png" alt="Mario Bros.(1983)" />
</span>
<span class="row-fill five-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/dragons-lair.png" alt="Dragon's Lair(1983)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/marble-madness.png" alt="Marble Madness(1984)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/paperboy.png" alt="Paperboy(1985)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/smb.png" alt="Super Mario Bros.(1985)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/legend-of-zelda.png" alt="The Legend of Zelda(1986)" />
</span>
<span class="row-fill five-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/metroid.png" alt="Metroid(1986)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/castlevania.png" alt="Castlevania II: Simon's Quest(1987)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/punch-out.png" alt="Mike Tyson's Punch-Out(1987)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/mega-man.png" alt="Mega Man 2(1988)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/madden.png" alt="John Madden Football(1988)" />
</span>
<span class="row-fill four-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/sim-city.png" alt="SimCity(1989)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/smb-3.png" alt="Super Mario Bros. 3(1990)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/super-mario-world.png" alt="Super Mario World(1990)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/f-zero.png" alt="F-Zero(1990)" />
</span>
<span class="row-fill">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/another-world.png" alt="Another World(1991)" />
    </span>
<span class="row-fill four-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/wolfenstein.png" alt="Wolfenstein 3D(1992)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/virtua-racing.png" alt="Virtua Racing(1992)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/mortal-kombat.png" alt="Mortal Kombat(1992)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/alone-in-the-dark.png" alt="Alone in the Dark(1992)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/virtua-fighter.png" alt="Virtua Fighter(1993)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/aladdin.png" alt="Aladdin(1993)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/doom.png" alt="Doom(1993)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/donkey-kong-country.png" alt="Donkey Kong Country(1994)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/panzer-dragoon.png" alt="Panzer Dragoon(1995)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/quake.png" alt="Quake(1996)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/super-mario-64.png" alt="Super Mario 64(1996)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/crash.png" alt="Crash Bandicoot(1996)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/star-fox-64.png" alt="Star Fox 64(1997)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/gran-turismo.png" alt="Gran Turismo(1997)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/half-life-one.png" alt="Half-Life(1998)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/metal-gear-solid-2.png" alt="Metal Gear Solid(1998)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/sonic-adventure.png" alt="Sonic Adventure(1998)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/shenmue.png" alt="Shenmue(1999)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/madden.png" alt="Madden 2001(2000)" />
</span>
<span class="row-fill two-images">
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/gran-turismo-three.png" alt="Gran Turismo 3: A-Spec(2001)" />
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/grand-theft-auto-three.png" alt="Grand Theft Auto III(2001)" />
</span>
<span class="row-fill three-images">
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/metal-gear-solid-two.png" alt="Metal Gear Solid 2: Sons of
Liberty(2001)" />
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/melee.png" alt="Super Smash Brothers
Melee(2001)" />
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/jet-set.png" alt="Jet Set Radio Future(2002)" />
</span>
<span class="row-fill five-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/super-mario-sunshine.png" alt="Super Mario Sunshine(2002)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/splinter-cell.png" alt="Splinter Cell(2002)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/wind-waker.png" alt="The legend of Zelda: The Wind
    Waker(2003)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/viewtiful-joe.png" alt="Viewtiful Joe(2003)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/far-cry.png" alt="Far Cy(2004)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/half-life-two.png" alt="Half-Life 2(2004)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/fear.png" alt="F.E.A.R.(2005)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/call-of-duty-two.png" alt="Call of Duty 2(2005)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/table-tennis.png" alt="Rockstar Games Presents Table
    Tennis(2006)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/dead-rising.png" alt="Dead Rising(2006)" />
</span>
<span class="row-fill">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/crysis.png" alt="Crysis(2007)" />
    </span>
    <span class="row-fill">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/heavenly-sword.png" alt="Heavenly Sword(2007)" />
    </span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/halo-three.png" alt="Halo 3(2007)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/team-fortress-two.png" alt="Team Fortress 2(2007)" />
</span>
    <span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/grand-theft-auto-four.png" alt="Grand Theft Auto 4(2008)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/mirrors-edge.png" alt="Mirror's Edge(2008)" />
</span>
<span class="row-fill">
<img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/flower.png" alt="Flower(2009)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/uncharted-two.png" alt="Uncharted 2: Among
    Thieves(2009)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/god-of-war-three.png" alt="God of War III(2010)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/gran-turismo-five.png" alt="Gran Turismo 5(2010)" />
</span>
<span class="row-fill two-images"><img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/red-dead-redemption.png" alt="Red Dead
    Redemption(2010)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/witcher-two.png" alt="Witcher 2: Assassin of
    Kings(2011)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/uncharted-three.png" alt="Uncharted 3: Drake's
    Deception(2011)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/rage.png" alt="Rage(2011)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/journey.png" alt="Journey(2012)" />
</span>
<span class="row-fill three-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/max-payne-three.png" alt="Max Payne 3(2012)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/killzone-shadow-fall.png" alt="Killzone: Shadow Fall(2013)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/ryse.png" alt="Ryse: Son of Rome(2013)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/sunset-overdrive.png" alt="Sunset Overdrive(2014)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/far-cry-four.png" alt="Far Cry 4(2014)" />
</span>
<span class="row-fill">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/the-order.png" alt="The Order: 1886(2015)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/star-wars-battlefront.png" alt="Star Wars: Battlefront(2015)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/firewatch.png" alt="Firewatch(2016)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/uncharted-four.png" alt="Uncharted 4: A Thief's End(2016)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/resident-evil-seven.png" alt="Resident Evil 7(2017)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/horizon-zero-dawn.png" alt="Horizon: Zero Dawn(2017)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/hellblade.png" alt="Hellblade: Senua's Sacrifice(2017)" />
</span>
<span class="row-fill">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/shadow-of-the-colossus.png" alt="Shadow of the Colossus(2018)" />
    </span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/god-of-war.png" alt="God of War(2018)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/red-dead-two.png" alt="Red Dead Redemption 2(2018)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/ace-combat.png" alt="Ace Combat 7: Skies Unknown(2019)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/control.png" alt="Control(2019)" />
</span>
<span class="row-fill two-images">
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/call-of-duty-modern-warfare.png" alt="Call of Duty: Modern Warfare(2019)" />
    <img src="/assets/images/blog-images/path-tracer/introduction/graphics-timeline/death-stranding.png" alt="Death Stranding(2019)" />
</span></p>

<hr />

<h2 id="why-explore-ray-tracing"><a id="why-explore-ray-tracing"></a>Why Explore Ray Tracing?</h2>
<p>To put it succinctly, I chose to learn more about ray tracing for the following reasons:</p>

<ul>
  <li>
    <p><strong>Graphics are neat.</strong></p>
  </li>
  <li><strong>Ray tracing is <em>very</em> hot right now.</strong>
    <ul>
      <li>Despite having been first described algorithmically by <a href="http://graphics.stanford.edu/courses/Appel.pdf">Arthur Appel in 1968</a>, ray tracing has only recently made its way to the mainstream (like the Nvidia 20 Series (2018), the first in the industry to implement realtime hardware ray tracing in a consumer product.</li>
    </ul>
  </li>
  <li><strong>Ray tracing will likely play a huge role in rendering for years to come.</strong>
    <ul>
      <li>Ray tracing is applicable in more fields than just CGI, like optical design and acoustic modeling.</li>
    </ul>
  </li>
</ul>]]></content><author><name>Evan</name></author><category term="graphics" /><category term="ray-tracing-in-one-weekend" /><summary type="html"><![CDATA[It took me long enough, but I finally dipped my toes into the waters of computer graphics earlier this year. Continue reading to learn about what ray tracing is and why I decided to explore it.]]></summary></entry><entry><title type="html">Building a Blog:</title><link href="eldun.github.io/2020/05/18/building-a-blog.html" rel="alternate" type="text/html" title="Building a Blog:" /><published>2020-05-18T00:00:00-04:00</published><updated>2020-05-18T00:00:00-04:00</updated><id>eldun.github.io/2020/05/18/building-a-blog</id><content type="html" xml:base="eldun.github.io/2020/05/18/building-a-blog.html"><![CDATA[<h2 id="humble-beginnings">Humble Beginnings</h2>
<p>I really decided on building a blog when I started working on Peter Shirley’s <a href="https://raytracing.github.io/">Ray Tracing in One Weekend</a> series. As excellent as the content is, some of the explanations and illustrations are a bit muddy. Searching for additional resources led me to <a href="http://viclw17.github.io/">Victor Li’s Blog</a>. Inspired by the clarity, variety, and layout of Victor’s blog, I constructed a similar site for myself to document my work and as a developer.</p>

<hr />

<h2 id="github-pages"><a id="github-pages"></a>GitHub Pages</h2>

<p>As you may have surmised from the URL, this site is hosted by GitHub Pages. Originally, I was going to register a domain from Hostinger, but GitHub Pages is FREE. Additionally, GitHub is probably the most sensible place to expand my portfolio, and a static site is all I really needed for a technical blog… for now.</p>

<p>The only real trouble I ran into was trying to use custom plugins. More on this issue and how I went about solving it <a href="#ruby-plugins">below</a>.</p>

<hr />

<h2 id="jekyll"><a id="jekyll"></a>Jekyll</h2>

<p>Straight from Jekyll’s GitHub page…</p>

<blockquote>
  <p>Jekyll is a simple, blog-aware, static site generator perfect for personal, project, or organization sites. Think of it like a file-based CMS, without all the complexity. Jekyll takes your content, renders Markdown and Liquid templates, and spits out a complete, static website ready to be served by Apache, Nginx or another web server. Jekyll is the engine behind GitHub Pages, which you can use to host sites right from your GitHub repositories.</p>
</blockquote>

<p>For the most part, Jekyll has been a breeze to work with, and their documentation is top-notch. I would recommend it to anyone looking to build a static site.</p>

<hr />

<h2 id="custom-ruby-plugins"><a id="custom-ruby-plugins"></a>Custom Ruby Plugins</h2>

<p>When constructing the site, this is where I ran into the most trouble. It turns out that when using the github-pages gem, the site is generated in safe mode and the plugins directory is a random string. Additionally, only a few Jekyll plugins are whitelisted by GitHub Pages. When constructing the tag feature, tags worked fine locally, but when pushing to the master branch, the live site had broken elements. Same with the archive page.</p>

<p>One solution is to build the site locally and push the generated site files to GitHub. In this way, GitHub would interpret the site as static files and not as a Jekyll project - skipping the build process. However, doing so would require a lot of manual file management and would be prone to human error.</p>

<p>The better solution, of course, is to automate. Big thanks to Josh Frankel and <a href="https://joshfrankel.me/blog/deploying-a-jekyll-blog-to-github-pages-with-custom-plugins-and-travisci/">his post</a> detailing the process.</p>

<p><span class="note">Update:</span></p>
<blockquote>
  <p>Since June 15th, 2021, the building on travis-ci.org is ceased. Please use travis-ci.com from now on.</p>
</blockquote>

<p>This includes a brand new subscription fee! And re-configuring!<br />I don’t feel like dealing with that (or looking into other options right now), so for the time being, I’ll just be <code class="language-plaintext highlighter-rouge">jekyll build</code>-ing <code class="language-plaintext highlighter-rouge">source</code> and pushing the updated directory <code class="language-plaintext highlighter-rouge">_site</code> to <code class="language-plaintext highlighter-rouge">master</code>.&lt;/span&gt;</p>

<p><span class="note">Update(Sep 2022): I wrote a little bash script to update the live site from the source branch. Git worktree is a neat feature! I’ve never used it before. Here’s the script:</span></p>

<pre><code class="language-bash">
#!/bin/bash
 
 
if [ $(basename $PWD) != eldun.github.io ]
then
    exit "Please execute 'update-live-site.sh' from the site's root directory"
fi
 
git checkout source
 
# Generate site from branch 'source'
bundle exec jekyll build

# Create a add directory 'live-site' which is essentially branch 'master'
git worktree add live-site master
 
# Move all generated files in _site to root directory of live site (mv doesn't have a recursive option, so I'm using cp)
cp -r _site/* live-site
rm -r _site

cd live-site
git add *
git commit -m "Update live site from branch 'source'"
git push

cd ..
git worktree remove live-site/
</code></pre>
<p>&lt;/span&gt;</p>

<p>The basic idea is as follows:
<img src="/assets/images/blog-images/howdy/github-pages-build-process.png" alt="Workflow for using custom plugins on GitHub Pages" /></p>

<hr />

<ul>
  <li>
    <p><strong>Create a new branch off of master (in my case, it’s called <em>source</em>).</strong></p>

    <p>The <em>source</em> branch is where my changes take place from here on out.
<em>Source</em> will contain the entire Jekyll project, but master will only contain the static <code class="language-plaintext highlighter-rouge">_site</code> folder.
It’s also recommended to make <em>source</em> the default branch, as well as protecting it.</p>
  </li>
  <li>
    <p><strong>Generate a GitHub <em>Personal Access Token</em> for Travis CI and give it repo scope/access.</strong></p>

    <p>This will allow Travis CI to perform pushes.</p>
  </li>
  <li>
    <p><strong>Configure the Jekyll site to work with Travis CI.</strong></p>

    <p>Buckle in, becuase the next part of the process introduces a lot of changes to the Jekyll site.</p>

    <ul>
      <li>
        <p><strong>Add the following to the <code class="language-plaintext highlighter-rouge">Gemfile</code>:</strong></p>

        <pre><code class="language-ruby">source "https://rubygems.org"
  ruby RUBY_VERSION

  # We'll need rake to build our site in TravisCI
  gem "rake", "~&gt; 12"
  gem "jekyll"

  # Optional: Add any custom plugins here.
  # Some useful examples are listed below
  group :jekyll_plugins do
  gem "jekyll-feed"
  gem "jekyll-sitemap"
  gem "jekyll-paginate-v2"
  gem "jekyll-seo-tag"
  gem "jekyll-compose", "~&gt; 0.5"
  gem "jekyll-redirect-from"
  end</code></pre>
      </li>
      <li>
        <p><strong>Ensure any Gemfiles used in the Gemfile are in <code class="language-plaintext highlighter-rouge">_config.yml</code> as well.</strong></p>
      </li>
      <li>
        <p><strong>Exclude certain files to ensure they don’t end up in <em>master</em> after Travis CI builds <em>source</em>.</strong></p>
      </li>
      <li>
        <p><strong>Sample <code class="language-plaintext highlighter-rouge">_config.yml</code>:</strong></p>

        <pre><code class="language-yaml">title: Your blog title
email: your.email@gmail.com

# many other settings
# ...

# Any plugins within jekyll_plugin group from Gemfile
plugins:
- jekyll-feed
- jekyll-sitemap
- jekyll-paginate-v2
- jekyll-seo-tag
- jekyll-compose
- jekyll-redirect-from

# Exclude these files from the build process results.
# Prevents them from showing up in the master branch which 
# is the live site.
exclude:
- vendor
- Gemfile
- Gemfile.lock
- LICENSE
- README.md
- Rakefile</code></pre>
      </li>
      <li>
        <p><strong>Since <em>master</em> will be used to display the static site, we need git to ignore changes to <code class="language-plaintext highlighter-rouge">_site</code>. Add the following to <code class="language-plaintext highlighter-rouge">.gitignore</code>:</strong></p>

        <pre><code class="language-git">.sass-cache
.jekyll-metadata
_site</code></pre>
      </li>
      <li>
        <p><strong>A <code class="language-plaintext highlighter-rouge">.travis.yml</code> file is needed to inform Travis CI how to run:</strong></p>

        <pre><code class="language-yaml">#All of this together basically says, “Using the source branch from this repo, push all the files found within the site directory to the master branch of the repo”.

language: ruby #Use Ruby
rvm:
    - 2.3.1 #Use RVM to set ruby version to 2.3.1
install:
    - bundle install #Run bundle install to install all gems.
deploy:
    provider: pages #Use TravisCI’s Github Pages provider
    skip_cleanup: true #Preserve files created during build phase.
    github_token: $GITHUB_TOKEN # Our personal access token. This is currently a reference to an environment variable which will be added in the TravisCI setup section below.
    local_dir: _site #Use all files found in this directory for deployment.
    target_branch: master #Push resulting build files to this branch on Github.
on:
    branch: source #Only run TravisCI for this branch.</code></pre>
      </li>
      <li>
        <p><strong>The <code class="language-plaintext highlighter-rouge">.travis.yml</code> only works by using the following <code class="language-plaintext highlighter-rouge">Rakefile</code> to manually build the site:</strong></p>

        <pre><code class="language-ruby"># filename: Rakefile
task :default do
puts "Running CI tasks..."

# Runs the jekyll build command for production
# TravisCI will now have a site directory with our
# statically generated files.
sh("JEKYLL_ENV=production bundle exec jekyll build")
puts "Jekyll successfully built"
end</code></pre>
      </li>
      <li>
        <p><strong>The <code class="language-plaintext highlighter-rouge">Rakefile</code> runs on every build. All checks must be passed before Travis CI will deploy the build.</strong></p>
      </li>
      <li>
        <p><strong>Set up Travis CI.</strong></p>

        <p>Sign in to Travis CI.
Find the appropriate repository and enable it.
Create a new environment variable named <code class="language-plaintext highlighter-rouge">GITHUB_TOKEN</code> from the repository settings page and enter the <em>Personal Access Token</em> from way back when.</p>
      </li>
      <li>
        <p><strong>Cross fingers and PUSH (to <em>source</em>).</strong></p>
      </li>
    </ul>
  </li>
</ul>

<p>The Travis CI build should start, complete, and the site will be live!</p>

<p>WOW! Quite a bit of work for some custom plugins! Namely, my archive page and tag system. Again, HUGE thanks to <a href="https://joshfrankel.me">Josh Frankel</a> for <a href="https://joshfrankel.me/blog/deploying-a-jekyll-blog-to-github-pages-with-custom-plugins-and-travisci/">his post</a>!</p>

<hr />

<h2 id="archive-page"><a id="archive-page"></a>Archive Page</h2>
<p>The archive page I have on my site at the time of this post is adapted from <a href="https://github.com/Sodaware/jekyll-archive-page">Sodaware’s Repository</a>. There’s not much to it, just some basic ruby data collection and a simple layout file. I would eventually like to style it in a way that is more pleasing on mobile devices.</p>

<hr />

<h2 id="tag-system"><a id="tag-system"></a>Tag System</h2>
<p>For post tagging, I followed an <a href="https://blog.lunarlogic.io/2019/managing-tags-in-jekyll-blog-easily/">example from Lunar Logic</a>. In the Lunar Logic post, the author, <a href="https://blog.lunarlogic.io/author/anna/">Anna Ślimak</a>, details using Jekyll hooks - which allow for fine-grained control over various aspects of the build process. For example, one could execute custom functionality every time Jekyll renders a post. That’s exactly what I’m doing on my site for the tags.</p>

<p>While tags can simply be entered in the Front Matter of posts, no html is generated for that specific tag. I could manually create a file for said tag in the tags directory, but the hook automatically does that work for me.</p>

<p>Here’s the code:</p>
<pre><code class="language-ruby">Jekyll::Hooks.register :posts, :post_write do |post|
    all_existing_tags = Dir.entries("tags")
      .map { |t| t.match(/(.*).md/) }
      .compact.map { |m| m[1] }
  
    tags = post['tags'].reject { |t| t.empty? }
    tags.each do |tag|
      generate_tag_file(tag) if !all_existing_tags.include?(tag)
    end
  end
  
  def generate_tag_file(tag)
    File.open("tags/#{tag}.md", "wb") do |file|
      file &lt;&lt; "---\nlayout: tag-page\ntag: #{tag}\n---\n"
    end
  end</code></pre>

<hr />

<h2 id="search-function"><a id="search-function"></a>Search Function</h2>
<p>The search function was adapted from <a href="https://github.com/christian-fei/Simple-Jekyll-Search">Christian Fei’s Simple Jekyll Search</a>. Here’s the rundown:</p>

<p>Jekyll is all client-side, so the required content for a search must be stored in a file on the site itself.</p>

<p>Within the root of the Jekyll project, a <code class="language-plaintext highlighter-rouge">.json</code> file is created from existing posts containing data to search through:</p>

<p><a id="search-json"></a><code class="language-plaintext highlighter-rouge">search.json</code>:</p>
<pre><code class="language-json">---
---
[
  {% for post in site.posts %}
    {

      "postTitle"       : "{{ post.title | escape }}",
      "postSubtitle"    : "{{ post.subtitle | escape }}",
      "postTags"        : "{{ post.tags | join: ', ' }}",
      "postDate"        : "{{ post.date }}",
      "postUrl"         : "{{ post.url }}"

    } {% unless forloop.last %},{% endunless %}
  {% endfor %}
]</code></pre>

<p>This code generates a <code class="language-plaintext highlighter-rouge">search.json</code> file in the <code class="language-plaintext highlighter-rouge">_site</code> directory. Don’t forget to add escape characters to prevent the <code class="language-plaintext highlighter-rouge">.json</code> file from getting messed up. <a href="https://shopify.github.io/liquid/">Liquid has some useful filters that can help out</a>. Here’s a snippet of my generated <code class="language-plaintext highlighter-rouge">search.json</code>:</p>

<pre><code class="language-json">...
    {

      "postTitle"       : "Building a Blog:",
      "postSubtitle"    : "Howdy!",
      "postTags"        : "web, ruby",
      "postDate"        : "2020-05-18 00:00:00 -0400",
      "postUrl"         : "/2020/05/18/building-a-blog.html"

    } ,
...</code></pre>

<p>If we wanted to include other aspects of the post in our search, such as the excerpt, content, or custom variables, we could easily follow the <a href="#search-json">template above</a>.</p>

<p>Save the <a href="https://github.com/christian-fei/Simple-Jekyll-Search/blob/master/dest/simple-jekyll-search.js">search script</a> in <code class="language-plaintext highlighter-rouge">/js/simple-jekyll-search.js</code>.</p>

<p>I placed the necessary HTML elements for the search function inside <code class="language-plaintext highlighter-rouge">/_includes/search-bar.html</code>:</p>

<pre><code class="language-html">&lt;!-- Html Elements for Search --&gt;
&lt;div id="search-container" style="visibility: hidden;"&gt;
  &lt;input type="text" id="search-input" placeholder="Search..." /&gt;
  &lt;ul id="results-container"&gt;&lt;/ul&gt;
&lt;/div&gt;

&lt;!-- Script pointing to search-script.js --&gt;
&lt;script src="/js/site-scripts/search-script.js" type="text/javascript"&gt;&lt;/script&gt;

&lt;!-- Configuration --&gt;
&lt;script&gt;
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    json: '/search.json',
    searchResultTemplate: '&lt;li&gt;&lt;a href="eldun.github.io{postUrl}"&gt;{postTitle} {postSubtitle}&lt;/a&gt;&lt;/li&gt;'
  })&lt;/script&gt;</code></pre>

<p>and included it right below the nav bar in <code class="language-plaintext highlighter-rouge">/_layouts/default.html</code>.</p>

<p>The <code class="language-plaintext highlighter-rouge">searchResultTemplate</code> variable above determines what is included in the dropdown search results.</p>

<p>Lastly, I added some javascript at <code class="language-plaintext highlighter-rouge">/js/site-scripts/toggle-search.js</code> to toggle the search bar from visible to hidden:</p>

<pre><code class="language-javascript">
 function toggleSearch() {
    var search_container = document.getElementById("search-container");
    var search_button = document.getElementById("search-button");
    var search_input = document.getElementById("search-input");
    var results_container = document.getElementById("results-container")


    if (search_container.style.visibility == "hidden") {
      search_container.style.visibility = "visible";
      search_input.value = "";
      results_container.style.visibility = "hidden";
      search_input.focus();

    search_input.oninput = handleInput;


      document.onkeydown = function(evt) {
        evt = evt || window.event;

        if (search_input.value == "") {
          results_container.style.visibility = "hidden";
        }
        else {
          results_container.style.visibility = "visible";
        }

        if (evt.keyCode == 27 &amp;&amp; search_container.style.visibility == "visible") {
          toggleSearch();
          return;
        }

    };

    function handleInput(e) {
      if (search_input.value != "") {
        results_container.style.visibility = "visible";
      }
      else {
        results_container.style.visibility = "hidden";
      }
    }

    } else {
      search_container.style.visibility = "hidden";
    }

    // Styling
    if (search_container.style.visibility == "visible"){
      search_button.style.color = "#42cad1"
      search_container.style.display = "block";
      // search_input.focus();
    }
    else {
      search_button.style.color = "#cc773f";
      search_container.style.display = "none";
    }
  }

 </code></pre>

<hr />

<h2 id="the-future"><a id="the-future"></a>The Future</h2>
<p>From here onwards, I plan to document as concisely and as compellingly as possible every personal project I undertake.</p>

<p>Oh, and to reformat my <code class="language-plaintext highlighter-rouge">style.css</code>. It’s a little sloppy.</p>

<hr />

<h2 id="updates"><a id="updates"></a>Updates</h2>

<h3 id="2021-04"><time>2021-04</time></h3>

<details>
<summary>Refactored my CSS into multiple files</summary>
<br />
I used to have one monolithic `style.scss` file. Thanks to [Sass](https://sass-lang.com/), variables, and some refactoring, this is the result:

<pre><code class="language-treeview">eldun.github.io/
    ├── assets/
    │   ├── css/
    │   │   └── style.scss
    │   ├── images/
    │   └── webfonts/
    ├── _sass/
    │   ├── fontawesome/
    │   └── site/
    │       ├── about-me.scss
    │       ├── archive.scss
    │       ├── header.scss
    │       ├── images.scss
    │       ├── mathjax.scss
    │       ├── nav.scss
    │       ├── post-navigation.scss
    │       ├── posts.scss
    │       ├── search.scss
    │       └── tags.scss
    └── ...</code></pre>

<p>`style.scss` is now mostly imports and high-level stylings:</p>

<pre><code class="language-scss">---
---
$color-primary: {{ site.data.colors["primary"]["dark-theme"] }};
$color-secondary:  {{ site.data.colors["secondary"]["dark-theme"] }};
$color-accent: {{ site.data.colors["accent"]["dark-theme"] }};
$color-clickable: {{ site.data.colors["clickable"]["dark-theme"] }};
$color-text: {{ site.data.colors["text"]["dark-theme"] }};

/* This file extends/overrides the CSS file used by "jekyll-theme-cayman" (_site/assets/css/style.css)
  https://help.github.com/en/github/working-with-github-pages/adding-a-theme-to-your-github-pages-site-using-jekyll#customizing-your-themes-css 
*/


// I took this out for a second, but putting it back in is easier than writing a bunch of responive css
@import "jekyll-theme-cayman";

// The default folder for scss files is _sass (this can be changed in config.yml)
// Was having a lot of trouble trying to use fontawesome icons with their relative paths
// before creating the_sass directory and moving the scss files there.
@import "fontawesome/fontawesome.scss";
@import "fontawesome/solid.scss";
@import "fontawesome/brands.scss";
@import "fontawesome/regular.scss";


@import "site/header.scss";
@import "site/nav.scss";

@import "site/posts.scss";
@import "site/post-navigation.scss";
@import "site/images.scss";

@import "site/mathjax.scss";

@import "site/about-me.scss";
@import "site/archive.scss";
@import "site/tags.scss";
@import "site/search.scss";

// CSS rules here</code></pre>
</details>

<details>
<summary>Started using <a class="btn" href="https://prismjs.com/" target="_blank">Prism Syntax highlighter</a>
</summary>
<br />
All I had to do was generate js and css files from Prism's site, plop them into my site directory, and link 'em. To decorate a code snippet with a specific language, all I need to do is specify a code block like so:
`&lt;pre&gt;&lt;code class="language-xxxx"&gt;`
<pre><code class="language-treeview">eldun.github.io
    ├── assets/
    │   ├── css/
    │   │   └── style.scss // import generated prism css here
    │   ├── images/
    │   └── webfonts/
    ├── _config.yml
    ├── _data/
    ├── downloads/
    ├── _drafts/
    ├── Gemfile
    ├── Gemfile.lock
    ├── _includes/
    ├── index.md
    ├── js/
    │   ├── post-scripts/
    │   └── site-scripts/
    │       ├── prism.js // generated by prism
    │       ├── search-script.js
    │       ├── toggle-search.js
    │       └── vanilla-back-to-top.min.js
    ├── _layouts/
    │   ├── archive-page.html
    │   ├── default.html // link to generated prism js here 
    │   ├── post.html
    │   └── tag-page.html
    ├── _plugins/
    ├── _posts/
    ├── Rakefile
    ├── _sass/
    │   ├── fontawesome/
    │   └── site/
    │       ├── about-me.scss
    │       ├── archive.scss
    │       ├── header.scss
    │       ├── images.scss
    │       ├── mathjax.scss
    │       ├── nav.scss
    │       ├── post-navigation.scss
    │       ├── posts.scss
    │       ├── prism.scss // generated by prism
    │       ├── search.scss
    │       └── tags.scss
    └── ...</code></pre>
</details>

<h3 id="2022-08"><time>2022-08</time></h3>

<details>

<summary>
Dynamic (&amp; prettier) Table of Contents
</summary>
<br />


Manual tables of content are time intensive and prone to authoring errors, I've come to find. Thankfully there's a <a href="https://stackoverflow.com/a/5233948">gem</a> for <a href="https://github.com/toshimaru/jekyll-toc">generating a 'toc' dynamically</a>, so I don't have to do too much work.

The instructions are as follows:
<img src="/assets/images/blog-images/howdy/jekyll-toc-install.png" />

After following these steps, I only need to change a few things in <code>_layouts/post.html`</code>:
<pre><code class="language-diff-markup diff-highlight">

&lt;div class="post-header inactive"&gt;
&lt;h1 id="post-title"&gt;{{ page.title }}&lt;/h1&gt;
&lt;h3 id="post-subtitle"&gt;{{ page.subtitle }}&lt;/h3&gt;
&lt;div class="post-date"&gt;
    &lt;i class="fas fa-calendar"&gt;&lt;/i&gt; &lt;time&gt;{{ page.date | date_to_string }}&lt;/time&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;img src="{{ page.header-image }}" alt="{{ page.header-image-alt }}" title="{{ post.header-image-description }}"&gt;

+ &lt;div class='table-of-contents'&gt;{{ content | toc_only }}&lt;/div&gt;

+ {{ content | inject_anchors }}

&lt;hr&gt;
&lt;h1&gt;&lt;i class="fas fa-hand-peace"&gt;&lt;/i&gt;&lt;/h1&gt;
&lt;div class="post-tags"&gt;
    {% include post-tags.html %}
&lt;/div&gt;


</code></pre>

Now I can just use regular headers instead of clumsy anchors, and my table of contents will be generated automatically.

</details>

<details>

<summary>
Add the ability to use Liquid in Front Matter
</summary>
<br />

I've started putting more of my post content into the [Front Matter](https://jekyllrb.com/docs/front-matter/) and letting my layout do most of the work. You can't natively use [Liquid](https://jekyllrb.com/docs/liquid/) in Front Matter, but there's a <a href="https://github.com/gemfarmer/jekyll-liquify">gem</a> for enabling such functionality.

</details>

<h3 id="2023-03"><time>2023-03</time></h3>

<details>

<summary>
Hook eldun.github.io up to a custom domain
</summary>
<br />

Looks are everything, sometimes. After spending some cash on a custom domain with mailchimp, I thought I'd be able to simply drag-n-drop my statically generated website into their filesystem and have it just work. Nope!

Based on my limited understanding of the internet, I had three options:
- Redirect eldun.github.io to eldun.net(which would require rebuilding my site with MailChimp's tools and changing my whole workflow)
- Redirect eldun.net to eldun.github.io (better, but still ugly)
- Ask for help from MailChimp support

I asked for help, and Fernando was able to set up a masked forward from eldun.net to eldun.github.io - showing the content of eldun.github.io, but leaving the URL as eldun.net. This will take effect (hopefully) in the next 48 hours. Oddly enough, the configuration for masked forwarding is hidden from the user in Mailchimp:

&gt; (09:00:00 PM) Evan: okay, are these settings available for me to look at within the domain settings?
&gt; (09:00:45 PM) Fernando : You would not be able to see that within the domain settings, if you need to make any changes to the forward you would reach out to us directly

</details>]]></content><author><name>Evan</name></author><category term="web" /><category term="ruby" /><summary type="html"><![CDATA[They say the best time to start a technical blog is twenty years ago, and that the second best time is today. Continue reading to learn about my site and how I built it.]]></summary></entry></feed>